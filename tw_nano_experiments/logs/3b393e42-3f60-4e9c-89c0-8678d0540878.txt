====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock and keep timing accurate, but skip checkpoint writes to save disk
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        if master_process:
            print("Checkpoint saving disabled; skipping state serialization.")
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 22:41:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     34%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   48C    P0             88W /  310W |    2363MiB /  81559MiB |     29%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             85W /  310W |    2363MiB /  81559MiB |     35%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   43C    P0             80W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0             86W /  310W |    2363MiB /  81559MiB |     29%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   52C    P0             88W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   44C    P0             82W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           77885      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           77886      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           77887      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           77888      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           77889      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           77890      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           77891      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           77892      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 123 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: focal
  clamp: [0.8, 1.2]
  schedule: constant
  focal_gamma: 2.0
====================================================================================================
step:0/500 val_loss:16.0073 train_time:269ms step_avg:nanms
step:1/500 train_loss:15.9981 train_time:56014ms step_avg:nanms w_mean:1.000 w_std:0.024 w_min:0.991 w_max:1.239
step:2/500 train_loss:9.3702 train_time:56390ms step_avg:nanms
step:3/500 train_loss:8.9749 train_time:56655ms step_avg:nanms
step:4/500 train_loss:8.6395 train_time:56920ms step_avg:nanms
step:5/500 train_loss:8.0956 train_time:57187ms step_avg:nanms
step:6/500 train_loss:7.7852 train_time:57464ms step_avg:nanms
step:7/500 train_loss:7.3915 train_time:57731ms step_avg:nanms
step:8/500 train_loss:7.5078 train_time:57999ms step_avg:nanms
step:9/500 train_loss:7.2765 train_time:58264ms step_avg:nanms
step:10/500 train_loss:7.0984 train_time:58537ms step_avg:nanms
step:11/500 train_loss:7.0928 train_time:267ms step_avg:nanms
step:12/500 train_loss:7.0474 train_time:535ms step_avg:nanms
step:13/500 train_loss:6.8631 train_time:803ms step_avg:267.77ms
step:14/500 train_loss:6.8648 train_time:1069ms step_avg:267.24ms
step:15/500 train_loss:6.8218 train_time:1343ms step_avg:268.57ms
step:16/500 train_loss:6.7357 train_time:1610ms step_avg:268.36ms
step:17/500 train_loss:6.7389 train_time:1879ms step_avg:268.44ms
step:18/500 train_loss:6.7719 train_time:2147ms step_avg:268.32ms
step:19/500 train_loss:6.5864 train_time:2417ms step_avg:268.51ms
step:20/500 train_loss:6.5920 train_time:2687ms step_avg:268.66ms
step:21/500 train_loss:6.2598 train_time:2960ms step_avg:269.07ms
step:22/500 train_loss:6.6538 train_time:3227ms step_avg:268.91ms
step:23/500 train_loss:6.9015 train_time:3495ms step_avg:268.88ms
step:24/500 train_loss:6.5298 train_time:3768ms step_avg:269.14ms
step:25/500 train_loss:6.6486 train_time:4035ms step_avg:269.00ms
step:26/500 train_loss:6.3739 train_time:4304ms step_avg:269.01ms
step:27/500 train_loss:6.2808 train_time:4571ms step_avg:268.90ms
step:28/500 train_loss:6.4554 train_time:4845ms step_avg:269.18ms
step:29/500 train_loss:6.1062 train_time:5112ms step_avg:269.06ms
step:30/500 train_loss:6.3883 train_time:5385ms step_avg:269.27ms
step:31/500 train_loss:6.2349 train_time:5651ms step_avg:269.12ms
step:32/500 train_loss:6.1967 train_time:5922ms step_avg:269.17ms
step:33/500 train_loss:6.0076 train_time:6190ms step_avg:269.12ms
step:34/500 train_loss:6.3489 train_time:6462ms step_avg:269.24ms
step:35/500 train_loss:6.2616 train_time:6729ms step_avg:269.15ms
step:36/500 train_loss:6.4153 train_time:6999ms step_avg:269.19ms
step:37/500 train_loss:6.3388 train_time:7269ms step_avg:269.21ms
step:38/500 train_loss:6.2299 train_time:7541ms step_avg:269.33ms
step:39/500 train_loss:6.1233 train_time:7810ms step_avg:269.30ms
step:40/500 train_loss:6.1753 train_time:8082ms step_avg:269.39ms
step:41/500 train_loss:6.0773 train_time:8351ms step_avg:269.38ms
step:42/500 train_loss:6.1228 train_time:8623ms step_avg:269.46ms
step:43/500 train_loss:5.9897 train_time:8891ms step_avg:269.43ms
step:44/500 train_loss:6.0694 train_time:9165ms step_avg:269.55ms
step:45/500 train_loss:6.0583 train_time:9431ms step_avg:269.45ms
step:46/500 train_loss:6.2466 train_time:9706ms step_avg:269.61ms
step:47/500 train_loss:6.0445 train_time:9979ms step_avg:269.70ms
step:48/500 train_loss:5.8854 train_time:10249ms step_avg:269.71ms
step:49/500 train_loss:6.1337 train_time:10518ms step_avg:269.69ms
step:50/500 train_loss:6.0002 train_time:10789ms step_avg:269.73ms
step:51/500 train_loss:6.1568 train_time:11064ms step_avg:269.86ms w_mean:1.000 w_std:0.001 w_min:1.000 w_max:1.250
step:52/500 train_loss:6.0157 train_time:11334ms step_avg:269.86ms
step:53/500 train_loss:5.8650 train_time:11605ms step_avg:269.88ms
step:54/500 train_loss:5.9956 train_time:11876ms step_avg:269.91ms
step:55/500 train_loss:5.9127 train_time:12150ms step_avg:269.99ms
step:56/500 train_loss:6.2217 train_time:12420ms step_avg:269.99ms
step:57/500 train_loss:5.9070 train_time:12690ms step_avg:269.99ms
step:58/500 train_loss:5.7881 train_time:12965ms step_avg:270.09ms
step:59/500 train_loss:5.9454 train_time:13236ms step_avg:270.11ms
step:60/500 train_loss:5.8851 train_time:13507ms step_avg:270.14ms
step:61/500 train_loss:5.9702 train_time:13781ms step_avg:270.22ms
step:62/500 train_loss:5.7734 train_time:14051ms step_avg:270.21ms
step:63/500 train_loss:5.8754 train_time:14322ms step_avg:270.23ms
step:64/500 train_loss:5.8420 train_time:14592ms step_avg:270.23ms
step:65/500 train_loss:5.7648 train_time:14865ms step_avg:270.27ms
step:66/500 train_loss:5.6720 train_time:15135ms step_avg:270.27ms
step:67/500 train_loss:5.8430 train_time:15407ms step_avg:270.30ms
step:68/500 train_loss:5.7060 train_time:15678ms step_avg:270.32ms
step:69/500 train_loss:5.9550 train_time:15949ms step_avg:270.33ms
step:70/500 train_loss:5.6209 train_time:16221ms step_avg:270.35ms
step:71/500 train_loss:5.6437 train_time:16492ms step_avg:270.36ms
step:72/500 train_loss:5.8525 train_time:16763ms step_avg:270.37ms
step:73/500 train_loss:5.7936 train_time:17035ms step_avg:270.39ms
step:74/500 train_loss:5.6827 train_time:17307ms step_avg:270.42ms
step:75/500 train_loss:5.7889 train_time:17581ms step_avg:270.48ms
step:76/500 train_loss:5.7552 train_time:17851ms step_avg:270.47ms
step:77/500 train_loss:5.7187 train_time:18122ms step_avg:270.48ms
step:78/500 train_loss:5.8106 train_time:18393ms step_avg:270.48ms
step:79/500 train_loss:5.8371 train_time:18666ms step_avg:270.52ms
step:80/500 train_loss:5.6921 train_time:18940ms step_avg:270.57ms
step:81/500 train_loss:5.7800 train_time:19210ms step_avg:270.56ms
step:82/500 train_loss:5.5469 train_time:19485ms step_avg:270.62ms
step:83/500 train_loss:5.7151 train_time:19757ms step_avg:270.64ms
step:84/500 train_loss:5.6869 train_time:20028ms step_avg:270.65ms
step:85/500 train_loss:5.6486 train_time:20301ms step_avg:270.68ms
step:86/500 train_loss:5.5168 train_time:20572ms step_avg:270.68ms
step:87/500 train_loss:5.7223 train_time:20845ms step_avg:270.71ms
step:88/500 train_loss:5.6306 train_time:21116ms step_avg:270.72ms
step:89/500 train_loss:5.6853 train_time:21390ms step_avg:270.76ms
step:90/500 train_loss:5.6693 train_time:21663ms step_avg:270.79ms
step:91/500 train_loss:5.5809 train_time:21935ms step_avg:270.80ms
step:92/500 train_loss:5.5912 train_time:22207ms step_avg:270.82ms
step:93/500 train_loss:5.6824 train_time:22482ms step_avg:270.87ms
step:94/500 train_loss:5.5319 train_time:22752ms step_avg:270.85ms
step:95/500 train_loss:5.5231 train_time:23024ms step_avg:270.87ms
step:96/500 train_loss:5.5429 train_time:23295ms step_avg:270.87ms
step:97/500 train_loss:5.4586 train_time:23569ms step_avg:270.91ms
step:98/500 train_loss:5.5346 train_time:23845ms step_avg:270.96ms
step:99/500 train_loss:5.4570 train_time:24115ms step_avg:270.96ms
step:100/500 train_loss:5.5813 train_time:24389ms step_avg:270.99ms
step:101/500 train_loss:5.5423 train_time:24662ms step_avg:271.01ms w_mean:1.000 w_std:0.002 w_min:1.000 w_max:1.250
step:102/500 train_loss:5.4330 train_time:24931ms step_avg:270.99ms
step:103/500 train_loss:5.5418 train_time:25206ms step_avg:271.04ms
step:104/500 train_loss:5.5058 train_time:25477ms step_avg:271.04ms
step:105/500 train_loss:5.3404 train_time:25751ms step_avg:271.06ms
step:106/500 train_loss:5.4518 train_time:26021ms step_avg:271.05ms
step:107/500 train_loss:5.6500 train_time:26292ms step_avg:271.05ms
step:108/500 train_loss:5.4378 train_time:26566ms step_avg:271.08ms
step:109/500 train_loss:5.2021 train_time:26839ms step_avg:271.10ms
step:110/500 train_loss:5.4051 train_time:27110ms step_avg:271.10ms
step:111/500 train_loss:5.3692 train_time:27386ms step_avg:271.15ms
step:112/500 train_loss:5.3459 train_time:27658ms step_avg:271.16ms
step:113/500 train_loss:5.4502 train_time:27930ms step_avg:271.16ms
step:114/500 train_loss:5.3826 train_time:28204ms step_avg:271.19ms
step:115/500 train_loss:5.2323 train_time:28475ms step_avg:271.19ms
step:116/500 train_loss:5.4012 train_time:28749ms step_avg:271.22ms
step:117/500 train_loss:5.2611 train_time:29019ms step_avg:271.21ms
step:118/500 train_loss:5.2541 train_time:29292ms step_avg:271.22ms
step:119/500 train_loss:5.3744 train_time:29565ms step_avg:271.24ms
step:120/500 train_loss:5.3623 train_time:29838ms step_avg:271.26ms
step:121/500 train_loss:5.2904 train_time:30110ms step_avg:271.26ms
step:122/500 train_loss:5.1840 train_time:30386ms step_avg:271.30ms
step:123/500 train_loss:5.2758 train_time:30660ms step_avg:271.32ms
step:124/500 train_loss:5.1452 train_time:30930ms step_avg:271.31ms
step:125/500 train_loss:5.4415 train_time:31205ms step_avg:271.34ms
step:125/500 val_loss:5.2721 train_time:31205ms step_avg:271.35ms
step:126/500 train_loss:5.2932 train_time:31480ms step_avg:271.38ms
step:127/500 train_loss:5.2704 train_time:31758ms step_avg:271.44ms
step:128/500 train_loss:5.3385 train_time:32031ms step_avg:271.45ms
step:129/500 train_loss:5.1938 train_time:32299ms step_avg:271.42ms
step:130/500 train_loss:5.4653 train_time:32574ms step_avg:271.45ms
step:131/500 train_loss:5.2481 train_time:32852ms step_avg:271.51ms
step:132/500 train_loss:5.2461 train_time:33127ms step_avg:271.53ms
step:133/500 train_loss:5.1933 train_time:33397ms step_avg:271.52ms
step:134/500 train_loss:5.2390 train_time:33670ms step_avg:271.53ms
step:135/500 train_loss:5.1667 train_time:33945ms step_avg:271.56ms
step:136/500 train_loss:5.2367 train_time:34216ms step_avg:271.55ms
step:137/500 train_loss:5.0329 train_time:34489ms step_avg:271.57ms
step:138/500 train_loss:5.1976 train_time:34764ms step_avg:271.59ms
step:139/500 train_loss:5.1523 train_time:35036ms step_avg:271.60ms
step:140/500 train_loss:5.1629 train_time:35308ms step_avg:271.60ms
step:141/500 train_loss:5.2165 train_time:35579ms step_avg:271.60ms
step:142/500 train_loss:5.1194 train_time:35852ms step_avg:271.61ms
step:143/500 train_loss:5.1950 train_time:36130ms step_avg:271.66ms
step:144/500 train_loss:5.0043 train_time:36401ms step_avg:271.65ms
step:145/500 train_loss:5.1621 train_time:36676ms step_avg:271.67ms
step:146/500 train_loss:5.1057 train_time:36951ms step_avg:271.70ms
step:147/500 train_loss:5.0134 train_time:37224ms step_avg:271.71ms
step:148/500 train_loss:5.1316 train_time:37495ms step_avg:271.70ms
step:149/500 train_loss:5.1059 train_time:37769ms step_avg:271.72ms
step:150/500 train_loss:5.1671 train_time:38042ms step_avg:271.73ms
step:151/500 train_loss:5.1820 train_time:38315ms step_avg:271.74ms w_mean:1.000 w_std:0.002 w_min:1.000 w_max:1.250
step:152/500 train_loss:5.1020 train_time:38589ms step_avg:271.76ms
step:153/500 train_loss:5.0780 train_time:38862ms step_avg:271.76ms
step:154/500 train_loss:5.1552 train_time:39134ms step_avg:271.77ms
step:155/500 train_loss:5.0941 train_time:39407ms step_avg:271.77ms
step:156/500 train_loss:5.0737 train_time:39679ms step_avg:271.77ms
step:157/500 train_loss:5.0896 train_time:39951ms step_avg:271.78ms
step:158/500 train_loss:5.2141 train_time:40226ms step_avg:271.80ms
step:159/500 train_loss:5.0067 train_time:40497ms step_avg:271.79ms
step:160/500 train_loss:5.0605 train_time:40773ms step_avg:271.82ms
step:161/500 train_loss:4.9168 train_time:41046ms step_avg:271.83ms
step:162/500 train_loss:5.0673 train_time:41315ms step_avg:271.81ms
step:163/500 train_loss:5.1019 train_time:41590ms step_avg:271.83ms
step:164/500 train_loss:5.0866 train_time:41865ms step_avg:271.85ms
step:165/500 train_loss:4.9166 train_time:42137ms step_avg:271.85ms
step:166/500 train_loss:5.0311 train_time:42411ms step_avg:271.86ms
step:167/500 train_loss:5.1897 train_time:42683ms step_avg:271.86ms
step:168/500 train_loss:4.9599 train_time:42955ms step_avg:271.87ms
step:169/500 train_loss:5.0458 train_time:43231ms step_avg:271.89ms
step:170/500 train_loss:4.9129 train_time:43501ms step_avg:271.88ms
step:171/500 train_loss:4.8496 train_time:43774ms step_avg:271.89ms
step:172/500 train_loss:4.9625 train_time:44050ms step_avg:271.91ms
step:173/500 train_loss:4.9381 train_time:44324ms step_avg:271.93ms
step:174/500 train_loss:4.9917 train_time:44594ms step_avg:271.91ms
step:175/500 train_loss:5.1334 train_time:44870ms step_avg:271.94ms
step:176/500 train_loss:5.0184 train_time:45143ms step_avg:271.95ms
step:177/500 train_loss:4.8543 train_time:45415ms step_avg:271.95ms
step:178/500 train_loss:4.8378 train_time:45688ms step_avg:271.95ms
step:179/500 train_loss:4.8742 train_time:45962ms step_avg:271.96ms
step:180/500 train_loss:4.9137 train_time:46236ms step_avg:271.97ms
step:181/500 train_loss:4.9023 train_time:46508ms step_avg:271.98ms
step:182/500 train_loss:5.0156 train_time:46779ms step_avg:271.97ms
step:183/500 train_loss:4.9073 train_time:47055ms step_avg:271.99ms
step:184/500 train_loss:4.8342 train_time:47330ms step_avg:272.01ms
step:185/500 train_loss:4.8637 train_time:47601ms step_avg:272.00ms
step:186/500 train_loss:4.9891 train_time:47872ms step_avg:272.00ms
step:187/500 train_loss:4.8735 train_time:48148ms step_avg:272.02ms
step:188/500 train_loss:5.1130 train_time:48422ms step_avg:272.03ms
step:189/500 train_loss:4.9087 train_time:48953ms step_avg:273.48ms
step:190/500 train_loss:4.8230 train_time:49493ms step_avg:274.96ms
step:191/500 train_loss:4.9849 train_time:49761ms step_avg:274.93ms
step:192/500 train_loss:4.8222 train_time:50032ms step_avg:274.90ms
step:193/500 train_loss:4.7412 train_time:50301ms step_avg:274.87ms
step:194/500 train_loss:4.9424 train_time:50576ms step_avg:274.87ms
step:195/500 train_loss:4.8825 train_time:50850ms step_avg:274.87ms
step:196/500 train_loss:5.0656 train_time:51120ms step_avg:274.84ms
step:197/500 train_loss:4.9589 train_time:51389ms step_avg:274.81ms
step:198/500 train_loss:4.7987 train_time:51665ms step_avg:274.81ms
step:199/500 train_loss:4.8409 train_time:51937ms step_avg:274.80ms
step:200/500 train_loss:4.7363 train_time:52210ms step_avg:274.79ms
step:201/500 train_loss:4.8206 train_time:52480ms step_avg:274.76ms w_mean:1.000 w_std:0.001 w_min:1.000 w_max:1.250
step:202/500 train_loss:4.7395 train_time:52753ms step_avg:274.76ms
step:203/500 train_loss:4.9660 train_time:53028ms step_avg:274.76ms
step:204/500 train_loss:4.8680 train_time:53300ms step_avg:274.74ms
step:205/500 train_loss:4.8378 train_time:53572ms step_avg:274.73ms
step:206/500 train_loss:4.9893 train_time:53846ms step_avg:274.73ms
step:207/500 train_loss:4.6685 train_time:54118ms step_avg:274.71ms
step:208/500 train_loss:4.8135 train_time:54391ms step_avg:274.70ms
step:209/500 train_loss:4.7715 train_time:54665ms step_avg:274.70ms
step:210/500 train_loss:4.9327 train_time:54937ms step_avg:274.69ms
step:211/500 train_loss:4.8570 train_time:55210ms step_avg:274.68ms
step:212/500 train_loss:4.7459 train_time:55484ms step_avg:274.67ms
step:213/500 train_loss:4.8747 train_time:55756ms step_avg:274.66ms
step:214/500 train_loss:4.7145 train_time:56031ms step_avg:274.66ms
step:215/500 train_loss:4.8087 train_time:56302ms step_avg:274.64ms
step:216/500 train_loss:4.6641 train_time:56575ms step_avg:274.64ms
step:217/500 train_loss:4.7818 train_time:56849ms step_avg:274.63ms
step:218/500 train_loss:4.7683 train_time:57120ms step_avg:274.62ms
step:219/500 train_loss:4.7417 train_time:57393ms step_avg:274.61ms
step:220/500 train_loss:4.7522 train_time:57668ms step_avg:274.61ms
step:221/500 train_loss:4.7792 train_time:57941ms step_avg:274.60ms
step:222/500 train_loss:4.8137 train_time:58212ms step_avg:274.59ms
step:223/500 train_loss:4.7556 train_time:58487ms step_avg:274.58ms
step:224/500 train_loss:4.7585 train_time:58757ms step_avg:274.57ms
step:225/500 train_loss:4.8848 train_time:59032ms step_avg:274.57ms
step:226/500 train_loss:4.6277 train_time:59303ms step_avg:274.55ms
step:227/500 train_loss:4.6575 train_time:59575ms step_avg:274.54ms
step:228/500 train_loss:4.6488 train_time:59851ms step_avg:274.55ms
step:229/500 train_loss:4.8113 train_time:60127ms step_avg:274.55ms
step:230/500 train_loss:4.6360 train_time:60395ms step_avg:274.52ms
step:231/500 train_loss:4.7913 train_time:60670ms step_avg:274.53ms
step:232/500 train_loss:4.6549 train_time:60944ms step_avg:274.52ms
step:233/500 train_loss:4.6190 train_time:61216ms step_avg:274.51ms
step:234/500 train_loss:4.8203 train_time:61489ms step_avg:274.50ms
step:235/500 train_loss:4.6605 train_time:61762ms step_avg:274.50ms
step:236/500 train_loss:4.5972 train_time:62036ms step_avg:274.49ms
step:237/500 train_loss:4.8479 train_time:62307ms step_avg:274.48ms
step:238/500 train_loss:4.7323 train_time:62578ms step_avg:274.46ms
step:239/500 train_loss:4.6411 train_time:62851ms step_avg:274.46ms
step:240/500 train_loss:4.7832 train_time:63126ms step_avg:274.46ms
step:241/500 train_loss:4.7666 train_time:63396ms step_avg:274.44ms
step:242/500 train_loss:4.6731 train_time:63671ms step_avg:274.44ms
step:243/500 train_loss:4.8255 train_time:63943ms step_avg:274.44ms
step:244/500 train_loss:4.6681 train_time:64216ms step_avg:274.43ms
step:245/500 train_loss:4.6752 train_time:64489ms step_avg:274.42ms
step:246/500 train_loss:4.7478 train_time:64760ms step_avg:274.41ms
step:247/500 train_loss:4.6986 train_time:65034ms step_avg:274.41ms
step:248/500 train_loss:4.6602 train_time:65307ms step_avg:274.40ms
step:249/500 train_loss:4.8230 train_time:65579ms step_avg:274.39ms
step:250/500 train_loss:4.5637 train_time:65852ms step_avg:274.38ms
step:250/500 val_loss:4.6688 train_time:65853ms step_avg:274.39ms
step:251/500 train_loss:4.6025 train_time:66128ms step_avg:274.39ms w_mean:1.000 w_std:0.001 w_min:1.000 w_max:1.250
step:252/500 train_loss:4.7388 train_time:66404ms step_avg:274.40ms
step:253/500 train_loss:4.7246 train_time:66680ms step_avg:274.40ms
step:254/500 train_loss:4.6030 train_time:66949ms step_avg:274.38ms
step:255/500 train_loss:4.6290 train_time:67219ms step_avg:274.36ms
step:256/500 train_loss:4.7502 train_time:67494ms step_avg:274.37ms
step:257/500 train_loss:4.7007 train_time:67768ms step_avg:274.36ms
step:258/500 train_loss:4.6689 train_time:68040ms step_avg:274.35ms
step:259/500 train_loss:4.6049 train_time:68316ms step_avg:274.36ms
step:260/500 train_loss:4.6198 train_time:68588ms step_avg:274.35ms
step:261/500 train_loss:4.6895 train_time:68862ms step_avg:274.35ms
step:262/500 train_loss:4.6941 train_time:69136ms step_avg:274.35ms
step:263/500 train_loss:4.5992 train_time:69409ms step_avg:274.34ms
step:264/500 train_loss:4.5494 train_time:69681ms step_avg:274.34ms
step:265/500 train_loss:4.6070 train_time:69956ms step_avg:274.34ms
step:266/500 train_loss:4.4611 train_time:70229ms step_avg:274.33ms
step:267/500 train_loss:4.5195 train_time:70500ms step_avg:274.32ms
step:268/500 train_loss:4.5593 train_time:70775ms step_avg:274.32ms
step:269/500 train_loss:4.5219 train_time:71052ms step_avg:274.33ms
step:270/500 train_loss:4.4853 train_time:71323ms step_avg:274.32ms
step:271/500 train_loss:4.7063 train_time:71596ms step_avg:274.31ms
step:272/500 train_loss:4.6377 train_time:71868ms step_avg:274.30ms
step:273/500 train_loss:4.4957 train_time:72141ms step_avg:274.30ms
step:274/500 train_loss:4.5462 train_time:72418ms step_avg:274.31ms
step:275/500 train_loss:4.6698 train_time:72690ms step_avg:274.30ms
step:276/500 train_loss:4.6800 train_time:72961ms step_avg:274.29ms
step:277/500 train_loss:4.8730 train_time:73238ms step_avg:274.30ms
step:278/500 train_loss:4.6267 train_time:73513ms step_avg:274.30ms
step:279/500 train_loss:4.7578 train_time:73784ms step_avg:274.29ms
step:280/500 train_loss:4.6044 train_time:74058ms step_avg:274.29ms
step:281/500 train_loss:4.6699 train_time:74335ms step_avg:274.30ms
step:282/500 train_loss:4.5618 train_time:74608ms step_avg:274.29ms
step:283/500 train_loss:4.6811 train_time:74880ms step_avg:274.29ms
step:284/500 train_loss:4.5018 train_time:75156ms step_avg:274.29ms
step:285/500 train_loss:4.6644 train_time:75427ms step_avg:274.28ms
step:286/500 train_loss:4.6498 train_time:75701ms step_avg:274.28ms
step:287/500 train_loss:4.6830 train_time:75975ms step_avg:274.28ms
step:288/500 train_loss:4.5438 train_time:76247ms step_avg:274.27ms
step:289/500 train_loss:4.6119 train_time:76521ms step_avg:274.27ms
step:290/500 train_loss:4.4738 train_time:76792ms step_avg:274.26ms
step:291/500 train_loss:4.4722 train_time:77062ms step_avg:274.24ms
step:292/500 train_loss:4.5949 train_time:77337ms step_avg:274.24ms
step:293/500 train_loss:4.4809 train_time:77611ms step_avg:274.24ms
step:294/500 train_loss:4.5312 train_time:77883ms step_avg:274.24ms
step:295/500 train_loss:4.5557 train_time:78156ms step_avg:274.23ms
step:296/500 train_loss:4.4231 train_time:78430ms step_avg:274.23ms
step:297/500 train_loss:4.4119 train_time:78702ms step_avg:274.22ms
step:298/500 train_loss:4.4348 train_time:78977ms step_avg:274.22ms
step:299/500 train_loss:4.5445 train_time:79252ms step_avg:274.23ms
step:300/500 train_loss:4.4300 train_time:79523ms step_avg:274.22ms
step:301/500 train_loss:4.6040 train_time:79796ms step_avg:274.21ms w_mean:1.000 w_std:0.002 w_min:1.000 w_max:1.250
step:302/500 train_loss:4.5790 train_time:80071ms step_avg:274.22ms
step:303/500 train_loss:4.5010 train_time:80343ms step_avg:274.21ms
step:304/500 train_loss:4.5657 train_time:80617ms step_avg:274.21ms
step:305/500 train_loss:4.5476 train_time:80887ms step_avg:274.19ms
step:306/500 train_loss:5.0227 train_time:81160ms step_avg:274.19ms
step:307/500 train_loss:4.5086 train_time:81437ms step_avg:274.20ms
step:308/500 train_loss:4.4111 train_time:81711ms step_avg:274.20ms
step:309/500 train_loss:4.5971 train_time:81983ms step_avg:274.19ms
step:310/500 train_loss:4.4027 train_time:82258ms step_avg:274.19ms
step:311/500 train_loss:4.6369 train_time:82531ms step_avg:274.19ms
step:312/500 train_loss:4.5438 train_time:82802ms step_avg:274.18ms
step:313/500 train_loss:4.4581 train_time:83076ms step_avg:274.18ms
step:314/500 train_loss:4.5820 train_time:83351ms step_avg:274.18ms
step:315/500 train_loss:4.7140 train_time:83623ms step_avg:274.17ms
step:316/500 train_loss:4.5521 train_time:83894ms step_avg:274.16ms
step:317/500 train_loss:4.4370 train_time:84167ms step_avg:274.16ms
step:318/500 train_loss:4.4493 train_time:84440ms step_avg:274.16ms
step:319/500 train_loss:4.4688 train_time:84718ms step_avg:274.17ms
step:320/500 train_loss:4.4166 train_time:84988ms step_avg:274.16ms
step:321/500 train_loss:4.5101 train_time:85261ms step_avg:274.15ms
step:322/500 train_loss:4.5253 train_time:85535ms step_avg:274.15ms
step:323/500 train_loss:4.4834 train_time:85810ms step_avg:274.15ms
step:324/500 train_loss:4.5545 train_time:86081ms step_avg:274.14ms
step:325/500 train_loss:4.5510 train_time:86357ms step_avg:274.15ms
step:326/500 train_loss:4.6223 train_time:86630ms step_avg:274.15ms
step:327/500 train_loss:4.4688 train_time:86901ms step_avg:274.14ms
step:328/500 train_loss:4.9098 train_time:87177ms step_avg:274.14ms
step:329/500 train_loss:4.6240 train_time:87453ms step_avg:274.15ms
step:330/500 train_loss:4.4116 train_time:87728ms step_avg:274.15ms
step:331/500 train_loss:4.3826 train_time:87999ms step_avg:274.14ms
step:332/500 train_loss:4.5278 train_time:88270ms step_avg:274.13ms
step:333/500 train_loss:4.4484 train_time:88543ms step_avg:274.13ms
step:334/500 train_loss:4.4360 train_time:88818ms step_avg:274.13ms
step:335/500 train_loss:4.3930 train_time:89089ms step_avg:274.12ms
step:336/500 train_loss:4.5869 train_time:89360ms step_avg:274.11ms
step:337/500 train_loss:4.5227 train_time:89637ms step_avg:274.12ms
step:338/500 train_loss:5.0577 train_time:89913ms step_avg:274.12ms
step:339/500 train_loss:4.4988 train_time:90182ms step_avg:274.11ms
step:340/500 train_loss:4.4675 train_time:90456ms step_avg:274.11ms
step:341/500 train_loss:4.4444 train_time:90731ms step_avg:274.11ms
step:342/500 train_loss:4.3922 train_time:91003ms step_avg:274.10ms
step:343/500 train_loss:4.3656 train_time:91276ms step_avg:274.10ms
step:344/500 train_loss:4.4297 train_time:91550ms step_avg:274.10ms
step:345/500 train_loss:4.5202 train_time:91822ms step_avg:274.10ms
step:346/500 train_loss:4.4180 train_time:92096ms step_avg:274.09ms
step:347/500 train_loss:4.3594 train_time:92369ms step_avg:274.09ms
step:348/500 train_loss:4.4057 train_time:92643ms step_avg:274.09ms
step:349/500 train_loss:4.4090 train_time:92917ms step_avg:274.09ms
step:350/500 train_loss:4.3374 train_time:93189ms step_avg:274.08ms
step:351/500 train_loss:4.0262 train_time:93461ms step_avg:274.08ms w_mean:1.000 w_std:0.002 w_min:1.000 w_max:1.250
step:352/500 train_loss:4.3169 train_time:93737ms step_avg:274.09ms
step:353/500 train_loss:4.6568 train_time:94012ms step_avg:274.09ms
step:354/500 train_loss:4.1974 train_time:94282ms step_avg:274.07ms
step:355/500 train_loss:4.4484 train_time:94556ms step_avg:274.08ms
step:356/500 train_loss:4.3469 train_time:94833ms step_avg:274.08ms
step:357/500 train_loss:4.4496 train_time:95103ms step_avg:274.07ms
step:358/500 train_loss:4.4506 train_time:95376ms step_avg:274.07ms
step:359/500 train_loss:4.3614 train_time:95647ms step_avg:274.06ms
step:360/500 train_loss:4.6618 train_time:95922ms step_avg:274.06ms
step:361/500 train_loss:4.0948 train_time:96194ms step_avg:274.06ms
step:362/500 train_loss:4.5757 train_time:96466ms step_avg:274.05ms
step:363/500 train_loss:4.4725 train_time:96739ms step_avg:274.05ms
step:364/500 train_loss:4.3589 train_time:97017ms step_avg:274.06ms
step:365/500 train_loss:4.2954 train_time:97287ms step_avg:274.05ms
step:366/500 train_loss:4.4471 train_time:97561ms step_avg:274.05ms
step:367/500 train_loss:4.3794 train_time:97833ms step_avg:274.04ms
step:368/500 train_loss:4.3641 train_time:98103ms step_avg:274.03ms
step:369/500 train_loss:4.3719 train_time:98376ms step_avg:274.03ms
step:370/500 train_loss:4.2611 train_time:98651ms step_avg:274.03ms
step:371/500 train_loss:4.4184 train_time:98923ms step_avg:274.02ms
step:372/500 train_loss:4.3440 train_time:99195ms step_avg:274.02ms
step:373/500 train_loss:4.2246 train_time:99466ms step_avg:274.01ms
step:374/500 train_loss:4.4174 train_time:99739ms step_avg:274.01ms
step:375/500 train_loss:4.3523 train_time:100016ms step_avg:274.02ms
step:375/500 val_loss:4.3650 train_time:100017ms step_avg:274.02ms
step:376/500 train_loss:4.3457 train_time:100291ms step_avg:274.02ms
step:377/500 train_loss:4.4006 train_time:100573ms step_avg:274.04ms
step:378/500 train_loss:4.3012 train_time:101102ms step_avg:274.73ms
step:379/500 train_loss:4.3515 train_time:101370ms step_avg:274.71ms
step:380/500 train_loss:4.4256 train_time:101900ms step_avg:275.41ms
step:381/500 train_loss:4.4544 train_time:102168ms step_avg:275.38ms
step:382/500 train_loss:4.3985 train_time:102436ms step_avg:275.37ms
step:383/500 train_loss:4.3773 train_time:102707ms step_avg:275.36ms
step:384/500 train_loss:4.2801 train_time:102983ms step_avg:275.36ms
step:385/500 train_loss:4.3852 train_time:103254ms step_avg:275.34ms
step:386/500 train_loss:4.2966 train_time:103525ms step_avg:275.33ms
step:387/500 train_loss:4.4225 train_time:103797ms step_avg:275.32ms
step:388/500 train_loss:4.6252 train_time:104076ms step_avg:275.33ms
step:389/500 train_loss:4.3209 train_time:104348ms step_avg:275.33ms
step:390/500 train_loss:4.2830 train_time:104619ms step_avg:275.31ms
step:391/500 train_loss:4.4080 train_time:104890ms step_avg:275.30ms
step:392/500 train_loss:4.3303 train_time:105165ms step_avg:275.30ms
step:393/500 train_loss:4.4349 train_time:105439ms step_avg:275.30ms
step:394/500 train_loss:4.2551 train_time:105709ms step_avg:275.28ms
step:395/500 train_loss:4.3904 train_time:105982ms step_avg:275.28ms
step:396/500 train_loss:4.1755 train_time:106252ms step_avg:275.26ms
step:397/500 train_loss:4.3381 train_time:106528ms step_avg:275.26ms
step:398/500 train_loss:4.4276 train_time:106799ms step_avg:275.25ms
step:399/500 train_loss:4.3786 train_time:107071ms step_avg:275.25ms
step:400/500 train_loss:4.3027 train_time:107346ms step_avg:275.25ms
step:401/500 train_loss:4.3680 train_time:107620ms step_avg:275.24ms w_mean:1.000 w_std:0.001 w_min:1.000 w_max:1.250
step:402/500 train_loss:4.4061 train_time:107888ms step_avg:275.22ms
step:403/500 train_loss:4.3726 train_time:108165ms step_avg:275.23ms
step:404/500 train_loss:4.4698 train_time:108440ms step_avg:275.23ms
step:405/500 train_loss:4.2513 train_time:108711ms step_avg:275.22ms
step:406/500 train_loss:4.2950 train_time:108983ms step_avg:275.21ms
step:407/500 train_loss:4.5731 train_time:109252ms step_avg:275.20ms
step:408/500 train_loss:4.3385 train_time:109528ms step_avg:275.20ms
step:409/500 train_loss:4.3330 train_time:109803ms step_avg:275.19ms
step:410/500 train_loss:4.3782 train_time:110072ms step_avg:275.18ms
step:411/500 train_loss:4.2613 train_time:110346ms step_avg:275.18ms
step:412/500 train_loss:4.2751 train_time:110618ms step_avg:275.17ms
step:413/500 train_loss:4.6981 train_time:110890ms step_avg:275.16ms
step:414/500 train_loss:4.1484 train_time:111166ms step_avg:275.16ms
step:415/500 train_loss:4.5114 train_time:111440ms step_avg:275.16ms
step:416/500 train_loss:4.2857 train_time:111713ms step_avg:275.16ms
step:417/500 train_loss:4.2802 train_time:111987ms step_avg:275.15ms
step:418/500 train_loss:4.4640 train_time:112260ms step_avg:275.15ms
step:419/500 train_loss:4.1997 train_time:112531ms step_avg:275.14ms
step:420/500 train_loss:4.2966 train_time:112807ms step_avg:275.14ms
step:421/500 train_loss:4.2665 train_time:113077ms step_avg:275.13ms
step:422/500 train_loss:4.1522 train_time:113350ms step_avg:275.12ms
step:423/500 train_loss:4.2606 train_time:113625ms step_avg:275.12ms
step:424/500 train_loss:4.3763 train_time:113900ms step_avg:275.12ms
step:425/500 train_loss:4.1751 train_time:114171ms step_avg:275.11ms
step:426/500 train_loss:4.3322 train_time:114446ms step_avg:275.11ms
step:427/500 train_loss:4.2188 train_time:114720ms step_avg:275.11ms
step:428/500 train_loss:4.3973 train_time:114991ms step_avg:275.10ms
step:429/500 train_loss:4.3507 train_time:115265ms step_avg:275.10ms
step:430/500 train_loss:4.2645 train_time:115540ms step_avg:275.10ms
step:431/500 train_loss:4.2411 train_time:115813ms step_avg:275.09ms
step:432/500 train_loss:4.1931 train_time:116085ms step_avg:275.08ms
step:433/500 train_loss:4.2724 train_time:116356ms step_avg:275.07ms
step:434/500 train_loss:4.3444 train_time:116630ms step_avg:275.07ms
step:435/500 train_loss:4.2813 train_time:116905ms step_avg:275.07ms
step:436/500 train_loss:4.3279 train_time:117176ms step_avg:275.06ms
step:437/500 train_loss:4.3359 train_time:117450ms step_avg:275.06ms
step:438/500 train_loss:4.2210 train_time:117726ms step_avg:275.06ms
step:439/500 train_loss:4.2382 train_time:118001ms step_avg:275.06ms
step:440/500 train_loss:4.2097 train_time:118271ms step_avg:275.05ms
step:441/500 train_loss:4.3857 train_time:118547ms step_avg:275.05ms
step:442/500 train_loss:4.2885 train_time:118820ms step_avg:275.05ms
step:443/500 train_loss:4.2694 train_time:119091ms step_avg:275.04ms
step:444/500 train_loss:4.1569 train_time:119365ms step_avg:275.03ms
step:445/500 train_loss:4.4206 train_time:119639ms step_avg:275.03ms
step:446/500 train_loss:4.3429 train_time:119910ms step_avg:275.02ms
step:447/500 train_loss:4.3417 train_time:120183ms step_avg:275.02ms
step:448/500 train_loss:4.2575 train_time:120456ms step_avg:275.01ms
step:449/500 train_loss:4.3439 train_time:120729ms step_avg:275.01ms
step:450/500 train_loss:4.1767 train_time:121005ms step_avg:275.01ms
step:451/500 train_loss:4.2266 train_time:121275ms step_avg:275.00ms w_mean:1.000 w_std:0.001 w_min:1.000 w_max:1.250
step:452/500 train_loss:4.1059 train_time:121551ms step_avg:275.00ms
step:453/500 train_loss:4.2084 train_time:121824ms step_avg:275.00ms
step:454/500 train_loss:4.1854 train_time:122096ms step_avg:274.99ms
step:455/500 train_loss:4.1583 train_time:122367ms step_avg:274.98ms
step:456/500 train_loss:4.3755 train_time:122641ms step_avg:274.98ms
step:457/500 train_loss:4.2270 train_time:122914ms step_avg:274.98ms
step:458/500 train_loss:4.3148 train_time:123186ms step_avg:274.97ms
step:459/500 train_loss:4.3506 train_time:123460ms step_avg:274.97ms
step:460/500 train_loss:4.1483 train_time:123733ms step_avg:274.96ms
step:461/500 train_loss:4.3229 train_time:124008ms step_avg:274.96ms
step:462/500 train_loss:4.2255 train_time:124280ms step_avg:274.96ms
step:463/500 train_loss:4.2133 train_time:124551ms step_avg:274.95ms
step:464/500 train_loss:4.2928 train_time:124827ms step_avg:274.95ms
step:465/500 train_loss:4.2321 train_time:125102ms step_avg:274.95ms
step:466/500 train_loss:4.2309 train_time:125372ms step_avg:274.94ms
step:467/500 train_loss:4.3584 train_time:125647ms step_avg:274.94ms
step:468/500 train_loss:4.3647 train_time:125922ms step_avg:274.94ms
step:469/500 train_loss:4.3270 train_time:126192ms step_avg:274.93ms
step:470/500 train_loss:4.2374 train_time:126464ms step_avg:274.92ms
step:471/500 train_loss:4.3181 train_time:126734ms step_avg:274.91ms
step:472/500 train_loss:4.3659 train_time:127009ms step_avg:274.91ms
step:473/500 train_loss:4.2728 train_time:127280ms step_avg:274.90ms
step:474/500 train_loss:4.2481 train_time:127553ms step_avg:274.90ms
step:475/500 train_loss:4.1203 train_time:127828ms step_avg:274.90ms
step:476/500 train_loss:4.5473 train_time:128102ms step_avg:274.90ms
step:477/500 train_loss:4.3027 train_time:128371ms step_avg:274.89ms
step:478/500 train_loss:4.1053 train_time:128646ms step_avg:274.89ms
step:479/500 train_loss:4.3102 train_time:128919ms step_avg:274.88ms
step:480/500 train_loss:4.2909 train_time:129191ms step_avg:274.87ms
step:481/500 train_loss:4.4117 train_time:129465ms step_avg:274.87ms
step:482/500 train_loss:4.2438 train_time:129740ms step_avg:274.87ms
step:483/500 train_loss:4.0637 train_time:130013ms step_avg:274.87ms
step:484/500 train_loss:4.3379 train_time:130283ms step_avg:274.86ms
step:485/500 train_loss:4.1830 train_time:130553ms step_avg:274.85ms
step:486/500 train_loss:4.2110 train_time:130827ms step_avg:274.85ms
step:487/500 train_loss:4.1552 train_time:131104ms step_avg:274.85ms
step:488/500 train_loss:4.1835 train_time:131374ms step_avg:274.84ms
step:489/500 train_loss:4.3924 train_time:131648ms step_avg:274.84ms
step:490/500 train_loss:4.2454 train_time:131923ms step_avg:274.84ms
step:491/500 train_loss:4.1491 train_time:132196ms step_avg:274.84ms
step:492/500 train_loss:4.1534 train_time:132469ms step_avg:274.83ms
step:493/500 train_loss:4.2660 train_time:132743ms step_avg:274.83ms
step:494/500 train_loss:4.1138 train_time:133015ms step_avg:274.82ms
step:495/500 train_loss:4.2594 train_time:133288ms step_avg:274.82ms
step:496/500 train_loss:4.1743 train_time:133562ms step_avg:274.82ms
step:497/500 train_loss:4.1177 train_time:133833ms step_avg:274.81ms
step:498/500 train_loss:4.2715 train_time:134108ms step_avg:274.81ms
step:499/500 train_loss:4.3507 train_time:134380ms step_avg:274.81ms
step:500/500 train_loss:4.4033 train_time:134652ms step_avg:274.80ms
step:500/500 val_loss:4.2518 train_time:134653ms step_avg:274.80ms
