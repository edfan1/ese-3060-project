====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock and keep timing accurate, but skip checkpoint writes to save disk
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        if master_process:
            print("Checkpoint saving disabled; skipping state serialization.")
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 23:13:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   45C    P0             83W /  310W |    2363MiB /  81559MiB |     15%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     19%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   48C    P0             87W /  310W |    2363MiB /  81559MiB |     34%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   44C    P0             85W /  310W |    2363MiB /  81559MiB |     32%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   43C    P0             80W /  310W |    2363MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0             86W /  310W |    2363MiB /  81559MiB |     34%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   52C    P0             88W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   45C    P0             83W /  310W |    2363MiB /  81559MiB |      5%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           85760      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           85761      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           85762      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           85763      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           85764      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           85765      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           85766      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           85767      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 42 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: focal
  clamp: [0.5, 2.0]
  schedule: constant
  focal_gamma: 2.0
  percentile_clamp: [0.05, 0.95]
====================================================================================================
step:0/500 val_loss:15.9787 train_time:284ms step_avg:nanms
step:1/500 train_loss:15.9775 train_time:60827ms step_avg:nanms w_mean:1.000 w_std:0.071 w_min:0.875 w_max:1.129
step:2/500 train_loss:9.3662 train_time:61363ms step_avg:nanms
step:3/500 train_loss:9.1001 train_time:61647ms step_avg:nanms
step:4/500 train_loss:8.7111 train_time:61931ms step_avg:nanms
step:5/500 train_loss:8.2220 train_time:62213ms step_avg:nanms
step:6/500 train_loss:7.8073 train_time:62495ms step_avg:nanms
step:7/500 train_loss:7.4871 train_time:62777ms step_avg:nanms
step:8/500 train_loss:7.6958 train_time:63059ms step_avg:nanms
step:9/500 train_loss:7.3920 train_time:63342ms step_avg:nanms
step:10/500 train_loss:7.2691 train_time:63626ms step_avg:nanms
step:11/500 train_loss:7.2312 train_time:285ms step_avg:nanms
step:12/500 train_loss:7.1910 train_time:568ms step_avg:nanms
step:13/500 train_loss:7.0154 train_time:851ms step_avg:283.65ms
step:14/500 train_loss:7.0860 train_time:1135ms step_avg:283.67ms
step:15/500 train_loss:6.9871 train_time:1419ms step_avg:283.89ms
step:16/500 train_loss:7.0006 train_time:1705ms step_avg:284.09ms
step:17/500 train_loss:6.9254 train_time:1989ms step_avg:284.19ms
step:18/500 train_loss:7.0163 train_time:2271ms step_avg:283.91ms
step:19/500 train_loss:6.8019 train_time:2554ms step_avg:283.82ms
step:20/500 train_loss:6.8429 train_time:2839ms step_avg:283.89ms
step:21/500 train_loss:6.5094 train_time:3125ms step_avg:284.10ms
step:22/500 train_loss:6.8737 train_time:3410ms step_avg:284.14ms
step:23/500 train_loss:7.0786 train_time:3694ms step_avg:284.12ms
step:24/500 train_loss:6.7457 train_time:3979ms step_avg:284.23ms
step:25/500 train_loss:6.8301 train_time:4266ms step_avg:284.42ms
step:26/500 train_loss:6.5857 train_time:4550ms step_avg:284.35ms
step:27/500 train_loss:6.5025 train_time:4832ms step_avg:284.25ms
step:28/500 train_loss:6.6751 train_time:5118ms step_avg:284.36ms
step:29/500 train_loss:6.3539 train_time:5403ms step_avg:284.39ms
step:30/500 train_loss:6.5881 train_time:5689ms step_avg:284.47ms
step:31/500 train_loss:6.4809 train_time:5972ms step_avg:284.39ms
step:32/500 train_loss:6.4130 train_time:6257ms step_avg:284.41ms
step:33/500 train_loss:6.2885 train_time:6546ms step_avg:284.61ms
step:34/500 train_loss:6.5736 train_time:6830ms step_avg:284.60ms
step:35/500 train_loss:6.4946 train_time:7115ms step_avg:284.59ms
step:36/500 train_loss:6.6020 train_time:7402ms step_avg:284.69ms
step:37/500 train_loss:6.5624 train_time:7688ms step_avg:284.76ms
step:38/500 train_loss:6.4275 train_time:7973ms step_avg:284.73ms
step:39/500 train_loss:6.3440 train_time:8257ms step_avg:284.72ms
step:40/500 train_loss:6.4033 train_time:8545ms step_avg:284.82ms
step:41/500 train_loss:6.3018 train_time:8831ms step_avg:284.87ms
step:42/500 train_loss:6.3292 train_time:9116ms step_avg:284.88ms
step:43/500 train_loss:6.2149 train_time:9402ms step_avg:284.91ms
step:44/500 train_loss:6.2913 train_time:9687ms step_avg:284.92ms
step:45/500 train_loss:6.2905 train_time:9972ms step_avg:284.93ms
step:46/500 train_loss:6.4451 train_time:10257ms step_avg:284.92ms
step:47/500 train_loss:6.2821 train_time:10545ms step_avg:284.99ms
step:48/500 train_loss:6.1300 train_time:10832ms step_avg:285.06ms
step:49/500 train_loss:6.3508 train_time:11114ms step_avg:284.98ms
step:50/500 train_loss:6.2193 train_time:11401ms step_avg:285.01ms
step:51/500 train_loss:6.3637 train_time:11684ms step_avg:284.98ms w_mean:1.000 w_std:0.714 w_min:0.154 w_max:2.555
step:52/500 train_loss:6.2304 train_time:11969ms step_avg:284.98ms
step:53/500 train_loss:6.1375 train_time:12254ms step_avg:284.97ms
step:54/500 train_loss:6.1949 train_time:12539ms step_avg:284.97ms
step:55/500 train_loss:6.1757 train_time:12827ms step_avg:285.05ms
step:56/500 train_loss:6.4000 train_time:13113ms step_avg:285.06ms
step:57/500 train_loss:6.1800 train_time:13399ms step_avg:285.09ms
step:58/500 train_loss:6.0083 train_time:13689ms step_avg:285.18ms
step:59/500 train_loss:6.2042 train_time:13974ms step_avg:285.18ms
step:60/500 train_loss:6.1076 train_time:14259ms step_avg:285.19ms
step:61/500 train_loss:6.2227 train_time:14548ms step_avg:285.25ms
step:62/500 train_loss:6.0241 train_time:14833ms step_avg:285.26ms
step:63/500 train_loss:6.1281 train_time:15122ms step_avg:285.31ms
step:64/500 train_loss:6.0764 train_time:15409ms step_avg:285.36ms
step:65/500 train_loss:5.9968 train_time:15694ms step_avg:285.34ms
step:66/500 train_loss:5.9120 train_time:15982ms step_avg:285.40ms
step:67/500 train_loss:6.1234 train_time:16268ms step_avg:285.40ms
step:68/500 train_loss:5.9310 train_time:16553ms step_avg:285.40ms
step:69/500 train_loss:6.2105 train_time:16841ms step_avg:285.44ms
step:70/500 train_loss:5.8596 train_time:17129ms step_avg:285.48ms
step:71/500 train_loss:5.9321 train_time:17414ms step_avg:285.47ms
step:72/500 train_loss:6.0645 train_time:17701ms step_avg:285.50ms
step:73/500 train_loss:6.0462 train_time:17990ms step_avg:285.55ms
step:74/500 train_loss:5.9403 train_time:18274ms step_avg:285.54ms
step:75/500 train_loss:6.0272 train_time:18562ms step_avg:285.56ms
step:76/500 train_loss:6.0211 train_time:18849ms step_avg:285.60ms
step:77/500 train_loss:5.9714 train_time:19134ms step_avg:285.58ms
step:78/500 train_loss:6.0579 train_time:19423ms step_avg:285.63ms
step:79/500 train_loss:6.0568 train_time:19710ms step_avg:285.66ms
step:80/500 train_loss:5.9409 train_time:19997ms step_avg:285.67ms
step:81/500 train_loss:6.0070 train_time:20288ms step_avg:285.75ms
step:82/500 train_loss:5.8194 train_time:20573ms step_avg:285.74ms
step:83/500 train_loss:5.9624 train_time:20860ms step_avg:285.75ms
step:84/500 train_loss:5.9419 train_time:21149ms step_avg:285.80ms
step:85/500 train_loss:5.9114 train_time:21435ms step_avg:285.80ms
step:86/500 train_loss:5.7499 train_time:21725ms step_avg:285.85ms
step:87/500 train_loss:5.9762 train_time:22011ms step_avg:285.86ms
step:88/500 train_loss:5.8588 train_time:22298ms step_avg:285.87ms
step:89/500 train_loss:5.9551 train_time:22587ms step_avg:285.92ms
step:90/500 train_loss:5.9157 train_time:22873ms step_avg:285.92ms
step:91/500 train_loss:5.8431 train_time:23160ms step_avg:285.93ms
step:92/500 train_loss:5.8432 train_time:23450ms step_avg:285.97ms
step:93/500 train_loss:5.9343 train_time:23736ms step_avg:285.98ms
step:94/500 train_loss:5.7749 train_time:24028ms step_avg:286.05ms
step:95/500 train_loss:5.7902 train_time:24314ms step_avg:286.05ms
step:96/500 train_loss:5.7777 train_time:24602ms step_avg:286.07ms
step:97/500 train_loss:5.7552 train_time:24890ms step_avg:286.10ms
step:98/500 train_loss:5.7754 train_time:25177ms step_avg:286.10ms
step:99/500 train_loss:5.7463 train_time:25464ms step_avg:286.11ms
step:100/500 train_loss:5.8133 train_time:25751ms step_avg:286.12ms
step:101/500 train_loss:5.8121 train_time:26036ms step_avg:286.11ms w_mean:1.000 w_std:0.751 w_min:0.137 w_max:2.719
step:102/500 train_loss:5.7061 train_time:26326ms step_avg:286.15ms
step:103/500 train_loss:5.8206 train_time:26613ms step_avg:286.17ms
step:104/500 train_loss:5.7604 train_time:26901ms step_avg:286.18ms
step:105/500 train_loss:5.6470 train_time:27190ms step_avg:286.22ms
step:106/500 train_loss:5.7114 train_time:27476ms step_avg:286.21ms
step:107/500 train_loss:5.9075 train_time:27765ms step_avg:286.23ms
step:108/500 train_loss:5.7021 train_time:28051ms step_avg:286.24ms
step:109/500 train_loss:5.5083 train_time:28339ms step_avg:286.25ms
step:110/500 train_loss:5.6791 train_time:28628ms step_avg:286.28ms
step:111/500 train_loss:5.6651 train_time:28914ms step_avg:286.28ms
step:112/500 train_loss:5.6344 train_time:29203ms step_avg:286.31ms
step:113/500 train_loss:5.7080 train_time:29492ms step_avg:286.33ms
step:114/500 train_loss:5.6861 train_time:29780ms step_avg:286.35ms
step:115/500 train_loss:5.4869 train_time:30068ms step_avg:286.36ms
step:116/500 train_loss:5.7245 train_time:30354ms step_avg:286.36ms
step:117/500 train_loss:5.5152 train_time:30644ms step_avg:286.39ms
step:118/500 train_loss:5.5794 train_time:30931ms step_avg:286.40ms
step:119/500 train_loss:5.6274 train_time:31218ms step_avg:286.40ms
step:120/500 train_loss:5.6628 train_time:31505ms step_avg:286.41ms
step:121/500 train_loss:5.5629 train_time:31793ms step_avg:286.43ms
step:122/500 train_loss:5.4972 train_time:32081ms step_avg:286.44ms
step:123/500 train_loss:5.5338 train_time:32371ms step_avg:286.47ms
step:124/500 train_loss:5.4711 train_time:32659ms step_avg:286.49ms
step:125/500 train_loss:5.7016 train_time:32950ms step_avg:286.52ms
step:125/500 val_loss:5.5824 train_time:32950ms step_avg:286.52ms
step:126/500 train_loss:5.5864 train_time:33223ms step_avg:286.41ms
step:127/500 train_loss:5.5452 train_time:33516ms step_avg:286.46ms
step:128/500 train_loss:5.6485 train_time:33804ms step_avg:286.47ms
step:129/500 train_loss:5.4829 train_time:34091ms step_avg:286.48ms
step:130/500 train_loss:5.7264 train_time:34380ms step_avg:286.50ms
step:131/500 train_loss:5.5681 train_time:34667ms step_avg:286.50ms
step:132/500 train_loss:5.5220 train_time:34957ms step_avg:286.54ms
step:133/500 train_loss:5.5124 train_time:35245ms step_avg:286.54ms
step:134/500 train_loss:5.4949 train_time:35534ms step_avg:286.57ms
step:135/500 train_loss:5.4880 train_time:35822ms step_avg:286.58ms
step:136/500 train_loss:5.5287 train_time:36109ms step_avg:286.58ms
step:137/500 train_loss:5.3743 train_time:36397ms step_avg:286.59ms
step:138/500 train_loss:5.4585 train_time:36684ms step_avg:286.60ms
step:139/500 train_loss:5.4824 train_time:36972ms step_avg:286.60ms
step:140/500 train_loss:5.4351 train_time:37261ms step_avg:286.62ms
step:141/500 train_loss:5.5207 train_time:37547ms step_avg:286.62ms
step:142/500 train_loss:5.4175 train_time:37837ms step_avg:286.65ms
step:143/500 train_loss:5.5144 train_time:38124ms step_avg:286.65ms
step:144/500 train_loss:5.3273 train_time:38413ms step_avg:286.66ms
step:145/500 train_loss:5.4895 train_time:38702ms step_avg:286.68ms
step:146/500 train_loss:5.4031 train_time:38988ms step_avg:286.68ms
step:147/500 train_loss:5.3470 train_time:39278ms step_avg:286.70ms
step:148/500 train_loss:5.4270 train_time:39565ms step_avg:286.71ms
step:149/500 train_loss:5.4151 train_time:39852ms step_avg:286.70ms
step:150/500 train_loss:5.4576 train_time:40141ms step_avg:286.72ms
step:151/500 train_loss:5.4882 train_time:40428ms step_avg:286.73ms w_mean:1.000 w_std:0.794 w_min:0.120 w_max:2.854
step:152/500 train_loss:5.4184 train_time:40717ms step_avg:286.74ms
step:153/500 train_loss:5.3799 train_time:41006ms step_avg:286.75ms
step:154/500 train_loss:5.4392 train_time:41294ms step_avg:286.77ms
step:155/500 train_loss:5.4154 train_time:41583ms step_avg:286.78ms
step:156/500 train_loss:5.3701 train_time:41871ms step_avg:286.79ms
step:157/500 train_loss:5.4118 train_time:42160ms step_avg:286.81ms
step:158/500 train_loss:5.5005 train_time:42447ms step_avg:286.80ms
step:159/500 train_loss:5.3475 train_time:42737ms step_avg:286.82ms
step:160/500 train_loss:5.3432 train_time:43024ms step_avg:286.83ms
step:161/500 train_loss:5.2652 train_time:43311ms step_avg:286.83ms
step:162/500 train_loss:5.3602 train_time:43601ms step_avg:286.85ms
step:163/500 train_loss:5.4183 train_time:43888ms step_avg:286.85ms
step:164/500 train_loss:5.4011 train_time:44178ms step_avg:286.87ms
step:165/500 train_loss:5.2311 train_time:44465ms step_avg:286.87ms
step:166/500 train_loss:5.3565 train_time:44751ms step_avg:286.87ms
step:167/500 train_loss:5.4636 train_time:45042ms step_avg:286.89ms
step:168/500 train_loss:5.3124 train_time:45329ms step_avg:286.89ms
step:169/500 train_loss:5.3360 train_time:45617ms step_avg:286.90ms
step:170/500 train_loss:5.2579 train_time:45904ms step_avg:286.90ms
step:171/500 train_loss:5.1947 train_time:46192ms step_avg:286.91ms
step:172/500 train_loss:5.2868 train_time:46481ms step_avg:286.92ms
step:173/500 train_loss:5.2534 train_time:46769ms step_avg:286.92ms
step:174/500 train_loss:5.3146 train_time:47058ms step_avg:286.94ms
step:175/500 train_loss:5.4197 train_time:47345ms step_avg:286.94ms
step:176/500 train_loss:5.3472 train_time:47632ms step_avg:286.94ms
step:177/500 train_loss:5.1800 train_time:47921ms step_avg:286.95ms
step:178/500 train_loss:5.1741 train_time:48208ms step_avg:286.95ms
step:179/500 train_loss:5.1898 train_time:48497ms step_avg:286.97ms
step:180/500 train_loss:5.2589 train_time:48785ms step_avg:286.97ms
step:181/500 train_loss:5.2216 train_time:49074ms step_avg:286.98ms
step:182/500 train_loss:5.3436 train_time:49361ms step_avg:286.98ms
step:183/500 train_loss:5.2187 train_time:49649ms step_avg:286.99ms
step:184/500 train_loss:5.1913 train_time:49940ms step_avg:287.01ms
step:185/500 train_loss:5.1741 train_time:50226ms step_avg:287.01ms
step:186/500 train_loss:5.3395 train_time:50514ms step_avg:287.01ms
step:187/500 train_loss:5.1789 train_time:50803ms step_avg:287.02ms
step:188/500 train_loss:5.4279 train_time:51091ms step_avg:287.03ms
step:189/500 train_loss:5.2326 train_time:51690ms step_avg:288.77ms
step:190/500 train_loss:5.1683 train_time:52314ms step_avg:290.64ms
step:191/500 train_loss:5.2957 train_time:52602ms step_avg:290.62ms
step:192/500 train_loss:5.1756 train_time:52888ms step_avg:290.60ms
step:193/500 train_loss:5.0634 train_time:53179ms step_avg:290.59ms
step:194/500 train_loss:5.2728 train_time:53464ms step_avg:290.57ms
step:195/500 train_loss:5.1965 train_time:53751ms step_avg:290.54ms
step:196/500 train_loss:5.3890 train_time:54040ms step_avg:290.54ms
step:197/500 train_loss:5.2773 train_time:54326ms step_avg:290.51ms
step:198/500 train_loss:5.1563 train_time:54614ms step_avg:290.50ms
step:199/500 train_loss:5.1669 train_time:54902ms step_avg:290.49ms
step:200/500 train_loss:5.0881 train_time:55190ms step_avg:290.47ms
step:201/500 train_loss:5.1566 train_time:55479ms step_avg:290.47ms w_mean:1.000 w_std:0.829 w_min:0.100 w_max:2.981
step:202/500 train_loss:5.1077 train_time:55765ms step_avg:290.44ms
step:203/500 train_loss:5.2599 train_time:56053ms step_avg:290.43ms
step:204/500 train_loss:5.2463 train_time:56343ms step_avg:290.43ms
step:205/500 train_loss:5.1347 train_time:56626ms step_avg:290.39ms
step:206/500 train_loss:5.3361 train_time:56914ms step_avg:290.38ms
step:207/500 train_loss:5.0054 train_time:57202ms step_avg:290.37ms
step:208/500 train_loss:5.1807 train_time:57488ms step_avg:290.34ms
step:209/500 train_loss:5.1006 train_time:57777ms step_avg:290.34ms
step:210/500 train_loss:5.2765 train_time:58064ms step_avg:290.32ms
step:211/500 train_loss:5.1844 train_time:58352ms step_avg:290.31ms
step:212/500 train_loss:5.1046 train_time:58643ms step_avg:290.31ms
step:213/500 train_loss:5.2219 train_time:58929ms step_avg:290.29ms
step:214/500 train_loss:5.0827 train_time:59217ms step_avg:290.28ms
step:215/500 train_loss:5.1337 train_time:59504ms step_avg:290.27ms
step:216/500 train_loss:5.0555 train_time:59792ms step_avg:290.25ms
step:217/500 train_loss:5.1288 train_time:60081ms step_avg:290.25ms
step:218/500 train_loss:5.1396 train_time:60369ms step_avg:290.24ms
step:219/500 train_loss:5.0614 train_time:60656ms step_avg:290.22ms
step:220/500 train_loss:5.1149 train_time:60944ms step_avg:290.21ms
step:221/500 train_loss:5.1169 train_time:61231ms step_avg:290.20ms
step:222/500 train_loss:5.1528 train_time:61520ms step_avg:290.19ms
step:223/500 train_loss:5.1115 train_time:61807ms step_avg:290.17ms
step:224/500 train_loss:5.1284 train_time:62096ms step_avg:290.17ms
step:225/500 train_loss:5.2039 train_time:62384ms step_avg:290.16ms
step:226/500 train_loss:4.9755 train_time:62671ms step_avg:290.14ms
step:227/500 train_loss:5.0075 train_time:62960ms step_avg:290.14ms
step:228/500 train_loss:5.0078 train_time:63247ms step_avg:290.13ms
step:229/500 train_loss:5.1241 train_time:63538ms step_avg:290.13ms
step:230/500 train_loss:5.0292 train_time:63825ms step_avg:290.12ms
step:231/500 train_loss:5.1231 train_time:64113ms step_avg:290.10ms
step:232/500 train_loss:5.0342 train_time:64402ms step_avg:290.10ms
step:233/500 train_loss:4.9521 train_time:64696ms step_avg:290.12ms
step:234/500 train_loss:5.1854 train_time:64983ms step_avg:290.10ms
step:235/500 train_loss:5.0006 train_time:65271ms step_avg:290.09ms
step:236/500 train_loss:4.9669 train_time:65560ms step_avg:290.09ms
step:237/500 train_loss:5.1938 train_time:65846ms step_avg:290.07ms
step:238/500 train_loss:5.0680 train_time:66137ms step_avg:290.07ms
step:239/500 train_loss:5.0106 train_time:66425ms step_avg:290.07ms
step:240/500 train_loss:5.1304 train_time:66712ms step_avg:290.05ms
step:241/500 train_loss:5.1104 train_time:67003ms step_avg:290.06ms
step:242/500 train_loss:5.0233 train_time:67290ms step_avg:290.04ms
step:243/500 train_loss:5.1784 train_time:67580ms step_avg:290.04ms
step:244/500 train_loss:5.0056 train_time:67867ms step_avg:290.03ms
step:245/500 train_loss:5.0404 train_time:68156ms step_avg:290.02ms
step:246/500 train_loss:5.0800 train_time:68442ms step_avg:290.01ms
step:247/500 train_loss:5.0549 train_time:68729ms step_avg:289.99ms
step:248/500 train_loss:5.0060 train_time:69017ms step_avg:289.99ms
step:249/500 train_loss:5.1905 train_time:69304ms step_avg:289.98ms
step:250/500 train_loss:4.9257 train_time:69591ms step_avg:289.96ms
step:250/500 val_loss:5.0139 train_time:69592ms step_avg:289.97ms
step:251/500 train_loss:4.9569 train_time:69866ms step_avg:289.90ms w_mean:1.000 w_std:0.857 w_min:0.083 w_max:3.075
step:252/500 train_loss:5.0839 train_time:70160ms step_avg:289.92ms
step:253/500 train_loss:5.0478 train_time:70448ms step_avg:289.91ms
step:254/500 train_loss:4.9937 train_time:70736ms step_avg:289.90ms
step:255/500 train_loss:4.9665 train_time:71024ms step_avg:289.89ms
step:256/500 train_loss:5.0954 train_time:71310ms step_avg:289.88ms
step:257/500 train_loss:5.0549 train_time:71599ms step_avg:289.87ms
step:258/500 train_loss:5.0428 train_time:71886ms step_avg:289.86ms
step:259/500 train_loss:4.9582 train_time:72173ms step_avg:289.85ms
step:260/500 train_loss:4.9799 train_time:72463ms step_avg:289.85ms
step:261/500 train_loss:5.0334 train_time:72748ms step_avg:289.83ms
step:262/500 train_loss:5.0328 train_time:73038ms step_avg:289.83ms
step:263/500 train_loss:4.9697 train_time:73327ms step_avg:289.83ms
step:264/500 train_loss:4.9144 train_time:73616ms step_avg:289.83ms
step:265/500 train_loss:4.9640 train_time:73903ms step_avg:289.81ms
step:266/500 train_loss:4.8576 train_time:74191ms step_avg:289.81ms
step:267/500 train_loss:4.8670 train_time:74482ms step_avg:289.81ms
step:268/500 train_loss:4.9454 train_time:74769ms step_avg:289.80ms
step:269/500 train_loss:4.8605 train_time:75058ms step_avg:289.80ms
step:270/500 train_loss:4.8810 train_time:75346ms step_avg:289.79ms
step:271/500 train_loss:5.0320 train_time:75634ms step_avg:289.79ms
step:272/500 train_loss:5.0143 train_time:75923ms step_avg:289.78ms
step:273/500 train_loss:4.8595 train_time:76210ms step_avg:289.77ms
step:274/500 train_loss:4.9224 train_time:76498ms step_avg:289.77ms
step:275/500 train_loss:5.0137 train_time:76786ms step_avg:289.76ms
step:276/500 train_loss:5.0201 train_time:77073ms step_avg:289.75ms
step:277/500 train_loss:5.2179 train_time:77360ms step_avg:289.74ms
step:278/500 train_loss:4.9754 train_time:77646ms step_avg:289.72ms
step:279/500 train_loss:5.1122 train_time:77935ms step_avg:289.72ms
step:280/500 train_loss:4.9528 train_time:78223ms step_avg:289.71ms
step:281/500 train_loss:5.0069 train_time:78511ms step_avg:289.71ms
step:282/500 train_loss:4.9163 train_time:78801ms step_avg:289.71ms
step:283/500 train_loss:5.0396 train_time:79087ms step_avg:289.69ms
step:284/500 train_loss:4.8584 train_time:79376ms step_avg:289.69ms
step:285/500 train_loss:5.0205 train_time:79663ms step_avg:289.68ms
step:286/500 train_loss:5.0044 train_time:79950ms step_avg:289.68ms
step:287/500 train_loss:5.0286 train_time:80241ms step_avg:289.68ms
step:288/500 train_loss:4.9155 train_time:80528ms step_avg:289.67ms
step:289/500 train_loss:4.9590 train_time:80822ms step_avg:289.68ms
step:290/500 train_loss:4.8295 train_time:81103ms step_avg:289.65ms
step:291/500 train_loss:4.8388 train_time:81390ms step_avg:289.65ms
step:292/500 train_loss:4.9631 train_time:81682ms step_avg:289.65ms
step:293/500 train_loss:4.8623 train_time:81968ms step_avg:289.64ms
step:294/500 train_loss:4.8943 train_time:82256ms step_avg:289.63ms
step:295/500 train_loss:4.9292 train_time:82544ms step_avg:289.63ms
step:296/500 train_loss:4.8000 train_time:82832ms step_avg:289.62ms
step:297/500 train_loss:4.7929 train_time:83122ms step_avg:289.62ms
step:298/500 train_loss:4.8219 train_time:83408ms step_avg:289.61ms
step:299/500 train_loss:4.9001 train_time:83695ms step_avg:289.60ms
step:300/500 train_loss:4.8040 train_time:83983ms step_avg:289.60ms
step:301/500 train_loss:4.9679 train_time:84270ms step_avg:289.59ms w_mean:1.000 w_std:0.852 w_min:0.085 w_max:3.067
step:302/500 train_loss:4.9364 train_time:84559ms step_avg:289.58ms
step:303/500 train_loss:4.8586 train_time:84845ms step_avg:289.57ms
step:304/500 train_loss:4.9242 train_time:85132ms step_avg:289.57ms
step:305/500 train_loss:4.9131 train_time:85422ms step_avg:289.57ms
step:306/500 train_loss:5.3355 train_time:85707ms step_avg:289.55ms
step:307/500 train_loss:4.8836 train_time:85997ms step_avg:289.55ms
step:308/500 train_loss:4.7815 train_time:86285ms step_avg:289.55ms
step:309/500 train_loss:4.9638 train_time:86572ms step_avg:289.54ms
step:310/500 train_loss:4.7864 train_time:86861ms step_avg:289.54ms
step:311/500 train_loss:4.9744 train_time:87147ms step_avg:289.52ms
step:312/500 train_loss:4.9369 train_time:87436ms step_avg:289.52ms
step:313/500 train_loss:4.8135 train_time:87723ms step_avg:289.52ms
step:314/500 train_loss:4.9622 train_time:88011ms step_avg:289.51ms
step:315/500 train_loss:5.0603 train_time:88300ms step_avg:289.51ms
step:316/500 train_loss:4.9272 train_time:88585ms step_avg:289.49ms
step:317/500 train_loss:4.8252 train_time:88873ms step_avg:289.49ms
step:318/500 train_loss:4.8342 train_time:89162ms step_avg:289.49ms
step:319/500 train_loss:4.8334 train_time:89449ms step_avg:289.48ms
step:320/500 train_loss:4.7904 train_time:89739ms step_avg:289.48ms
step:321/500 train_loss:4.8784 train_time:90026ms step_avg:289.47ms
step:322/500 train_loss:4.8801 train_time:90315ms step_avg:289.47ms
step:323/500 train_loss:4.8505 train_time:90603ms step_avg:289.47ms
step:324/500 train_loss:4.9265 train_time:90890ms step_avg:289.46ms
step:325/500 train_loss:4.9211 train_time:91181ms step_avg:289.46ms
step:326/500 train_loss:4.9712 train_time:91467ms step_avg:289.45ms
step:327/500 train_loss:4.8323 train_time:91754ms step_avg:289.44ms
step:328/500 train_loss:5.2364 train_time:92043ms step_avg:289.44ms
step:329/500 train_loss:4.9843 train_time:92330ms step_avg:289.44ms
step:330/500 train_loss:4.8047 train_time:92621ms step_avg:289.44ms
step:331/500 train_loss:4.7609 train_time:92909ms step_avg:289.43ms
step:332/500 train_loss:4.8995 train_time:93196ms step_avg:289.43ms
step:333/500 train_loss:4.8297 train_time:93484ms step_avg:289.42ms
step:334/500 train_loss:4.8084 train_time:93771ms step_avg:289.42ms
step:335/500 train_loss:4.7812 train_time:94061ms step_avg:289.42ms
step:336/500 train_loss:4.9620 train_time:94348ms step_avg:289.41ms
step:337/500 train_loss:4.9065 train_time:94637ms step_avg:289.41ms
step:338/500 train_loss:5.3834 train_time:94923ms step_avg:289.40ms
step:339/500 train_loss:4.8891 train_time:95212ms step_avg:289.40ms
step:340/500 train_loss:4.8301 train_time:95501ms step_avg:289.40ms
step:341/500 train_loss:4.8250 train_time:95788ms step_avg:289.39ms
step:342/500 train_loss:4.7698 train_time:96077ms step_avg:289.39ms
step:343/500 train_loss:4.7558 train_time:96364ms step_avg:289.38ms
step:344/500 train_loss:4.8255 train_time:96651ms step_avg:289.37ms
step:345/500 train_loss:4.8873 train_time:96942ms step_avg:289.38ms
step:346/500 train_loss:4.8007 train_time:97228ms step_avg:289.37ms
step:347/500 train_loss:4.7560 train_time:97520ms step_avg:289.38ms
step:348/500 train_loss:4.8111 train_time:97808ms step_avg:289.37ms
step:349/500 train_loss:4.7984 train_time:98096ms step_avg:289.37ms
step:350/500 train_loss:4.7222 train_time:98384ms step_avg:289.37ms
step:351/500 train_loss:4.4350 train_time:98672ms step_avg:289.36ms w_mean:1.000 w_std:0.904 w_min:0.078 w_max:3.324
step:352/500 train_loss:4.7088 train_time:98960ms step_avg:289.36ms
step:353/500 train_loss:5.0165 train_time:99247ms step_avg:289.35ms
step:354/500 train_loss:4.6135 train_time:99535ms step_avg:289.35ms
step:355/500 train_loss:4.8122 train_time:99823ms step_avg:289.34ms
step:356/500 train_loss:4.7513 train_time:100116ms step_avg:289.35ms
step:357/500 train_loss:4.8245 train_time:100404ms step_avg:289.35ms
step:358/500 train_loss:4.8808 train_time:100693ms step_avg:289.35ms
step:359/500 train_loss:4.7427 train_time:100984ms step_avg:289.35ms
step:360/500 train_loss:5.0621 train_time:101271ms step_avg:289.35ms
step:361/500 train_loss:4.5336 train_time:101560ms step_avg:289.34ms
step:362/500 train_loss:4.9385 train_time:101846ms step_avg:289.34ms
step:363/500 train_loss:4.8530 train_time:102133ms step_avg:289.33ms
step:364/500 train_loss:4.7433 train_time:102422ms step_avg:289.33ms
step:365/500 train_loss:4.6913 train_time:102709ms step_avg:289.32ms
step:366/500 train_loss:4.8344 train_time:102998ms step_avg:289.32ms
step:367/500 train_loss:4.7607 train_time:103286ms step_avg:289.32ms
step:368/500 train_loss:4.7496 train_time:103573ms step_avg:289.31ms
step:369/500 train_loss:4.7543 train_time:103861ms step_avg:289.31ms
step:370/500 train_loss:4.6676 train_time:104148ms step_avg:289.30ms
step:371/500 train_loss:4.7850 train_time:104437ms step_avg:289.30ms
step:372/500 train_loss:4.7536 train_time:104726ms step_avg:289.30ms
step:373/500 train_loss:4.6205 train_time:105014ms step_avg:289.29ms
step:374/500 train_loss:4.7820 train_time:105303ms step_avg:289.29ms
step:375/500 train_loss:4.7564 train_time:105589ms step_avg:289.29ms
step:375/500 val_loss:4.7531 train_time:105590ms step_avg:289.29ms
step:376/500 train_loss:4.7368 train_time:105862ms step_avg:289.24ms
step:377/500 train_loss:4.8082 train_time:106155ms step_avg:289.25ms
step:378/500 train_loss:4.6962 train_time:106755ms step_avg:290.10ms
step:379/500 train_loss:4.7188 train_time:107042ms step_avg:290.09ms
step:380/500 train_loss:4.8200 train_time:107638ms step_avg:290.91ms
step:381/500 train_loss:4.8226 train_time:107922ms step_avg:290.90ms
step:382/500 train_loss:4.7966 train_time:108209ms step_avg:290.88ms
step:383/500 train_loss:4.7958 train_time:108498ms step_avg:290.88ms
step:384/500 train_loss:4.6728 train_time:108784ms step_avg:290.87ms
step:385/500 train_loss:4.7650 train_time:109073ms step_avg:290.86ms
step:386/500 train_loss:4.6923 train_time:109361ms step_avg:290.85ms
step:387/500 train_loss:4.8058 train_time:109647ms step_avg:290.84ms
step:388/500 train_loss:5.0045 train_time:109937ms step_avg:290.84ms
step:389/500 train_loss:4.7072 train_time:110224ms step_avg:290.83ms
step:390/500 train_loss:4.6688 train_time:110511ms step_avg:290.82ms
step:391/500 train_loss:4.7775 train_time:110801ms step_avg:290.81ms
step:392/500 train_loss:4.7246 train_time:111087ms step_avg:290.80ms
step:393/500 train_loss:4.8051 train_time:111374ms step_avg:290.79ms
step:394/500 train_loss:4.6573 train_time:111659ms step_avg:290.78ms
step:395/500 train_loss:4.7740 train_time:111946ms step_avg:290.77ms
step:396/500 train_loss:4.6030 train_time:112237ms step_avg:290.77ms
step:397/500 train_loss:4.7154 train_time:112522ms step_avg:290.76ms
step:398/500 train_loss:4.8385 train_time:112810ms step_avg:290.75ms
step:399/500 train_loss:4.7516 train_time:113100ms step_avg:290.75ms
step:400/500 train_loss:4.7012 train_time:113385ms step_avg:290.73ms
step:401/500 train_loss:4.7632 train_time:113674ms step_avg:290.73ms w_mean:1.000 w_std:0.881 w_min:0.074 w_max:3.175
step:402/500 train_loss:4.7802 train_time:113960ms step_avg:290.71ms
step:403/500 train_loss:4.7687 train_time:114247ms step_avg:290.70ms
step:404/500 train_loss:4.8352 train_time:114538ms step_avg:290.71ms
step:405/500 train_loss:4.6587 train_time:114824ms step_avg:290.69ms
step:406/500 train_loss:4.6803 train_time:115112ms step_avg:290.69ms
step:407/500 train_loss:4.9313 train_time:115401ms step_avg:290.68ms
step:408/500 train_loss:4.7239 train_time:115688ms step_avg:290.67ms
step:409/500 train_loss:4.7079 train_time:115978ms step_avg:290.67ms
step:410/500 train_loss:4.7567 train_time:116264ms step_avg:290.66ms
step:411/500 train_loss:4.6586 train_time:116551ms step_avg:290.65ms
step:412/500 train_loss:4.6783 train_time:116841ms step_avg:290.65ms
step:413/500 train_loss:5.0589 train_time:117128ms step_avg:290.64ms
step:414/500 train_loss:4.5661 train_time:117417ms step_avg:290.64ms
step:415/500 train_loss:4.8763 train_time:117704ms step_avg:290.63ms
step:416/500 train_loss:4.6818 train_time:117991ms step_avg:290.62ms
step:417/500 train_loss:4.6728 train_time:118281ms step_avg:290.62ms
step:418/500 train_loss:4.8233 train_time:118567ms step_avg:290.60ms
step:419/500 train_loss:4.5937 train_time:118856ms step_avg:290.60ms
step:420/500 train_loss:4.6774 train_time:119144ms step_avg:290.60ms
step:421/500 train_loss:4.6875 train_time:119431ms step_avg:290.59ms
step:422/500 train_loss:4.5610 train_time:119719ms step_avg:290.58ms
step:423/500 train_loss:4.6452 train_time:120005ms step_avg:290.57ms
step:424/500 train_loss:4.7518 train_time:120295ms step_avg:290.57ms
step:425/500 train_loss:4.6063 train_time:120582ms step_avg:290.56ms
step:426/500 train_loss:4.7546 train_time:120868ms step_avg:290.55ms
step:427/500 train_loss:4.6264 train_time:121155ms step_avg:290.54ms
step:428/500 train_loss:4.7664 train_time:121443ms step_avg:290.53ms
step:429/500 train_loss:4.7508 train_time:121730ms step_avg:290.52ms
step:430/500 train_loss:4.6463 train_time:122017ms step_avg:290.52ms
step:431/500 train_loss:4.6371 train_time:122304ms step_avg:290.51ms
step:432/500 train_loss:4.6133 train_time:122593ms step_avg:290.51ms
step:433/500 train_loss:4.6728 train_time:122882ms step_avg:290.50ms
step:434/500 train_loss:4.7509 train_time:123169ms step_avg:290.49ms
step:435/500 train_loss:4.6850 train_time:123459ms step_avg:290.49ms
step:436/500 train_loss:4.7190 train_time:123745ms step_avg:290.48ms
step:437/500 train_loss:4.7288 train_time:124034ms step_avg:290.48ms
step:438/500 train_loss:4.6376 train_time:124321ms step_avg:290.47ms
step:439/500 train_loss:4.6434 train_time:124609ms step_avg:290.46ms
step:440/500 train_loss:4.5724 train_time:124900ms step_avg:290.46ms
step:441/500 train_loss:4.7703 train_time:125186ms step_avg:290.45ms
step:442/500 train_loss:4.6984 train_time:125476ms step_avg:290.45ms
step:443/500 train_loss:4.6569 train_time:125762ms step_avg:290.44ms
step:444/500 train_loss:4.5717 train_time:126050ms step_avg:290.44ms
step:445/500 train_loss:4.7897 train_time:126341ms step_avg:290.44ms
step:446/500 train_loss:4.7265 train_time:126627ms step_avg:290.43ms
step:447/500 train_loss:4.7286 train_time:126916ms step_avg:290.43ms
step:448/500 train_loss:4.6578 train_time:127204ms step_avg:290.42ms
step:449/500 train_loss:4.7211 train_time:127493ms step_avg:290.42ms
step:450/500 train_loss:4.5773 train_time:127781ms step_avg:290.41ms
step:451/500 train_loss:4.6107 train_time:128069ms step_avg:290.41ms w_mean:1.000 w_std:0.896 w_min:0.076 w_max:3.237
step:452/500 train_loss:4.5216 train_time:128358ms step_avg:290.40ms
step:453/500 train_loss:4.6145 train_time:128645ms step_avg:290.40ms
step:454/500 train_loss:4.5839 train_time:128935ms step_avg:290.39ms
step:455/500 train_loss:4.5818 train_time:129221ms step_avg:290.39ms
step:456/500 train_loss:4.7443 train_time:129510ms step_avg:290.38ms
step:457/500 train_loss:4.6122 train_time:129800ms step_avg:290.38ms
step:458/500 train_loss:4.7017 train_time:130086ms step_avg:290.37ms
step:459/500 train_loss:4.7396 train_time:130376ms step_avg:290.37ms
step:460/500 train_loss:4.5571 train_time:130663ms step_avg:290.36ms
step:461/500 train_loss:4.7084 train_time:130950ms step_avg:290.36ms
step:462/500 train_loss:4.6410 train_time:131241ms step_avg:290.36ms
step:463/500 train_loss:4.5943 train_time:131528ms step_avg:290.35ms
step:464/500 train_loss:4.6991 train_time:131817ms step_avg:290.35ms
step:465/500 train_loss:4.6263 train_time:132104ms step_avg:290.34ms
step:466/500 train_loss:4.6351 train_time:132393ms step_avg:290.34ms
step:467/500 train_loss:4.7490 train_time:132682ms step_avg:290.33ms
step:468/500 train_loss:4.7574 train_time:132969ms step_avg:290.33ms
step:469/500 train_loss:4.7089 train_time:133258ms step_avg:290.32ms
step:470/500 train_loss:4.6241 train_time:133545ms step_avg:290.31ms
step:471/500 train_loss:4.7189 train_time:133833ms step_avg:290.31ms
step:472/500 train_loss:4.7646 train_time:134121ms step_avg:290.30ms
step:473/500 train_loss:4.6716 train_time:134408ms step_avg:290.30ms
step:474/500 train_loss:4.6403 train_time:134697ms step_avg:290.30ms
step:475/500 train_loss:4.5380 train_time:134984ms step_avg:290.29ms
step:476/500 train_loss:4.9249 train_time:135273ms step_avg:290.29ms
step:477/500 train_loss:4.6851 train_time:135565ms step_avg:290.29ms
step:478/500 train_loss:4.5274 train_time:135851ms step_avg:290.28ms
step:479/500 train_loss:4.6824 train_time:136142ms step_avg:290.28ms
step:480/500 train_loss:4.6835 train_time:136429ms step_avg:290.27ms
step:481/500 train_loss:4.7905 train_time:136718ms step_avg:290.27ms
step:482/500 train_loss:4.6490 train_time:137005ms step_avg:290.26ms
step:483/500 train_loss:4.4823 train_time:137293ms step_avg:290.26ms
step:484/500 train_loss:4.7280 train_time:137583ms step_avg:290.26ms
step:485/500 train_loss:4.6032 train_time:137870ms step_avg:290.25ms
step:486/500 train_loss:4.6193 train_time:138159ms step_avg:290.25ms
step:487/500 train_loss:4.5748 train_time:138445ms step_avg:290.24ms
step:488/500 train_loss:4.5812 train_time:138735ms step_avg:290.24ms
step:489/500 train_loss:4.7758 train_time:139022ms step_avg:290.23ms
step:490/500 train_loss:4.6557 train_time:139310ms step_avg:290.23ms
step:491/500 train_loss:4.5615 train_time:139601ms step_avg:290.23ms
step:492/500 train_loss:4.5604 train_time:139888ms step_avg:290.22ms
step:493/500 train_loss:4.6677 train_time:140177ms step_avg:290.22ms
step:494/500 train_loss:4.5161 train_time:140464ms step_avg:290.22ms
step:495/500 train_loss:4.6595 train_time:140754ms step_avg:290.21ms
step:496/500 train_loss:4.5897 train_time:141041ms step_avg:290.21ms
step:497/500 train_loss:4.5684 train_time:141327ms step_avg:290.20ms
step:498/500 train_loss:4.6591 train_time:141617ms step_avg:290.20ms
step:499/500 train_loss:4.7461 train_time:141905ms step_avg:290.19ms
step:500/500 train_loss:4.8077 train_time:142192ms step_avg:290.19ms
step:500/500 val_loss:4.6509 train_time:142193ms step_avg:290.19ms
