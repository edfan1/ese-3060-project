====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # also save the seed for reproducibility
        if args.seed is not None:
            log['seed'] = args.seed
        # save token weighting config
        if tw_config.enabled:
            log['tw_config'] = {
                'function': tw_config.function,
                'clamp_min': tw_config.clamp_min,
                'clamp_max': tw_config.clamp_max,
                'schedule': tw_config.schedule,
            }
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 21:26:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             85W /  310W |    2363MiB /  81559MiB |     30%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   49C    P0             88W /  310W |    2363MiB /  81559MiB |     11%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             86W /  310W |    2363MiB /  81559MiB |     30%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   43C    P0             81W /  310W |    2363MiB /  81559MiB |     34%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0             85W /  310W |    2363MiB /  81559MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   52C    P0             88W /  310W |    2363MiB /  81559MiB |     28%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   45C    P0             83W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           63832      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           63833      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           63834      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           63835      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           63836      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           63837      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           63838      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           63839      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 456 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: sqrt
  clamp: [0.7, 1.5]
  schedule: constant
====================================================================================================
step:0/500 val_loss:16.0011 train_time:269ms step_avg:nanms
step:1/500 train_loss:15.9963 train_time:72876ms step_avg:nanms w_mean:1.000 w_std:0.021 w_min:0.700 w_max:1.083
step:2/500 train_loss:9.3789 train_time:73277ms step_avg:nanms
step:3/500 train_loss:8.9908 train_time:73557ms step_avg:nanms
step:4/500 train_loss:8.7342 train_time:73838ms step_avg:nanms
step:5/500 train_loss:8.2279 train_time:74119ms step_avg:nanms
step:6/500 train_loss:7.7096 train_time:74400ms step_avg:nanms
step:7/500 train_loss:7.4071 train_time:74681ms step_avg:nanms
step:8/500 train_loss:7.5725 train_time:74962ms step_avg:nanms
step:9/500 train_loss:7.3048 train_time:75241ms step_avg:nanms
step:10/500 train_loss:7.0859 train_time:75521ms step_avg:nanms
step:11/500 train_loss:7.0481 train_time:280ms step_avg:nanms
step:12/500 train_loss:6.9876 train_time:561ms step_avg:nanms
step:13/500 train_loss:6.8089 train_time:838ms step_avg:279.37ms
step:14/500 train_loss:6.8155 train_time:1119ms step_avg:279.84ms
step:15/500 train_loss:6.7982 train_time:1401ms step_avg:280.17ms
step:16/500 train_loss:6.7099 train_time:1676ms step_avg:279.31ms
step:17/500 train_loss:6.7520 train_time:1958ms step_avg:279.77ms
step:18/500 train_loss:6.7376 train_time:2239ms step_avg:279.83ms
step:19/500 train_loss:6.5943 train_time:2520ms step_avg:279.95ms
step:20/500 train_loss:6.5806 train_time:2800ms step_avg:280.00ms
step:21/500 train_loss:6.2593 train_time:3081ms step_avg:280.07ms
step:22/500 train_loss:6.6355 train_time:3362ms step_avg:280.16ms
step:23/500 train_loss:6.8807 train_time:3643ms step_avg:280.24ms
step:24/500 train_loss:6.5109 train_time:3923ms step_avg:280.20ms
step:25/500 train_loss:6.6384 train_time:4203ms step_avg:280.18ms
step:26/500 train_loss:6.3489 train_time:4482ms step_avg:280.14ms
step:27/500 train_loss:6.2859 train_time:4763ms step_avg:280.20ms
step:28/500 train_loss:6.4326 train_time:5043ms step_avg:280.14ms
step:29/500 train_loss:6.1155 train_time:5323ms step_avg:280.15ms
step:30/500 train_loss:6.3799 train_time:5603ms step_avg:280.13ms
step:31/500 train_loss:6.2391 train_time:5882ms step_avg:280.11ms
step:32/500 train_loss:6.2055 train_time:6163ms step_avg:280.14ms
step:33/500 train_loss:6.0126 train_time:6443ms step_avg:280.13ms
step:34/500 train_loss:6.3603 train_time:6724ms step_avg:280.18ms
step:35/500 train_loss:6.2454 train_time:7005ms step_avg:280.19ms
step:36/500 train_loss:6.4271 train_time:7284ms step_avg:280.16ms
step:37/500 train_loss:6.3286 train_time:7564ms step_avg:280.15ms
step:38/500 train_loss:6.2372 train_time:7844ms step_avg:280.14ms
step:39/500 train_loss:6.1119 train_time:8123ms step_avg:280.11ms
step:40/500 train_loss:6.1870 train_time:8403ms step_avg:280.10ms
step:41/500 train_loss:6.0756 train_time:8683ms step_avg:280.09ms
step:42/500 train_loss:6.1186 train_time:8963ms step_avg:280.08ms
step:43/500 train_loss:5.9887 train_time:9243ms step_avg:280.10ms
step:44/500 train_loss:6.0696 train_time:9524ms step_avg:280.11ms
step:45/500 train_loss:6.0611 train_time:9803ms step_avg:280.09ms
step:46/500 train_loss:6.2369 train_time:10082ms step_avg:280.07ms
step:47/500 train_loss:6.0446 train_time:10363ms step_avg:280.09ms
step:48/500 train_loss:5.8902 train_time:10643ms step_avg:280.08ms
step:49/500 train_loss:6.1259 train_time:10924ms step_avg:280.10ms
step:50/500 train_loss:6.0001 train_time:11203ms step_avg:280.08ms
step:51/500 train_loss:6.1383 train_time:11483ms step_avg:280.07ms w_mean:1.000 w_std:0.231 w_min:0.706 w_max:1.514
step:52/500 train_loss:6.0193 train_time:11764ms step_avg:280.09ms
step:53/500 train_loss:5.8619 train_time:12043ms step_avg:280.07ms
step:54/500 train_loss:5.9900 train_time:12323ms step_avg:280.07ms
step:55/500 train_loss:5.9139 train_time:12604ms step_avg:280.09ms
step:56/500 train_loss:6.2087 train_time:12884ms step_avg:280.08ms
step:57/500 train_loss:5.9173 train_time:13163ms step_avg:280.07ms
step:58/500 train_loss:5.7819 train_time:13443ms step_avg:280.06ms
step:59/500 train_loss:5.9561 train_time:13723ms step_avg:280.06ms
step:60/500 train_loss:5.8706 train_time:14004ms step_avg:280.07ms
step:61/500 train_loss:5.9890 train_time:14283ms step_avg:280.06ms
step:62/500 train_loss:5.7672 train_time:14564ms step_avg:280.07ms
step:63/500 train_loss:5.8897 train_time:14843ms step_avg:280.06ms
step:64/500 train_loss:5.8398 train_time:15123ms step_avg:280.06ms
step:65/500 train_loss:5.8314 train_time:15405ms step_avg:280.10ms
step:66/500 train_loss:5.6739 train_time:15680ms step_avg:280.00ms
step:67/500 train_loss:5.8579 train_time:15963ms step_avg:280.04ms
step:68/500 train_loss:5.7008 train_time:16243ms step_avg:280.06ms
step:69/500 train_loss:5.9574 train_time:16523ms step_avg:280.06ms
step:70/500 train_loss:5.6196 train_time:16803ms step_avg:280.06ms
step:71/500 train_loss:5.6566 train_time:17084ms step_avg:280.06ms
step:72/500 train_loss:5.8444 train_time:17365ms step_avg:280.07ms
step:73/500 train_loss:5.7992 train_time:17644ms step_avg:280.06ms
step:74/500 train_loss:5.6874 train_time:17924ms step_avg:280.06ms
step:75/500 train_loss:5.8003 train_time:18204ms step_avg:280.06ms
step:76/500 train_loss:5.7591 train_time:18484ms step_avg:280.06ms
step:77/500 train_loss:5.7239 train_time:18765ms step_avg:280.07ms
step:78/500 train_loss:5.8151 train_time:19044ms step_avg:280.05ms
step:79/500 train_loss:5.8136 train_time:19323ms step_avg:280.04ms
step:80/500 train_loss:5.7096 train_time:19603ms step_avg:280.05ms
step:81/500 train_loss:5.7651 train_time:19883ms step_avg:280.05ms
step:82/500 train_loss:5.5644 train_time:20163ms step_avg:280.05ms
step:83/500 train_loss:5.7121 train_time:20444ms step_avg:280.05ms
step:84/500 train_loss:5.6927 train_time:20723ms step_avg:280.05ms
step:85/500 train_loss:5.6507 train_time:21004ms step_avg:280.05ms
step:86/500 train_loss:5.5135 train_time:21284ms step_avg:280.05ms
step:87/500 train_loss:5.7358 train_time:21564ms step_avg:280.05ms
step:88/500 train_loss:5.6188 train_time:21843ms step_avg:280.04ms
step:89/500 train_loss:5.7008 train_time:22124ms step_avg:280.05ms
step:90/500 train_loss:5.6525 train_time:22404ms step_avg:280.05ms
step:91/500 train_loss:5.5994 train_time:22684ms step_avg:280.05ms
step:92/500 train_loss:5.5775 train_time:22964ms step_avg:280.05ms
step:93/500 train_loss:5.6953 train_time:23244ms step_avg:280.04ms
step:94/500 train_loss:5.5303 train_time:23523ms step_avg:280.04ms
step:95/500 train_loss:5.5278 train_time:23804ms step_avg:280.05ms
step:96/500 train_loss:5.5420 train_time:24084ms step_avg:280.04ms
step:97/500 train_loss:5.4673 train_time:24363ms step_avg:280.04ms
step:98/500 train_loss:5.5369 train_time:24643ms step_avg:280.04ms
step:99/500 train_loss:5.4593 train_time:24924ms step_avg:280.05ms
step:100/500 train_loss:5.5793 train_time:25204ms step_avg:280.05ms
step:101/500 train_loss:5.5460 train_time:25484ms step_avg:280.04ms w_mean:1.000 w_std:0.242 w_min:0.706 w_max:1.514
step:102/500 train_loss:5.4498 train_time:25764ms step_avg:280.04ms
step:103/500 train_loss:5.5517 train_time:26044ms step_avg:280.04ms
step:104/500 train_loss:5.5165 train_time:26323ms step_avg:280.03ms
step:105/500 train_loss:5.3452 train_time:26603ms step_avg:280.03ms
step:106/500 train_loss:5.4716 train_time:26884ms step_avg:280.04ms
step:107/500 train_loss:5.6325 train_time:27164ms step_avg:280.05ms
step:108/500 train_loss:5.4671 train_time:27444ms step_avg:280.04ms
step:109/500 train_loss:5.1943 train_time:27724ms step_avg:280.04ms
step:110/500 train_loss:5.4271 train_time:28004ms step_avg:280.04ms
step:111/500 train_loss:5.3686 train_time:28284ms step_avg:280.04ms
step:112/500 train_loss:5.3666 train_time:28564ms step_avg:280.04ms
step:113/500 train_loss:5.4416 train_time:28844ms step_avg:280.03ms
step:114/500 train_loss:5.3936 train_time:29124ms step_avg:280.03ms
step:115/500 train_loss:5.2360 train_time:29405ms step_avg:280.05ms
step:116/500 train_loss:5.4189 train_time:29685ms step_avg:280.05ms
step:117/500 train_loss:5.2692 train_time:29964ms step_avg:280.04ms
step:118/500 train_loss:5.2746 train_time:30243ms step_avg:280.03ms
step:119/500 train_loss:5.3656 train_time:30523ms step_avg:280.03ms
step:120/500 train_loss:5.3813 train_time:30804ms step_avg:280.03ms
step:121/500 train_loss:5.2867 train_time:31084ms step_avg:280.03ms
step:122/500 train_loss:5.2082 train_time:31363ms step_avg:280.03ms
step:123/500 train_loss:5.2728 train_time:31644ms step_avg:280.03ms
step:124/500 train_loss:5.1723 train_time:31924ms step_avg:280.03ms
step:125/500 train_loss:5.4380 train_time:32204ms step_avg:280.04ms
step:125/500 val_loss:5.2917 train_time:32205ms step_avg:280.05ms
step:126/500 train_loss:5.3115 train_time:32486ms step_avg:280.05ms
step:127/500 train_loss:5.2777 train_time:32767ms step_avg:280.06ms
step:128/500 train_loss:5.3523 train_time:33050ms step_avg:280.08ms
step:129/500 train_loss:5.2013 train_time:33333ms step_avg:280.11ms
step:130/500 train_loss:5.4699 train_time:33614ms step_avg:280.12ms
step:131/500 train_loss:5.2593 train_time:33894ms step_avg:280.12ms
step:132/500 train_loss:5.2626 train_time:34175ms step_avg:280.13ms
step:133/500 train_loss:5.2045 train_time:34456ms step_avg:280.13ms
step:134/500 train_loss:5.2392 train_time:34736ms step_avg:280.13ms
step:135/500 train_loss:5.1870 train_time:35016ms step_avg:280.13ms
step:136/500 train_loss:5.2325 train_time:35295ms step_avg:280.12ms
step:137/500 train_loss:5.0616 train_time:35575ms step_avg:280.12ms
step:138/500 train_loss:5.1951 train_time:35856ms step_avg:280.13ms
step:139/500 train_loss:5.1734 train_time:36136ms step_avg:280.12ms
step:140/500 train_loss:5.1692 train_time:36415ms step_avg:280.12ms
step:141/500 train_loss:5.2281 train_time:36696ms step_avg:280.12ms
step:142/500 train_loss:5.1280 train_time:36976ms step_avg:280.12ms
step:143/500 train_loss:5.2110 train_time:37255ms step_avg:280.12ms
step:144/500 train_loss:5.0268 train_time:37536ms step_avg:280.12ms
step:145/500 train_loss:5.1807 train_time:37816ms step_avg:280.12ms
step:146/500 train_loss:5.1127 train_time:38095ms step_avg:280.11ms
step:147/500 train_loss:5.0375 train_time:38375ms step_avg:280.11ms
step:148/500 train_loss:5.1396 train_time:38655ms step_avg:280.11ms
step:149/500 train_loss:5.1303 train_time:38935ms step_avg:280.11ms
step:150/500 train_loss:5.1700 train_time:39215ms step_avg:280.11ms
step:151/500 train_loss:5.1966 train_time:39496ms step_avg:280.11ms w_mean:1.000 w_std:0.249 w_min:0.707 w_max:1.514
step:152/500 train_loss:5.1109 train_time:39776ms step_avg:280.11ms
step:153/500 train_loss:5.0978 train_time:40056ms step_avg:280.11ms
step:154/500 train_loss:5.1590 train_time:40335ms step_avg:280.11ms
step:155/500 train_loss:5.1090 train_time:40615ms step_avg:280.10ms
step:156/500 train_loss:5.0859 train_time:40895ms step_avg:280.11ms
step:157/500 train_loss:5.1023 train_time:41175ms step_avg:280.10ms
step:158/500 train_loss:5.2245 train_time:41457ms step_avg:280.12ms
step:159/500 train_loss:5.0160 train_time:41736ms step_avg:280.11ms
step:160/500 train_loss:5.0745 train_time:42015ms step_avg:280.10ms
step:161/500 train_loss:4.9254 train_time:42297ms step_avg:280.11ms
step:162/500 train_loss:5.0888 train_time:42576ms step_avg:280.11ms
step:163/500 train_loss:5.1039 train_time:42856ms step_avg:280.11ms
step:164/500 train_loss:5.1083 train_time:43136ms step_avg:280.10ms
step:165/500 train_loss:4.9323 train_time:43416ms step_avg:280.10ms
step:166/500 train_loss:5.0451 train_time:43697ms step_avg:280.11ms
step:167/500 train_loss:5.1864 train_time:43976ms step_avg:280.10ms
step:168/500 train_loss:4.9840 train_time:44256ms step_avg:280.10ms
step:169/500 train_loss:5.0520 train_time:44536ms step_avg:280.10ms
step:170/500 train_loss:4.9367 train_time:44815ms step_avg:280.10ms
step:171/500 train_loss:4.8706 train_time:45096ms step_avg:280.10ms
step:172/500 train_loss:4.9823 train_time:45376ms step_avg:280.10ms
step:173/500 train_loss:4.9480 train_time:45655ms step_avg:280.09ms
step:174/500 train_loss:5.0151 train_time:45935ms step_avg:280.09ms
step:175/500 train_loss:5.1361 train_time:46214ms step_avg:280.09ms
step:176/500 train_loss:5.0349 train_time:46495ms step_avg:280.09ms
step:177/500 train_loss:4.8703 train_time:46775ms step_avg:280.09ms
step:178/500 train_loss:4.8540 train_time:47056ms step_avg:280.09ms
step:179/500 train_loss:4.8883 train_time:47336ms step_avg:280.09ms
step:180/500 train_loss:4.9287 train_time:47616ms step_avg:280.09ms
step:181/500 train_loss:4.9245 train_time:47896ms step_avg:280.09ms
step:182/500 train_loss:5.0216 train_time:48176ms step_avg:280.09ms
step:183/500 train_loss:4.9283 train_time:48455ms step_avg:280.09ms
step:184/500 train_loss:4.8463 train_time:48736ms step_avg:280.09ms
step:185/500 train_loss:4.8838 train_time:49016ms step_avg:280.09ms
step:186/500 train_loss:4.9981 train_time:49296ms step_avg:280.09ms
step:187/500 train_loss:4.8923 train_time:49575ms step_avg:280.09ms
step:188/500 train_loss:5.1151 train_time:49855ms step_avg:280.09ms
step:189/500 train_loss:4.9241 train_time:50381ms step_avg:281.46ms
step:190/500 train_loss:4.8449 train_time:50934ms step_avg:282.96ms
step:191/500 train_loss:4.9879 train_time:51214ms step_avg:282.95ms
step:192/500 train_loss:4.8458 train_time:51494ms step_avg:282.93ms
step:193/500 train_loss:4.7481 train_time:51775ms step_avg:282.92ms
step:194/500 train_loss:4.9707 train_time:52056ms step_avg:282.91ms
step:195/500 train_loss:4.8927 train_time:52335ms step_avg:282.89ms
step:196/500 train_loss:5.0857 train_time:52614ms step_avg:282.87ms
step:197/500 train_loss:4.9668 train_time:52895ms step_avg:282.86ms
step:198/500 train_loss:4.8265 train_time:53175ms step_avg:282.85ms
step:199/500 train_loss:4.8526 train_time:53455ms step_avg:282.83ms
step:200/500 train_loss:4.7629 train_time:53736ms step_avg:282.82ms
step:201/500 train_loss:4.8311 train_time:54015ms step_avg:282.80ms w_mean:1.000 w_std:0.260 w_min:0.707 w_max:1.515
step:202/500 train_loss:4.7641 train_time:54296ms step_avg:282.79ms
step:203/500 train_loss:4.9710 train_time:54576ms step_avg:282.77ms
step:204/500 train_loss:4.8949 train_time:54855ms step_avg:282.76ms
step:205/500 train_loss:4.8467 train_time:55135ms step_avg:282.74ms
step:206/500 train_loss:5.0179 train_time:55414ms step_avg:282.73ms
step:207/500 train_loss:4.6853 train_time:55694ms step_avg:282.71ms
step:208/500 train_loss:4.8386 train_time:55975ms step_avg:282.70ms
step:209/500 train_loss:4.7825 train_time:56256ms step_avg:282.69ms
step:210/500 train_loss:4.9538 train_time:56535ms step_avg:282.67ms
step:211/500 train_loss:4.8750 train_time:56816ms step_avg:282.67ms
step:212/500 train_loss:4.7678 train_time:57095ms step_avg:282.65ms
step:213/500 train_loss:4.8948 train_time:57376ms step_avg:282.64ms
step:214/500 train_loss:4.7400 train_time:57655ms step_avg:282.62ms
step:215/500 train_loss:4.8198 train_time:57934ms step_avg:282.61ms
step:216/500 train_loss:4.6987 train_time:58215ms step_avg:282.60ms
step:217/500 train_loss:4.8080 train_time:58495ms step_avg:282.59ms
step:218/500 train_loss:4.7863 train_time:58775ms step_avg:282.57ms
step:219/500 train_loss:4.7585 train_time:59056ms step_avg:282.56ms
step:220/500 train_loss:4.7707 train_time:59336ms step_avg:282.55ms
step:221/500 train_loss:4.8001 train_time:59615ms step_avg:282.54ms
step:222/500 train_loss:4.8242 train_time:59896ms step_avg:282.53ms
step:223/500 train_loss:4.7803 train_time:60174ms step_avg:282.51ms
step:224/500 train_loss:4.7730 train_time:60455ms step_avg:282.50ms
step:225/500 train_loss:4.9027 train_time:60735ms step_avg:282.49ms
step:226/500 train_loss:4.6393 train_time:61015ms step_avg:282.48ms
step:227/500 train_loss:4.6825 train_time:61295ms step_avg:282.47ms
step:228/500 train_loss:4.6641 train_time:61576ms step_avg:282.46ms
step:229/500 train_loss:4.8326 train_time:61856ms step_avg:282.45ms
step:230/500 train_loss:4.6634 train_time:62135ms step_avg:282.43ms
step:231/500 train_loss:4.8062 train_time:62415ms step_avg:282.42ms
step:232/500 train_loss:4.6772 train_time:62696ms step_avg:282.42ms
step:233/500 train_loss:4.6387 train_time:62976ms step_avg:282.40ms
step:234/500 train_loss:4.8393 train_time:63256ms step_avg:282.39ms
step:235/500 train_loss:4.6769 train_time:63536ms step_avg:282.38ms
step:236/500 train_loss:4.6268 train_time:63815ms step_avg:282.37ms
step:237/500 train_loss:4.8540 train_time:64095ms step_avg:282.36ms
step:238/500 train_loss:4.7551 train_time:64376ms step_avg:282.35ms
step:239/500 train_loss:4.6550 train_time:64657ms step_avg:282.34ms
step:240/500 train_loss:4.8104 train_time:64935ms step_avg:282.33ms
step:241/500 train_loss:4.7784 train_time:65215ms step_avg:282.32ms
step:242/500 train_loss:4.7058 train_time:65495ms step_avg:282.31ms
step:243/500 train_loss:4.8331 train_time:65775ms step_avg:282.30ms
step:244/500 train_loss:4.6927 train_time:66055ms step_avg:282.28ms
step:245/500 train_loss:4.6922 train_time:66335ms step_avg:282.28ms
step:246/500 train_loss:4.7694 train_time:66615ms step_avg:282.27ms
step:247/500 train_loss:4.7186 train_time:66895ms step_avg:282.26ms
step:248/500 train_loss:4.6886 train_time:67174ms step_avg:282.25ms
step:249/500 train_loss:4.8376 train_time:67455ms step_avg:282.24ms
step:250/500 train_loss:4.5932 train_time:67735ms step_avg:282.23ms
step:250/500 val_loss:4.6824 train_time:67736ms step_avg:282.24ms
step:251/500 train_loss:4.6167 train_time:68018ms step_avg:282.23ms w_mean:1.000 w_std:0.264 w_min:0.707 w_max:1.515
step:252/500 train_loss:4.7630 train_time:68300ms step_avg:282.23ms
step:253/500 train_loss:4.7428 train_time:68582ms step_avg:282.23ms
step:254/500 train_loss:4.6343 train_time:68864ms step_avg:282.23ms
step:255/500 train_loss:4.6453 train_time:69144ms step_avg:282.22ms
step:256/500 train_loss:4.7730 train_time:69425ms step_avg:282.21ms
step:257/500 train_loss:4.7200 train_time:69705ms step_avg:282.21ms
step:258/500 train_loss:4.6964 train_time:69985ms step_avg:282.20ms
step:259/500 train_loss:4.6301 train_time:70265ms step_avg:282.19ms
step:260/500 train_loss:4.6414 train_time:70545ms step_avg:282.18ms
step:261/500 train_loss:4.7087 train_time:70825ms step_avg:282.17ms
step:262/500 train_loss:4.7167 train_time:71105ms step_avg:282.16ms
step:263/500 train_loss:4.6266 train_time:71385ms step_avg:282.16ms
step:264/500 train_loss:4.5689 train_time:71665ms step_avg:282.15ms
step:265/500 train_loss:4.6282 train_time:71945ms step_avg:282.14ms
step:266/500 train_loss:4.4790 train_time:72226ms step_avg:282.13ms
step:267/500 train_loss:4.5420 train_time:72505ms step_avg:282.12ms
step:268/500 train_loss:4.5811 train_time:72785ms step_avg:282.11ms
step:269/500 train_loss:4.5438 train_time:73065ms step_avg:282.11ms
step:270/500 train_loss:4.5134 train_time:73346ms step_avg:282.10ms
step:271/500 train_loss:4.7275 train_time:73625ms step_avg:282.09ms
step:272/500 train_loss:4.6644 train_time:73905ms step_avg:282.08ms
step:273/500 train_loss:4.5241 train_time:74185ms step_avg:282.07ms
step:274/500 train_loss:4.5760 train_time:74465ms step_avg:282.06ms
step:275/500 train_loss:4.6799 train_time:74745ms step_avg:282.06ms
step:276/500 train_loss:4.7006 train_time:75024ms step_avg:282.05ms
step:277/500 train_loss:4.8892 train_time:75305ms step_avg:282.04ms
step:278/500 train_loss:4.6458 train_time:75585ms step_avg:282.03ms
step:279/500 train_loss:4.7759 train_time:75865ms step_avg:282.03ms
step:280/500 train_loss:4.6186 train_time:76145ms step_avg:282.02ms
step:281/500 train_loss:4.6798 train_time:76425ms step_avg:282.01ms
step:282/500 train_loss:4.5880 train_time:76705ms step_avg:282.00ms
step:283/500 train_loss:4.6961 train_time:76984ms step_avg:281.99ms
step:284/500 train_loss:4.5262 train_time:77265ms step_avg:281.99ms
step:285/500 train_loss:4.6798 train_time:77545ms step_avg:281.98ms
step:286/500 train_loss:4.6782 train_time:77825ms step_avg:281.97ms
step:287/500 train_loss:4.6935 train_time:78104ms step_avg:281.96ms
step:288/500 train_loss:4.5732 train_time:78385ms step_avg:281.96ms
step:289/500 train_loss:4.6218 train_time:78665ms step_avg:281.95ms
step:290/500 train_loss:4.4968 train_time:78945ms step_avg:281.95ms
step:291/500 train_loss:4.4873 train_time:79226ms step_avg:281.94ms
step:292/500 train_loss:4.6194 train_time:79505ms step_avg:281.93ms
step:293/500 train_loss:4.5045 train_time:79785ms step_avg:281.93ms
step:294/500 train_loss:4.5587 train_time:80066ms step_avg:281.92ms
step:295/500 train_loss:4.5726 train_time:80346ms step_avg:281.92ms
step:296/500 train_loss:4.4495 train_time:80625ms step_avg:281.90ms
step:297/500 train_loss:4.4331 train_time:80906ms step_avg:281.90ms
step:298/500 train_loss:4.4678 train_time:81185ms step_avg:281.89ms
step:299/500 train_loss:4.5596 train_time:81464ms step_avg:281.88ms
step:300/500 train_loss:4.4591 train_time:81745ms step_avg:281.88ms
step:301/500 train_loss:4.6186 train_time:82025ms step_avg:281.87ms w_mean:1.000 w_std:0.265 w_min:0.707 w_max:1.516
step:302/500 train_loss:4.6067 train_time:82306ms step_avg:281.87ms
step:303/500 train_loss:4.5168 train_time:82585ms step_avg:281.86ms
step:304/500 train_loss:4.5954 train_time:82865ms step_avg:281.85ms
step:305/500 train_loss:4.5733 train_time:83145ms step_avg:281.85ms
step:306/500 train_loss:5.0372 train_time:83425ms step_avg:281.84ms
step:307/500 train_loss:4.5290 train_time:83705ms step_avg:281.83ms
step:308/500 train_loss:4.4371 train_time:83985ms step_avg:281.83ms
step:309/500 train_loss:4.6217 train_time:84264ms step_avg:281.82ms
step:310/500 train_loss:4.4306 train_time:84545ms step_avg:281.82ms
step:311/500 train_loss:4.6513 train_time:84826ms step_avg:281.81ms
step:312/500 train_loss:4.5741 train_time:85105ms step_avg:281.80ms
step:313/500 train_loss:4.4761 train_time:85385ms step_avg:281.80ms
step:314/500 train_loss:4.6050 train_time:85665ms step_avg:281.79ms
step:315/500 train_loss:4.7335 train_time:85946ms step_avg:281.79ms
step:316/500 train_loss:4.5765 train_time:86225ms step_avg:281.78ms
step:317/500 train_loss:4.4595 train_time:86505ms step_avg:281.78ms
step:318/500 train_loss:4.4821 train_time:86785ms step_avg:281.77ms
step:319/500 train_loss:4.4931 train_time:87064ms step_avg:281.76ms
step:320/500 train_loss:4.4426 train_time:87344ms step_avg:281.76ms
step:321/500 train_loss:4.5337 train_time:87624ms step_avg:281.75ms
step:322/500 train_loss:4.5489 train_time:87905ms step_avg:281.75ms
step:323/500 train_loss:4.5111 train_time:88185ms step_avg:281.74ms
step:324/500 train_loss:4.5811 train_time:88465ms step_avg:281.74ms
step:325/500 train_loss:4.5710 train_time:88745ms step_avg:281.73ms
step:326/500 train_loss:4.6467 train_time:89025ms step_avg:281.73ms
step:327/500 train_loss:4.4879 train_time:89305ms step_avg:281.72ms
step:328/500 train_loss:4.9350 train_time:89585ms step_avg:281.72ms
step:329/500 train_loss:4.6429 train_time:89866ms step_avg:281.71ms
step:330/500 train_loss:4.4423 train_time:90146ms step_avg:281.71ms
step:331/500 train_loss:4.4006 train_time:90425ms step_avg:281.70ms
step:332/500 train_loss:4.5491 train_time:90704ms step_avg:281.69ms
step:333/500 train_loss:4.4790 train_time:90984ms step_avg:281.68ms
step:334/500 train_loss:4.4588 train_time:91265ms step_avg:281.68ms
step:335/500 train_loss:4.4328 train_time:91545ms step_avg:281.68ms
step:336/500 train_loss:4.6122 train_time:91825ms step_avg:281.67ms
step:337/500 train_loss:4.5543 train_time:92105ms step_avg:281.67ms
step:338/500 train_loss:5.0771 train_time:92385ms step_avg:281.66ms
step:339/500 train_loss:4.5304 train_time:92665ms step_avg:281.66ms
step:340/500 train_loss:4.4945 train_time:92945ms step_avg:281.65ms
step:341/500 train_loss:4.4764 train_time:93225ms step_avg:281.65ms
step:342/500 train_loss:4.4213 train_time:93505ms step_avg:281.64ms
step:343/500 train_loss:4.4003 train_time:93785ms step_avg:281.64ms
step:344/500 train_loss:4.4609 train_time:94065ms step_avg:281.63ms
step:345/500 train_loss:4.5462 train_time:94346ms step_avg:281.63ms
step:346/500 train_loss:4.4385 train_time:94626ms step_avg:281.62ms
step:347/500 train_loss:4.3920 train_time:94905ms step_avg:281.62ms
step:348/500 train_loss:4.4478 train_time:95185ms step_avg:281.61ms
step:349/500 train_loss:4.4393 train_time:95465ms step_avg:281.61ms
step:350/500 train_loss:4.3674 train_time:95745ms step_avg:281.60ms
step:351/500 train_loss:4.0685 train_time:96025ms step_avg:281.60ms w_mean:1.000 w_std:0.270 w_min:0.711 w_max:1.523
step:352/500 train_loss:4.3469 train_time:96305ms step_avg:281.59ms
step:353/500 train_loss:4.6882 train_time:96585ms step_avg:281.59ms
step:354/500 train_loss:4.2340 train_time:96866ms step_avg:281.59ms
step:355/500 train_loss:4.4706 train_time:97145ms step_avg:281.58ms
step:356/500 train_loss:4.3841 train_time:97425ms step_avg:281.58ms
step:357/500 train_loss:4.4762 train_time:97705ms step_avg:281.57ms
step:358/500 train_loss:4.4784 train_time:97985ms step_avg:281.57ms
step:359/500 train_loss:4.3904 train_time:98264ms step_avg:281.56ms
step:360/500 train_loss:4.7076 train_time:98545ms step_avg:281.56ms
step:361/500 train_loss:4.1281 train_time:98825ms step_avg:281.55ms
step:362/500 train_loss:4.6023 train_time:99105ms step_avg:281.55ms
step:363/500 train_loss:4.4988 train_time:99386ms step_avg:281.55ms
step:364/500 train_loss:4.3915 train_time:99666ms step_avg:281.54ms
step:365/500 train_loss:4.3245 train_time:99946ms step_avg:281.54ms
step:366/500 train_loss:4.4829 train_time:100226ms step_avg:281.53ms
step:367/500 train_loss:4.4103 train_time:100506ms step_avg:281.53ms
step:368/500 train_loss:4.3994 train_time:100785ms step_avg:281.52ms
step:369/500 train_loss:4.4021 train_time:101065ms step_avg:281.52ms
step:370/500 train_loss:4.2967 train_time:101345ms step_avg:281.51ms
step:371/500 train_loss:4.4357 train_time:101625ms step_avg:281.51ms
step:372/500 train_loss:4.3828 train_time:101905ms step_avg:281.50ms
step:373/500 train_loss:4.2584 train_time:102184ms step_avg:281.50ms
step:374/500 train_loss:4.4440 train_time:102465ms step_avg:281.50ms
step:375/500 train_loss:4.3768 train_time:102746ms step_avg:281.50ms
step:375/500 val_loss:4.3950 train_time:102747ms step_avg:281.50ms
step:376/500 train_loss:4.3733 train_time:103027ms step_avg:281.50ms
step:377/500 train_loss:4.4348 train_time:103310ms step_avg:281.50ms
step:378/500 train_loss:4.3290 train_time:103837ms step_avg:282.17ms
step:379/500 train_loss:4.3724 train_time:104116ms step_avg:282.16ms
step:380/500 train_loss:4.4653 train_time:104658ms step_avg:282.86ms
step:381/500 train_loss:4.4818 train_time:104937ms step_avg:282.85ms
step:382/500 train_loss:4.4327 train_time:105215ms step_avg:282.84ms
step:383/500 train_loss:4.4148 train_time:105495ms step_avg:282.83ms
step:384/500 train_loss:4.3142 train_time:105775ms step_avg:282.82ms
step:385/500 train_loss:4.4130 train_time:106055ms step_avg:282.81ms
step:386/500 train_loss:4.3286 train_time:106335ms step_avg:282.81ms
step:387/500 train_loss:4.4587 train_time:106615ms step_avg:282.80ms
step:388/500 train_loss:4.6493 train_time:106894ms step_avg:282.79ms
step:389/500 train_loss:4.3490 train_time:107175ms step_avg:282.78ms
step:390/500 train_loss:4.3088 train_time:107455ms step_avg:282.78ms
step:391/500 train_loss:4.4347 train_time:107734ms step_avg:282.77ms
step:392/500 train_loss:4.3611 train_time:108015ms step_avg:282.76ms
step:393/500 train_loss:4.4613 train_time:108294ms step_avg:282.75ms
step:394/500 train_loss:4.2960 train_time:108575ms step_avg:282.75ms
step:395/500 train_loss:4.4197 train_time:108855ms step_avg:282.74ms
step:396/500 train_loss:4.2008 train_time:109135ms step_avg:282.73ms
step:397/500 train_loss:4.3698 train_time:109416ms step_avg:282.73ms
step:398/500 train_loss:4.4651 train_time:109695ms step_avg:282.72ms
step:399/500 train_loss:4.4097 train_time:109976ms step_avg:282.71ms
step:400/500 train_loss:4.3341 train_time:110256ms step_avg:282.71ms
step:401/500 train_loss:4.4025 train_time:110535ms step_avg:282.70ms w_mean:1.000 w_std:0.269 w_min:0.708 w_max:1.516
step:402/500 train_loss:4.4327 train_time:110815ms step_avg:282.69ms
step:403/500 train_loss:4.4044 train_time:111095ms step_avg:282.68ms
step:404/500 train_loss:4.4954 train_time:111374ms step_avg:282.68ms
step:405/500 train_loss:4.2840 train_time:111655ms step_avg:282.67ms
step:406/500 train_loss:4.3309 train_time:111935ms step_avg:282.66ms
step:407/500 train_loss:4.5962 train_time:112214ms step_avg:282.66ms
step:408/500 train_loss:4.3662 train_time:112496ms step_avg:282.65ms
step:409/500 train_loss:4.3545 train_time:112776ms step_avg:282.65ms
step:410/500 train_loss:4.4044 train_time:113055ms step_avg:282.64ms
step:411/500 train_loss:4.2958 train_time:113335ms step_avg:282.63ms
step:412/500 train_loss:4.3086 train_time:113615ms step_avg:282.62ms
step:413/500 train_loss:4.7208 train_time:113895ms step_avg:282.62ms
step:414/500 train_loss:4.1828 train_time:114175ms step_avg:282.61ms
step:415/500 train_loss:4.5344 train_time:114454ms step_avg:282.60ms
step:416/500 train_loss:4.3177 train_time:114735ms step_avg:282.60ms
step:417/500 train_loss:4.3084 train_time:115015ms step_avg:282.59ms
step:418/500 train_loss:4.4879 train_time:115295ms step_avg:282.59ms
step:419/500 train_loss:4.2264 train_time:115575ms step_avg:282.58ms
step:420/500 train_loss:4.3240 train_time:115854ms step_avg:282.57ms
step:421/500 train_loss:4.3044 train_time:116134ms step_avg:282.57ms
step:422/500 train_loss:4.1924 train_time:116415ms step_avg:282.56ms
step:423/500 train_loss:4.2937 train_time:116695ms step_avg:282.56ms
step:424/500 train_loss:4.4077 train_time:116975ms step_avg:282.55ms
step:425/500 train_loss:4.2165 train_time:117256ms step_avg:282.54ms
step:426/500 train_loss:4.3790 train_time:117536ms step_avg:282.54ms
step:427/500 train_loss:4.2533 train_time:117815ms step_avg:282.53ms
step:428/500 train_loss:4.4323 train_time:118095ms step_avg:282.52ms
step:429/500 train_loss:4.3873 train_time:118375ms step_avg:282.52ms
step:430/500 train_loss:4.2954 train_time:118656ms step_avg:282.51ms
step:431/500 train_loss:4.2705 train_time:118935ms step_avg:282.51ms
step:432/500 train_loss:4.2215 train_time:119216ms step_avg:282.50ms
step:433/500 train_loss:4.3082 train_time:119496ms step_avg:282.50ms
step:434/500 train_loss:4.3848 train_time:119776ms step_avg:282.49ms
step:435/500 train_loss:4.3090 train_time:120060ms step_avg:282.49ms
step:436/500 train_loss:4.3551 train_time:120336ms step_avg:282.48ms
step:437/500 train_loss:4.3681 train_time:120616ms step_avg:282.47ms
step:438/500 train_loss:4.2521 train_time:120895ms step_avg:282.47ms
step:439/500 train_loss:4.2708 train_time:121175ms step_avg:282.46ms
step:440/500 train_loss:4.2317 train_time:121455ms step_avg:282.45ms
step:441/500 train_loss:4.4237 train_time:121735ms step_avg:282.45ms
step:442/500 train_loss:4.3271 train_time:122016ms step_avg:282.45ms
step:443/500 train_loss:4.2996 train_time:122295ms step_avg:282.44ms
step:444/500 train_loss:4.1969 train_time:122575ms step_avg:282.43ms
step:445/500 train_loss:4.4441 train_time:122855ms step_avg:282.43ms
step:446/500 train_loss:4.3662 train_time:123135ms step_avg:282.42ms
step:447/500 train_loss:4.3687 train_time:123416ms step_avg:282.42ms
step:448/500 train_loss:4.2924 train_time:123697ms step_avg:282.41ms
step:449/500 train_loss:4.3735 train_time:123977ms step_avg:282.41ms
step:450/500 train_loss:4.2133 train_time:124256ms step_avg:282.40ms
step:451/500 train_loss:4.2546 train_time:124535ms step_avg:282.39ms w_mean:1.000 w_std:0.272 w_min:0.709 w_max:1.519
step:452/500 train_loss:4.1449 train_time:124816ms step_avg:282.39ms
step:453/500 train_loss:4.2415 train_time:125096ms step_avg:282.38ms
step:454/500 train_loss:4.2157 train_time:125376ms step_avg:282.38ms
step:455/500 train_loss:4.2018 train_time:125655ms step_avg:282.37ms
step:456/500 train_loss:4.3986 train_time:125935ms step_avg:282.37ms
step:457/500 train_loss:4.2533 train_time:126215ms step_avg:282.36ms
step:458/500 train_loss:4.3448 train_time:126495ms step_avg:282.35ms
step:459/500 train_loss:4.3791 train_time:126775ms step_avg:282.35ms
step:460/500 train_loss:4.1819 train_time:127055ms step_avg:282.34ms
step:461/500 train_loss:4.3507 train_time:127335ms step_avg:282.34ms
step:462/500 train_loss:4.2573 train_time:127616ms step_avg:282.34ms
step:463/500 train_loss:4.2322 train_time:127895ms step_avg:282.33ms
step:464/500 train_loss:4.3300 train_time:128175ms step_avg:282.32ms
step:465/500 train_loss:4.2601 train_time:128455ms step_avg:282.32ms
step:466/500 train_loss:4.2631 train_time:128736ms step_avg:282.32ms
step:467/500 train_loss:4.3928 train_time:129016ms step_avg:282.31ms
step:468/500 train_loss:4.3909 train_time:129296ms step_avg:282.31ms
step:469/500 train_loss:4.3541 train_time:129575ms step_avg:282.30ms
step:470/500 train_loss:4.2671 train_time:129855ms step_avg:282.29ms
step:471/500 train_loss:4.3553 train_time:130136ms step_avg:282.29ms
step:472/500 train_loss:4.3963 train_time:130415ms step_avg:282.28ms
step:473/500 train_loss:4.3030 train_time:130695ms step_avg:282.28ms
step:474/500 train_loss:4.2758 train_time:130975ms step_avg:282.27ms
step:475/500 train_loss:4.1539 train_time:131256ms step_avg:282.27ms
step:476/500 train_loss:4.5745 train_time:131536ms step_avg:282.27ms
step:477/500 train_loss:4.3301 train_time:131816ms step_avg:282.26ms
step:478/500 train_loss:4.1356 train_time:132096ms step_avg:282.26ms
step:479/500 train_loss:4.3336 train_time:132376ms step_avg:282.25ms
step:480/500 train_loss:4.3200 train_time:132656ms step_avg:282.25ms
step:481/500 train_loss:4.4451 train_time:132936ms step_avg:282.24ms
step:482/500 train_loss:4.2794 train_time:133215ms step_avg:282.24ms
step:483/500 train_loss:4.1026 train_time:133495ms step_avg:282.23ms
step:484/500 train_loss:4.3689 train_time:133775ms step_avg:282.23ms
step:485/500 train_loss:4.2189 train_time:134055ms step_avg:282.22ms
step:486/500 train_loss:4.2446 train_time:134336ms step_avg:282.22ms
step:487/500 train_loss:4.1933 train_time:134616ms step_avg:282.21ms
step:488/500 train_loss:4.2116 train_time:134896ms step_avg:282.21ms
step:489/500 train_loss:4.4238 train_time:135176ms step_avg:282.20ms
step:490/500 train_loss:4.2800 train_time:135455ms step_avg:282.20ms
step:491/500 train_loss:4.1804 train_time:135736ms step_avg:282.19ms
step:492/500 train_loss:4.1866 train_time:136015ms step_avg:282.19ms
step:493/500 train_loss:4.2984 train_time:136296ms step_avg:282.19ms
step:494/500 train_loss:4.1425 train_time:136575ms step_avg:282.18ms
step:495/500 train_loss:4.2920 train_time:136855ms step_avg:282.17ms
step:496/500 train_loss:4.2151 train_time:137135ms step_avg:282.17ms
step:497/500 train_loss:4.1624 train_time:137415ms step_avg:282.17ms
step:498/500 train_loss:4.2991 train_time:137695ms step_avg:282.16ms
step:499/500 train_loss:4.3836 train_time:137975ms step_avg:282.16ms
step:500/500 train_loss:4.4379 train_time:138255ms step_avg:282.15ms
step:500/500 val_loss:4.2833 train_time:138256ms step_avg:282.16ms
