====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock and keep timing accurate, but skip checkpoint writes to save disk
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        if master_process:
            print("Checkpoint saving disabled; skipping state serialization.")
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 22:50:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     17%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     36%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   49C    P0             89W /  310W |    2363MiB /  81559MiB |     28%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             85W /  310W |    2363MiB /  81559MiB |     30%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   43C    P0             81W /  310W |    2363MiB /  81559MiB |     36%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0             85W /  310W |    2363MiB /  81559MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   52C    P0             87W /  310W |    2363MiB /  81559MiB |     30%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   45C    P0             83W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           80216      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           80217      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           80218      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           80219      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           80220      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           80221      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           80222      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           80223      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 123 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: focal
  clamp: [0.6, 1.5]
  schedule: constant
  focal_gamma: 2.0
====================================================================================================
step:0/500 val_loss:16.0073 train_time:268ms step_avg:nanms
step:1/500 train_loss:15.9981 train_time:61827ms step_avg:nanms w_mean:1.000 w_std:0.080 w_min:0.799 w_max:1.332
step:2/500 train_loss:9.3712 train_time:62421ms step_avg:nanms
step:3/500 train_loss:8.9606 train_time:62706ms step_avg:nanms
step:4/500 train_loss:8.6543 train_time:62994ms step_avg:nanms
step:5/500 train_loss:8.1059 train_time:63277ms step_avg:nanms
step:6/500 train_loss:7.7442 train_time:63561ms step_avg:nanms
step:7/500 train_loss:7.3093 train_time:63845ms step_avg:nanms
step:8/500 train_loss:7.4773 train_time:64130ms step_avg:nanms
step:9/500 train_loss:7.2709 train_time:64415ms step_avg:nanms
step:10/500 train_loss:7.0928 train_time:64704ms step_avg:nanms
step:11/500 train_loss:7.0825 train_time:287ms step_avg:nanms
step:12/500 train_loss:7.0083 train_time:571ms step_avg:nanms
step:13/500 train_loss:6.8258 train_time:853ms step_avg:284.49ms
step:14/500 train_loss:6.8316 train_time:1136ms step_avg:284.00ms
step:15/500 train_loss:6.7881 train_time:1420ms step_avg:283.98ms
step:16/500 train_loss:6.7087 train_time:1707ms step_avg:284.55ms
step:17/500 train_loss:6.7155 train_time:1992ms step_avg:284.60ms
step:18/500 train_loss:6.7462 train_time:2277ms step_avg:284.57ms
step:19/500 train_loss:6.5667 train_time:2561ms step_avg:284.59ms
step:20/500 train_loss:6.5750 train_time:2848ms step_avg:284.80ms
step:21/500 train_loss:6.2458 train_time:3131ms step_avg:284.68ms
step:22/500 train_loss:6.6344 train_time:3416ms step_avg:284.64ms
step:23/500 train_loss:6.8773 train_time:3703ms step_avg:284.82ms
step:24/500 train_loss:6.5093 train_time:3990ms step_avg:284.98ms
step:25/500 train_loss:6.6295 train_time:4273ms step_avg:284.88ms
step:26/500 train_loss:6.3570 train_time:4558ms step_avg:284.88ms
step:27/500 train_loss:6.2738 train_time:4844ms step_avg:284.94ms
step:28/500 train_loss:6.4506 train_time:5130ms step_avg:284.99ms
step:29/500 train_loss:6.0998 train_time:5412ms step_avg:284.83ms
step:30/500 train_loss:6.3782 train_time:5693ms step_avg:284.67ms
step:31/500 train_loss:6.2257 train_time:5976ms step_avg:284.59ms
step:32/500 train_loss:6.1893 train_time:6262ms step_avg:284.63ms
step:33/500 train_loss:6.0003 train_time:6549ms step_avg:284.72ms
step:34/500 train_loss:6.3416 train_time:6832ms step_avg:284.69ms
step:35/500 train_loss:6.2526 train_time:7114ms step_avg:284.57ms
step:36/500 train_loss:6.4082 train_time:7394ms step_avg:284.39ms
step:37/500 train_loss:6.3315 train_time:7692ms step_avg:284.89ms
step:38/500 train_loss:6.2227 train_time:7976ms step_avg:284.85ms
step:39/500 train_loss:6.1165 train_time:8261ms step_avg:284.86ms
step:40/500 train_loss:6.1734 train_time:8545ms step_avg:284.85ms
step:41/500 train_loss:6.0745 train_time:8830ms step_avg:284.83ms
step:42/500 train_loss:6.1147 train_time:9112ms step_avg:284.75ms
step:43/500 train_loss:5.9815 train_time:9395ms step_avg:284.68ms
step:44/500 train_loss:6.0614 train_time:9682ms step_avg:284.76ms
step:45/500 train_loss:6.0532 train_time:9967ms step_avg:284.78ms
step:46/500 train_loss:6.2396 train_time:10251ms step_avg:284.76ms
step:47/500 train_loss:6.0371 train_time:10533ms step_avg:284.68ms
step:48/500 train_loss:5.8776 train_time:10814ms step_avg:284.59ms
step:49/500 train_loss:6.1274 train_time:11098ms step_avg:284.57ms
step:50/500 train_loss:5.9973 train_time:11384ms step_avg:284.60ms
step:51/500 train_loss:6.1546 train_time:11670ms step_avg:284.64ms w_mean:1.000 w_std:0.010 w_min:1.000 w_max:1.666
step:52/500 train_loss:6.0131 train_time:11953ms step_avg:284.59ms
step:53/500 train_loss:5.8600 train_time:12236ms step_avg:284.57ms
step:54/500 train_loss:5.9913 train_time:12520ms step_avg:284.54ms
step:55/500 train_loss:5.9071 train_time:12805ms step_avg:284.55ms
step:56/500 train_loss:6.2159 train_time:13091ms step_avg:284.58ms
step:57/500 train_loss:5.8995 train_time:13373ms step_avg:284.54ms
step:58/500 train_loss:5.7811 train_time:13657ms step_avg:284.52ms
step:59/500 train_loss:5.9381 train_time:13941ms step_avg:284.51ms
step:60/500 train_loss:5.8797 train_time:14226ms step_avg:284.53ms
step:61/500 train_loss:5.9670 train_time:14510ms step_avg:284.52ms
step:62/500 train_loss:5.7671 train_time:14793ms step_avg:284.48ms
step:63/500 train_loss:5.8669 train_time:15078ms step_avg:284.49ms
step:64/500 train_loss:5.8333 train_time:15361ms step_avg:284.47ms
step:65/500 train_loss:5.7479 train_time:15648ms step_avg:284.51ms
step:66/500 train_loss:5.6649 train_time:15932ms step_avg:284.50ms
step:67/500 train_loss:5.8335 train_time:16215ms step_avg:284.47ms
step:68/500 train_loss:5.7015 train_time:16499ms step_avg:284.47ms
step:69/500 train_loss:5.9461 train_time:16784ms step_avg:284.47ms
step:70/500 train_loss:5.6157 train_time:17070ms step_avg:284.50ms
step:71/500 train_loss:5.6436 train_time:17352ms step_avg:284.46ms
step:72/500 train_loss:5.8429 train_time:17633ms step_avg:284.40ms
step:73/500 train_loss:5.7824 train_time:17913ms step_avg:284.33ms
step:74/500 train_loss:5.6700 train_time:18197ms step_avg:284.32ms
step:75/500 train_loss:5.7828 train_time:18483ms step_avg:284.35ms
step:76/500 train_loss:5.7473 train_time:18771ms step_avg:284.41ms
step:77/500 train_loss:5.7129 train_time:19053ms step_avg:284.37ms
step:78/500 train_loss:5.8007 train_time:19336ms step_avg:284.35ms
step:79/500 train_loss:5.8274 train_time:19619ms step_avg:284.33ms
step:80/500 train_loss:5.6954 train_time:19904ms step_avg:284.34ms
step:81/500 train_loss:5.7760 train_time:20191ms step_avg:284.38ms
step:82/500 train_loss:5.5413 train_time:20473ms step_avg:284.34ms
step:83/500 train_loss:5.7173 train_time:20754ms step_avg:284.30ms
step:84/500 train_loss:5.6798 train_time:21034ms step_avg:284.25ms
step:85/500 train_loss:5.6435 train_time:21315ms step_avg:284.20ms
step:86/500 train_loss:5.5121 train_time:21600ms step_avg:284.21ms
step:87/500 train_loss:5.7215 train_time:21887ms step_avg:284.24ms
step:88/500 train_loss:5.6236 train_time:22170ms step_avg:284.23ms
step:89/500 train_loss:5.6861 train_time:22452ms step_avg:284.21ms
step:90/500 train_loss:5.6635 train_time:22735ms step_avg:284.18ms
step:91/500 train_loss:5.5755 train_time:23016ms step_avg:284.15ms
step:92/500 train_loss:5.5852 train_time:23297ms step_avg:284.12ms
step:93/500 train_loss:5.6814 train_time:23584ms step_avg:284.14ms
step:94/500 train_loss:5.5308 train_time:23869ms step_avg:284.16ms
step:95/500 train_loss:5.5185 train_time:24152ms step_avg:284.14ms
step:96/500 train_loss:5.5410 train_time:24436ms step_avg:284.14ms
step:97/500 train_loss:5.4574 train_time:24720ms step_avg:284.14ms
step:98/500 train_loss:5.5325 train_time:25003ms step_avg:284.12ms
step:99/500 train_loss:5.4558 train_time:25290ms step_avg:284.15ms
step:100/500 train_loss:5.5758 train_time:25573ms step_avg:284.15ms
step:101/500 train_loss:5.5397 train_time:25857ms step_avg:284.15ms w_mean:1.000 w_std:0.013 w_min:0.999 w_max:1.665
step:102/500 train_loss:5.4313 train_time:26142ms step_avg:284.16ms
step:103/500 train_loss:5.5392 train_time:26428ms step_avg:284.17ms
step:104/500 train_loss:5.5044 train_time:26713ms step_avg:284.18ms
step:105/500 train_loss:5.3364 train_time:26998ms step_avg:284.19ms
step:106/500 train_loss:5.4502 train_time:27289ms step_avg:284.26ms
step:107/500 train_loss:5.6462 train_time:27572ms step_avg:284.25ms
step:108/500 train_loss:5.4358 train_time:27856ms step_avg:284.24ms
step:109/500 train_loss:5.1976 train_time:28140ms step_avg:284.24ms
step:110/500 train_loss:5.3965 train_time:28423ms step_avg:284.23ms
step:111/500 train_loss:5.3655 train_time:28709ms step_avg:284.24ms
step:112/500 train_loss:5.3414 train_time:28992ms step_avg:284.23ms
step:113/500 train_loss:5.4463 train_time:29274ms step_avg:284.21ms
step:114/500 train_loss:5.3793 train_time:29556ms step_avg:284.20ms
step:115/500 train_loss:5.2314 train_time:29843ms step_avg:284.22ms
step:116/500 train_loss:5.4007 train_time:30128ms step_avg:284.23ms
step:117/500 train_loss:5.2612 train_time:30411ms step_avg:284.22ms
step:118/500 train_loss:5.2530 train_time:30694ms step_avg:284.20ms
step:119/500 train_loss:5.3738 train_time:30979ms step_avg:284.21ms
step:120/500 train_loss:5.3591 train_time:31264ms step_avg:284.22ms
step:121/500 train_loss:5.2866 train_time:31554ms step_avg:284.27ms
step:122/500 train_loss:5.1810 train_time:31838ms step_avg:284.26ms
step:123/500 train_loss:5.2767 train_time:32121ms step_avg:284.26ms
step:124/500 train_loss:5.1451 train_time:32407ms step_avg:284.27ms
step:125/500 train_loss:5.4368 train_time:32691ms step_avg:284.27ms
step:125/500 val_loss:5.2682 train_time:32692ms step_avg:284.27ms
step:126/500 train_loss:5.2904 train_time:32962ms step_avg:284.15ms
step:127/500 train_loss:5.2668 train_time:33250ms step_avg:284.18ms
step:128/500 train_loss:5.3315 train_time:33534ms step_avg:284.18ms
step:129/500 train_loss:5.1898 train_time:33819ms step_avg:284.19ms
step:130/500 train_loss:5.4615 train_time:34101ms step_avg:284.18ms
step:131/500 train_loss:5.2462 train_time:34384ms step_avg:284.17ms
step:132/500 train_loss:5.2453 train_time:34669ms step_avg:284.18ms
step:133/500 train_loss:5.1928 train_time:34955ms step_avg:284.19ms
step:134/500 train_loss:5.2366 train_time:35240ms step_avg:284.19ms
step:135/500 train_loss:5.1628 train_time:35521ms step_avg:284.17ms
step:136/500 train_loss:5.2325 train_time:35802ms step_avg:284.14ms
step:137/500 train_loss:5.0309 train_time:36085ms step_avg:284.13ms
step:138/500 train_loss:5.1940 train_time:36370ms step_avg:284.14ms
step:139/500 train_loss:5.1532 train_time:36656ms step_avg:284.15ms
step:140/500 train_loss:5.1609 train_time:36940ms step_avg:284.15ms
step:141/500 train_loss:5.2131 train_time:37221ms step_avg:284.13ms
step:142/500 train_loss:5.1161 train_time:37502ms step_avg:284.10ms
step:143/500 train_loss:5.1914 train_time:37782ms step_avg:284.08ms
step:144/500 train_loss:5.0071 train_time:38066ms step_avg:284.07ms
step:145/500 train_loss:5.1630 train_time:38351ms step_avg:284.08ms
step:146/500 train_loss:5.1012 train_time:38636ms step_avg:284.09ms
step:147/500 train_loss:5.0090 train_time:38921ms step_avg:284.09ms
step:148/500 train_loss:5.1317 train_time:39202ms step_avg:284.07ms
step:149/500 train_loss:5.1089 train_time:39482ms step_avg:284.04ms
step:150/500 train_loss:5.1691 train_time:39765ms step_avg:284.03ms
step:151/500 train_loss:5.1784 train_time:40050ms step_avg:284.05ms w_mean:1.000 w_std:0.012 w_min:0.999 w_max:1.666
step:152/500 train_loss:5.0997 train_time:40336ms step_avg:284.06ms
step:153/500 train_loss:5.0772 train_time:40620ms step_avg:284.06ms
step:154/500 train_loss:5.1562 train_time:40902ms step_avg:284.04ms
step:155/500 train_loss:5.0903 train_time:41182ms step_avg:284.01ms
step:156/500 train_loss:5.0708 train_time:41466ms step_avg:284.02ms
step:157/500 train_loss:5.0865 train_time:41753ms step_avg:284.03ms
step:158/500 train_loss:5.2114 train_time:42038ms step_avg:284.04ms
step:159/500 train_loss:4.9991 train_time:42321ms step_avg:284.03ms
step:160/500 train_loss:5.0562 train_time:42603ms step_avg:284.02ms
step:161/500 train_loss:4.9169 train_time:42883ms step_avg:283.99ms
step:162/500 train_loss:5.0635 train_time:43167ms step_avg:283.99ms
step:163/500 train_loss:5.1022 train_time:43452ms step_avg:284.00ms
step:164/500 train_loss:5.0862 train_time:43737ms step_avg:284.01ms
step:165/500 train_loss:4.9151 train_time:44021ms step_avg:284.00ms
step:166/500 train_loss:5.0324 train_time:44304ms step_avg:284.00ms
step:167/500 train_loss:5.1847 train_time:44583ms step_avg:283.97ms
step:168/500 train_loss:4.9645 train_time:44869ms step_avg:283.98ms
step:169/500 train_loss:5.0444 train_time:45154ms step_avg:283.99ms
step:170/500 train_loss:4.9115 train_time:45439ms step_avg:283.99ms
step:171/500 train_loss:4.8530 train_time:45721ms step_avg:283.98ms
step:172/500 train_loss:4.9582 train_time:46002ms step_avg:283.96ms
step:173/500 train_loss:4.9372 train_time:46285ms step_avg:283.95ms
step:174/500 train_loss:4.9932 train_time:46566ms step_avg:283.94ms
step:175/500 train_loss:5.1348 train_time:46852ms step_avg:283.95ms
step:176/500 train_loss:5.0184 train_time:47138ms step_avg:283.96ms
step:177/500 train_loss:4.8548 train_time:47422ms step_avg:283.96ms
step:178/500 train_loss:4.8347 train_time:47704ms step_avg:283.95ms
step:179/500 train_loss:4.8709 train_time:47984ms step_avg:283.93ms
step:180/500 train_loss:4.9147 train_time:48269ms step_avg:283.93ms
step:181/500 train_loss:4.8996 train_time:48555ms step_avg:283.95ms
step:182/500 train_loss:5.0154 train_time:48842ms step_avg:283.96ms
step:183/500 train_loss:4.9044 train_time:49122ms step_avg:283.94ms
step:184/500 train_loss:4.8295 train_time:49404ms step_avg:283.93ms
step:185/500 train_loss:4.8603 train_time:49688ms step_avg:283.93ms
step:186/500 train_loss:4.9847 train_time:49975ms step_avg:283.95ms
step:187/500 train_loss:4.8711 train_time:50260ms step_avg:283.95ms
step:188/500 train_loss:5.1081 train_time:50542ms step_avg:283.94ms
step:189/500 train_loss:4.9049 train_time:51070ms step_avg:285.31ms
step:190/500 train_loss:4.8192 train_time:51646ms step_avg:286.92ms
step:191/500 train_loss:4.9810 train_time:51935ms step_avg:286.93ms
step:192/500 train_loss:4.8178 train_time:52221ms step_avg:286.93ms
step:193/500 train_loss:4.7447 train_time:52503ms step_avg:286.90ms
step:194/500 train_loss:4.9420 train_time:52786ms step_avg:286.88ms
step:195/500 train_loss:4.8807 train_time:53071ms step_avg:286.87ms
step:196/500 train_loss:5.0656 train_time:53359ms step_avg:286.88ms
step:197/500 train_loss:4.9597 train_time:53640ms step_avg:286.85ms
step:198/500 train_loss:4.8002 train_time:53922ms step_avg:286.82ms
step:199/500 train_loss:4.8415 train_time:54203ms step_avg:286.79ms
step:200/500 train_loss:4.7339 train_time:54487ms step_avg:286.78ms
step:201/500 train_loss:4.8175 train_time:54771ms step_avg:286.76ms w_mean:1.000 w_std:0.010 w_min:1.000 w_max:1.666
step:202/500 train_loss:4.7432 train_time:55057ms step_avg:286.76ms
step:203/500 train_loss:4.9687 train_time:55340ms step_avg:286.74ms
step:204/500 train_loss:4.8666 train_time:55623ms step_avg:286.72ms
step:205/500 train_loss:4.8353 train_time:55906ms step_avg:286.70ms
step:206/500 train_loss:4.9898 train_time:56191ms step_avg:286.69ms
step:207/500 train_loss:4.6682 train_time:56476ms step_avg:286.68ms
step:208/500 train_loss:4.8134 train_time:56761ms step_avg:286.67ms
step:209/500 train_loss:4.7675 train_time:57044ms step_avg:286.65ms
step:210/500 train_loss:4.9329 train_time:57327ms step_avg:286.63ms
step:211/500 train_loss:4.8513 train_time:57610ms step_avg:286.62ms
step:212/500 train_loss:4.7445 train_time:57895ms step_avg:286.61ms
step:213/500 train_loss:4.8796 train_time:58180ms step_avg:286.60ms
step:214/500 train_loss:4.7164 train_time:58462ms step_avg:286.58ms
step:215/500 train_loss:4.8092 train_time:58744ms step_avg:286.56ms
step:216/500 train_loss:4.6675 train_time:59028ms step_avg:286.55ms
step:217/500 train_loss:4.7835 train_time:59315ms step_avg:286.55ms
step:218/500 train_loss:4.7633 train_time:59599ms step_avg:286.53ms
step:219/500 train_loss:4.7408 train_time:59882ms step_avg:286.51ms
step:220/500 train_loss:4.7513 train_time:60163ms step_avg:286.49ms
step:221/500 train_loss:4.7783 train_time:60446ms step_avg:286.48ms
step:222/500 train_loss:4.8126 train_time:60731ms step_avg:286.47ms
step:223/500 train_loss:4.7558 train_time:61016ms step_avg:286.46ms
step:224/500 train_loss:4.7587 train_time:61300ms step_avg:286.45ms
step:225/500 train_loss:4.8817 train_time:61582ms step_avg:286.43ms
step:226/500 train_loss:4.6299 train_time:61866ms step_avg:286.42ms
step:227/500 train_loss:4.6533 train_time:62152ms step_avg:286.42ms
step:228/500 train_loss:4.6486 train_time:62439ms step_avg:286.42ms
step:229/500 train_loss:4.8119 train_time:62721ms step_avg:286.40ms
step:230/500 train_loss:4.6356 train_time:63004ms step_avg:286.38ms
step:231/500 train_loss:4.7941 train_time:63284ms step_avg:286.35ms
step:232/500 train_loss:4.6542 train_time:63568ms step_avg:286.34ms
step:233/500 train_loss:4.6171 train_time:63854ms step_avg:286.34ms
step:234/500 train_loss:4.8252 train_time:64139ms step_avg:286.34ms
step:235/500 train_loss:4.6593 train_time:64421ms step_avg:286.31ms
step:236/500 train_loss:4.5984 train_time:64702ms step_avg:286.29ms
step:237/500 train_loss:4.8507 train_time:64982ms step_avg:286.26ms
step:238/500 train_loss:4.7298 train_time:65267ms step_avg:286.26ms
step:239/500 train_loss:4.6422 train_time:65552ms step_avg:286.25ms
step:240/500 train_loss:4.7814 train_time:65838ms step_avg:286.25ms
step:241/500 train_loss:4.7672 train_time:66121ms step_avg:286.24ms
step:242/500 train_loss:4.6745 train_time:66402ms step_avg:286.22ms
step:243/500 train_loss:4.8288 train_time:66684ms step_avg:286.20ms
step:244/500 train_loss:4.6670 train_time:66968ms step_avg:286.19ms
step:245/500 train_loss:4.6772 train_time:67257ms step_avg:286.20ms
step:246/500 train_loss:4.7470 train_time:67541ms step_avg:286.19ms
step:247/500 train_loss:4.7002 train_time:67824ms step_avg:286.18ms
step:248/500 train_loss:4.6591 train_time:68106ms step_avg:286.16ms
step:249/500 train_loss:4.8211 train_time:68390ms step_avg:286.15ms
step:250/500 train_loss:4.5633 train_time:68676ms step_avg:286.15ms
step:250/500 val_loss:4.6696 train_time:68676ms step_avg:286.15ms
step:251/500 train_loss:4.6054 train_time:68944ms step_avg:286.07ms w_mean:1.000 w_std:0.011 w_min:1.000 w_max:1.666
step:252/500 train_loss:4.7338 train_time:69232ms step_avg:286.08ms
step:253/500 train_loss:4.7278 train_time:69517ms step_avg:286.08ms
step:254/500 train_loss:4.6031 train_time:69800ms step_avg:286.07ms
step:255/500 train_loss:4.6270 train_time:70083ms step_avg:286.05ms
step:256/500 train_loss:4.7601 train_time:70365ms step_avg:286.04ms
step:257/500 train_loss:4.6998 train_time:70650ms step_avg:286.03ms
step:258/500 train_loss:4.6732 train_time:70935ms step_avg:286.03ms
step:259/500 train_loss:4.6074 train_time:71220ms step_avg:286.02ms
step:260/500 train_loss:4.6214 train_time:71501ms step_avg:286.01ms
step:261/500 train_loss:4.6908 train_time:71785ms step_avg:286.00ms
step:262/500 train_loss:4.6998 train_time:72069ms step_avg:285.99ms
step:263/500 train_loss:4.6038 train_time:72353ms step_avg:285.98ms
step:264/500 train_loss:4.5514 train_time:72638ms step_avg:285.98ms
step:265/500 train_loss:4.6064 train_time:72921ms step_avg:285.96ms
step:266/500 train_loss:4.4574 train_time:73205ms step_avg:285.96ms
step:267/500 train_loss:4.5182 train_time:73490ms step_avg:285.95ms
step:268/500 train_loss:4.5614 train_time:73775ms step_avg:285.95ms
step:269/500 train_loss:4.5181 train_time:74060ms step_avg:285.94ms
step:270/500 train_loss:4.4897 train_time:74341ms step_avg:285.93ms
step:271/500 train_loss:4.7076 train_time:74624ms step_avg:285.92ms
step:272/500 train_loss:4.6453 train_time:74910ms step_avg:285.91ms
step:273/500 train_loss:4.5026 train_time:75195ms step_avg:285.91ms
step:274/500 train_loss:4.5484 train_time:75479ms step_avg:285.90ms
step:275/500 train_loss:4.6726 train_time:75760ms step_avg:285.89ms
step:276/500 train_loss:4.6775 train_time:76041ms step_avg:285.87ms
step:277/500 train_loss:4.8809 train_time:76324ms step_avg:285.86ms
step:278/500 train_loss:4.6250 train_time:76609ms step_avg:285.86ms
step:279/500 train_loss:4.7596 train_time:76895ms step_avg:285.85ms
step:280/500 train_loss:4.6112 train_time:77179ms step_avg:285.85ms
step:281/500 train_loss:4.6630 train_time:77460ms step_avg:285.83ms
step:282/500 train_loss:4.5671 train_time:77742ms step_avg:285.82ms
step:283/500 train_loss:4.6836 train_time:78025ms step_avg:285.81ms
step:284/500 train_loss:4.5026 train_time:78310ms step_avg:285.80ms
step:285/500 train_loss:4.6713 train_time:78595ms step_avg:285.80ms
step:286/500 train_loss:4.6538 train_time:78880ms step_avg:285.80ms
step:287/500 train_loss:4.6856 train_time:79162ms step_avg:285.78ms
step:288/500 train_loss:4.5510 train_time:79441ms step_avg:285.76ms
step:289/500 train_loss:4.6149 train_time:79724ms step_avg:285.75ms
step:290/500 train_loss:4.4765 train_time:80010ms step_avg:285.75ms
step:291/500 train_loss:4.4746 train_time:80296ms step_avg:285.75ms
step:292/500 train_loss:4.5990 train_time:80579ms step_avg:285.74ms
step:293/500 train_loss:4.4855 train_time:80863ms step_avg:285.73ms
step:294/500 train_loss:4.5370 train_time:81142ms step_avg:285.71ms
step:295/500 train_loss:4.5578 train_time:81425ms step_avg:285.70ms
step:296/500 train_loss:4.4254 train_time:81709ms step_avg:285.70ms
step:297/500 train_loss:4.4142 train_time:81996ms step_avg:285.70ms
step:298/500 train_loss:4.4419 train_time:82280ms step_avg:285.69ms
step:299/500 train_loss:4.5485 train_time:82561ms step_avg:285.68ms
step:300/500 train_loss:4.4320 train_time:82842ms step_avg:285.66ms
step:301/500 train_loss:4.6133 train_time:83126ms step_avg:285.66ms w_mean:1.000 w_std:0.007 w_min:1.000 w_max:1.666
step:302/500 train_loss:4.5841 train_time:83411ms step_avg:285.65ms
step:303/500 train_loss:4.5097 train_time:83696ms step_avg:285.65ms
step:304/500 train_loss:4.5717 train_time:83980ms step_avg:285.65ms
step:305/500 train_loss:4.5576 train_time:84262ms step_avg:285.63ms
step:306/500 train_loss:5.0281 train_time:84542ms step_avg:285.61ms
step:307/500 train_loss:4.5156 train_time:84825ms step_avg:285.61ms
step:308/500 train_loss:4.4128 train_time:85114ms step_avg:285.62ms
step:309/500 train_loss:4.6071 train_time:85399ms step_avg:285.62ms
step:310/500 train_loss:4.4040 train_time:85681ms step_avg:285.60ms
step:311/500 train_loss:4.6383 train_time:85961ms step_avg:285.59ms
step:312/500 train_loss:4.5505 train_time:86242ms step_avg:285.57ms
step:313/500 train_loss:4.4671 train_time:86525ms step_avg:285.56ms
step:314/500 train_loss:4.5909 train_time:86812ms step_avg:285.57ms
step:315/500 train_loss:4.7257 train_time:87098ms step_avg:285.57ms
step:316/500 train_loss:4.5596 train_time:87380ms step_avg:285.56ms
step:317/500 train_loss:4.4516 train_time:87663ms step_avg:285.55ms
step:318/500 train_loss:4.4609 train_time:87943ms step_avg:285.53ms
step:319/500 train_loss:4.4758 train_time:88228ms step_avg:285.53ms
step:320/500 train_loss:4.4229 train_time:88512ms step_avg:285.52ms
step:321/500 train_loss:4.5193 train_time:88797ms step_avg:285.52ms
step:322/500 train_loss:4.5332 train_time:89082ms step_avg:285.52ms
step:323/500 train_loss:4.4928 train_time:89363ms step_avg:285.51ms
step:324/500 train_loss:4.5646 train_time:89645ms step_avg:285.49ms
step:325/500 train_loss:4.5555 train_time:89931ms step_avg:285.50ms
step:326/500 train_loss:4.6367 train_time:90216ms step_avg:285.49ms
step:327/500 train_loss:4.4831 train_time:90499ms step_avg:285.49ms
step:328/500 train_loss:4.9187 train_time:90781ms step_avg:285.47ms
step:329/500 train_loss:4.6321 train_time:91064ms step_avg:285.47ms
step:330/500 train_loss:4.4219 train_time:91344ms step_avg:285.45ms
step:331/500 train_loss:4.4024 train_time:91629ms step_avg:285.45ms
step:332/500 train_loss:4.5375 train_time:91914ms step_avg:285.45ms
step:333/500 train_loss:4.4579 train_time:92199ms step_avg:285.45ms
step:334/500 train_loss:4.4505 train_time:92481ms step_avg:285.44ms
step:335/500 train_loss:4.4016 train_time:92764ms step_avg:285.43ms
step:336/500 train_loss:4.6055 train_time:93046ms step_avg:285.42ms
step:337/500 train_loss:4.5365 train_time:93330ms step_avg:285.41ms
step:338/500 train_loss:5.0806 train_time:93616ms step_avg:285.41ms
step:339/500 train_loss:4.5127 train_time:93900ms step_avg:285.41ms
step:340/500 train_loss:4.4821 train_time:94182ms step_avg:285.40ms
step:341/500 train_loss:4.4559 train_time:94465ms step_avg:285.39ms
step:342/500 train_loss:4.4053 train_time:94748ms step_avg:285.39ms
step:343/500 train_loss:4.3754 train_time:95032ms step_avg:285.38ms
step:344/500 train_loss:4.4437 train_time:95318ms step_avg:285.38ms
step:345/500 train_loss:4.5315 train_time:95601ms step_avg:285.38ms
step:346/500 train_loss:4.4258 train_time:95883ms step_avg:285.37ms
step:347/500 train_loss:4.3755 train_time:96164ms step_avg:285.35ms
step:348/500 train_loss:4.4356 train_time:96446ms step_avg:285.34ms
step:349/500 train_loss:4.4236 train_time:96730ms step_avg:285.34ms
step:350/500 train_loss:4.3479 train_time:97017ms step_avg:285.34ms
step:351/500 train_loss:4.0345 train_time:97300ms step_avg:285.34ms w_mean:1.000 w_std:0.009 w_min:1.000 w_max:1.666
step:352/500 train_loss:4.3245 train_time:97581ms step_avg:285.33ms
step:353/500 train_loss:4.6733 train_time:97864ms step_avg:285.32ms
step:354/500 train_loss:4.2113 train_time:98144ms step_avg:285.30ms
step:355/500 train_loss:4.4621 train_time:98429ms step_avg:285.30ms
step:356/500 train_loss:4.3746 train_time:98714ms step_avg:285.30ms
step:357/500 train_loss:4.4643 train_time:98999ms step_avg:285.30ms
step:358/500 train_loss:4.4872 train_time:99281ms step_avg:285.29ms
step:359/500 train_loss:4.3758 train_time:99561ms step_avg:285.28ms
step:360/500 train_loss:4.7212 train_time:99844ms step_avg:285.27ms
step:361/500 train_loss:4.1109 train_time:100129ms step_avg:285.27ms
step:362/500 train_loss:4.5930 train_time:100415ms step_avg:285.27ms
step:363/500 train_loss:4.4934 train_time:100700ms step_avg:285.27ms
step:364/500 train_loss:4.3753 train_time:100982ms step_avg:285.26ms
step:365/500 train_loss:4.3113 train_time:101262ms step_avg:285.25ms
step:366/500 train_loss:4.4690 train_time:101545ms step_avg:285.24ms
step:367/500 train_loss:4.3919 train_time:101830ms step_avg:285.24ms
step:368/500 train_loss:4.3802 train_time:102116ms step_avg:285.24ms
step:369/500 train_loss:4.3909 train_time:102399ms step_avg:285.23ms
step:370/500 train_loss:4.2772 train_time:102682ms step_avg:285.23ms
step:371/500 train_loss:4.4264 train_time:102965ms step_avg:285.22ms
step:372/500 train_loss:4.3680 train_time:103247ms step_avg:285.21ms
step:373/500 train_loss:4.2436 train_time:103531ms step_avg:285.21ms
step:374/500 train_loss:4.4303 train_time:103818ms step_avg:285.21ms
step:375/500 train_loss:4.3640 train_time:104101ms step_avg:285.21ms
step:375/500 val_loss:4.3818 train_time:104102ms step_avg:285.21ms
step:376/500 train_loss:4.3675 train_time:104370ms step_avg:285.16ms
step:377/500 train_loss:4.4173 train_time:104653ms step_avg:285.16ms
step:378/500 train_loss:4.3154 train_time:105180ms step_avg:285.81ms
step:379/500 train_loss:4.3650 train_time:105466ms step_avg:285.81ms
step:380/500 train_loss:4.4418 train_time:106061ms step_avg:286.65ms
step:381/500 train_loss:4.4775 train_time:106346ms step_avg:286.65ms
step:382/500 train_loss:4.4219 train_time:106626ms step_avg:286.63ms
step:383/500 train_loss:4.4003 train_time:106907ms step_avg:286.61ms
step:384/500 train_loss:4.2946 train_time:107192ms step_avg:286.61ms
step:385/500 train_loss:4.3990 train_time:107478ms step_avg:286.61ms
step:386/500 train_loss:4.3109 train_time:107764ms step_avg:286.61ms
step:387/500 train_loss:4.4451 train_time:108047ms step_avg:286.60ms
step:388/500 train_loss:4.6415 train_time:108329ms step_avg:286.58ms
step:389/500 train_loss:4.3351 train_time:108612ms step_avg:286.57ms
step:390/500 train_loss:4.2908 train_time:108898ms step_avg:286.57ms
step:391/500 train_loss:4.4265 train_time:109185ms step_avg:286.57ms
step:392/500 train_loss:4.3447 train_time:109467ms step_avg:286.56ms
step:393/500 train_loss:4.4497 train_time:109749ms step_avg:286.55ms
step:394/500 train_loss:4.2732 train_time:110031ms step_avg:286.54ms
step:395/500 train_loss:4.4026 train_time:110315ms step_avg:286.53ms
step:396/500 train_loss:4.1963 train_time:110604ms step_avg:286.54ms
step:397/500 train_loss:4.3529 train_time:110887ms step_avg:286.53ms
step:398/500 train_loss:4.4507 train_time:111172ms step_avg:286.53ms
step:399/500 train_loss:4.4097 train_time:111458ms step_avg:286.52ms
step:400/500 train_loss:4.3135 train_time:111743ms step_avg:286.52ms
step:401/500 train_loss:4.3881 train_time:112028ms step_avg:286.52ms w_mean:1.000 w_std:0.007 w_min:1.000 w_max:1.666
step:402/500 train_loss:4.4199 train_time:112310ms step_avg:286.51ms
step:403/500 train_loss:4.3866 train_time:112599ms step_avg:286.51ms
step:404/500 train_loss:4.4810 train_time:112885ms step_avg:286.51ms
step:405/500 train_loss:4.2730 train_time:113168ms step_avg:286.50ms
step:406/500 train_loss:4.3109 train_time:113452ms step_avg:286.49ms
step:407/500 train_loss:4.5895 train_time:113735ms step_avg:286.49ms
step:408/500 train_loss:4.3496 train_time:114021ms step_avg:286.48ms
step:409/500 train_loss:4.3427 train_time:114307ms step_avg:286.48ms
step:410/500 train_loss:4.3934 train_time:114590ms step_avg:286.48ms
step:411/500 train_loss:4.2806 train_time:114874ms step_avg:286.47ms
step:412/500 train_loss:4.2949 train_time:115156ms step_avg:286.46ms
step:413/500 train_loss:4.7228 train_time:115441ms step_avg:286.46ms
step:414/500 train_loss:4.1626 train_time:115725ms step_avg:286.45ms
step:415/500 train_loss:4.5237 train_time:116006ms step_avg:286.43ms
step:416/500 train_loss:4.3009 train_time:116287ms step_avg:286.42ms
step:417/500 train_loss:4.2966 train_time:116570ms step_avg:286.41ms
step:418/500 train_loss:4.4802 train_time:116854ms step_avg:286.41ms
step:419/500 train_loss:4.2120 train_time:117141ms step_avg:286.41ms
step:420/500 train_loss:4.3109 train_time:117427ms step_avg:286.41ms
step:421/500 train_loss:4.2878 train_time:117707ms step_avg:286.39ms
step:422/500 train_loss:4.1659 train_time:117988ms step_avg:286.38ms
step:423/500 train_loss:4.2726 train_time:118269ms step_avg:286.37ms
step:424/500 train_loss:4.3874 train_time:118561ms step_avg:286.38ms
step:425/500 train_loss:4.2020 train_time:118840ms step_avg:286.36ms
step:426/500 train_loss:4.3518 train_time:119125ms step_avg:286.36ms
step:427/500 train_loss:4.2405 train_time:119407ms step_avg:286.35ms
step:428/500 train_loss:4.4145 train_time:119690ms step_avg:286.34ms
step:429/500 train_loss:4.3672 train_time:119976ms step_avg:286.34ms
step:430/500 train_loss:4.2774 train_time:120268ms step_avg:286.35ms
step:431/500 train_loss:4.2569 train_time:120548ms step_avg:286.34ms
step:432/500 train_loss:4.2208 train_time:120829ms step_avg:286.33ms
step:433/500 train_loss:4.2805 train_time:121112ms step_avg:286.32ms
step:434/500 train_loss:4.3601 train_time:121397ms step_avg:286.31ms
step:435/500 train_loss:4.2992 train_time:121683ms step_avg:286.31ms
step:436/500 train_loss:4.3356 train_time:121967ms step_avg:286.31ms
step:437/500 train_loss:4.3533 train_time:122250ms step_avg:286.30ms
step:438/500 train_loss:4.2395 train_time:122532ms step_avg:286.29ms
step:439/500 train_loss:4.2534 train_time:122815ms step_avg:286.28ms
step:440/500 train_loss:4.2191 train_time:123099ms step_avg:286.28ms
step:441/500 train_loss:4.4057 train_time:123385ms step_avg:286.28ms
step:442/500 train_loss:4.3133 train_time:123667ms step_avg:286.27ms
step:443/500 train_loss:4.2832 train_time:123949ms step_avg:286.26ms
step:444/500 train_loss:4.1725 train_time:124229ms step_avg:286.24ms
step:445/500 train_loss:4.4401 train_time:124511ms step_avg:286.23ms
step:446/500 train_loss:4.3535 train_time:124794ms step_avg:286.22ms
step:447/500 train_loss:4.3489 train_time:125078ms step_avg:286.22ms
step:448/500 train_loss:4.2714 train_time:125364ms step_avg:286.22ms
step:449/500 train_loss:4.3543 train_time:125647ms step_avg:286.21ms
step:450/500 train_loss:4.1997 train_time:125928ms step_avg:286.20ms
step:451/500 train_loss:4.2296 train_time:126208ms step_avg:286.19ms w_mean:1.000 w_std:0.005 w_min:1.000 w_max:1.666
step:452/500 train_loss:4.1255 train_time:126491ms step_avg:286.18ms
step:453/500 train_loss:4.2236 train_time:126773ms step_avg:286.17ms
step:454/500 train_loss:4.1970 train_time:127056ms step_avg:286.16ms
step:455/500 train_loss:4.1717 train_time:127340ms step_avg:286.16ms
step:456/500 train_loss:4.3862 train_time:127625ms step_avg:286.15ms
step:457/500 train_loss:4.2402 train_time:127907ms step_avg:286.15ms
step:458/500 train_loss:4.3334 train_time:128188ms step_avg:286.13ms
step:459/500 train_loss:4.3596 train_time:128472ms step_avg:286.13ms
step:460/500 train_loss:4.1604 train_time:128755ms step_avg:286.12ms
step:461/500 train_loss:4.3336 train_time:129039ms step_avg:286.12ms
step:462/500 train_loss:4.2409 train_time:129323ms step_avg:286.11ms
step:463/500 train_loss:4.2185 train_time:129606ms step_avg:286.11ms
step:464/500 train_loss:4.3069 train_time:129888ms step_avg:286.10ms
step:465/500 train_loss:4.2447 train_time:130172ms step_avg:286.09ms
step:466/500 train_loss:4.2454 train_time:130455ms step_avg:286.09ms
step:467/500 train_loss:4.3747 train_time:130737ms step_avg:286.08ms
step:468/500 train_loss:4.3902 train_time:131022ms step_avg:286.07ms
step:469/500 train_loss:4.3369 train_time:131306ms step_avg:286.07ms
step:470/500 train_loss:4.2546 train_time:131588ms step_avg:286.06ms
step:471/500 train_loss:4.3374 train_time:131870ms step_avg:286.05ms
step:472/500 train_loss:4.3806 train_time:132154ms step_avg:286.05ms
step:473/500 train_loss:4.2827 train_time:132437ms step_avg:286.04ms
step:474/500 train_loss:4.2635 train_time:132723ms step_avg:286.04ms
step:475/500 train_loss:4.1329 train_time:133008ms step_avg:286.04ms
step:476/500 train_loss:4.5704 train_time:133288ms step_avg:286.03ms
step:477/500 train_loss:4.3143 train_time:133571ms step_avg:286.02ms
step:478/500 train_loss:4.1240 train_time:133856ms step_avg:286.02ms
step:479/500 train_loss:4.3178 train_time:134142ms step_avg:286.02ms
step:480/500 train_loss:4.3047 train_time:134425ms step_avg:286.01ms
step:481/500 train_loss:4.4285 train_time:134708ms step_avg:286.00ms
step:482/500 train_loss:4.2592 train_time:134992ms step_avg:286.00ms
step:483/500 train_loss:4.0728 train_time:135276ms step_avg:286.00ms
step:484/500 train_loss:4.3479 train_time:135561ms step_avg:285.99ms
step:485/500 train_loss:4.1937 train_time:135846ms step_avg:285.99ms
step:486/500 train_loss:4.2303 train_time:136127ms step_avg:285.98ms
step:487/500 train_loss:4.1758 train_time:136408ms step_avg:285.97ms
step:488/500 train_loss:4.1926 train_time:136690ms step_avg:285.96ms
step:489/500 train_loss:4.4088 train_time:136974ms step_avg:285.96ms
step:490/500 train_loss:4.2665 train_time:137258ms step_avg:285.95ms
step:491/500 train_loss:4.1661 train_time:137543ms step_avg:285.95ms
step:492/500 train_loss:4.1668 train_time:137828ms step_avg:285.95ms
step:493/500 train_loss:4.2793 train_time:138109ms step_avg:285.94ms
step:494/500 train_loss:4.1227 train_time:138393ms step_avg:285.94ms
step:495/500 train_loss:4.2738 train_time:138677ms step_avg:285.93ms
step:496/500 train_loss:4.1881 train_time:138963ms step_avg:285.93ms
step:497/500 train_loss:4.1436 train_time:139246ms step_avg:285.93ms
step:498/500 train_loss:4.2868 train_time:139528ms step_avg:285.92ms
step:499/500 train_loss:4.3728 train_time:139808ms step_avg:285.91ms
step:500/500 train_loss:4.4228 train_time:140091ms step_avg:285.90ms
step:500/500 val_loss:4.2662 train_time:140092ms step_avg:285.90ms
