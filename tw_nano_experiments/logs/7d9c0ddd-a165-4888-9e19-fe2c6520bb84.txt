====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock and keep timing accurate, but skip checkpoint writes to save disk
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        if master_process:
            print("Checkpoint saving disabled; skipping state serialization.")
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 22:36:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   34C    P0             77W /  310W |    2363MiB /  81559MiB |     12%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   34C    P0             79W /  310W |    2363MiB /  81559MiB |     29%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   41C    P0             83W /  310W |    2363MiB /  81559MiB |     31%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   36C    P0             80W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   35C    P0             78W /  310W |    2363MiB /  81559MiB |     28%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   40C    P0             80W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   38C    P0             79W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   34C    P0             79W /  310W |    2363MiB /  81559MiB |     31%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           76554      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           76555      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           76556      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           76557      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           76558      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           76559      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           76560      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           76561      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 42 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: focal
  clamp: [0.8, 1.2]
  schedule: constant
  focal_gamma: 2.0
====================================================================================================
step:0/500 val_loss:15.9787 train_time:265ms step_avg:nanms
step:1/500 train_loss:15.9775 train_time:73653ms step_avg:nanms w_mean:1.000 w_std:0.023 w_min:0.992 w_max:1.240
step:2/500 train_loss:9.3669 train_time:74719ms step_avg:nanms
step:3/500 train_loss:8.9265 train_time:74986ms step_avg:nanms
step:4/500 train_loss:8.7505 train_time:75254ms step_avg:nanms
step:5/500 train_loss:8.2532 train_time:75521ms step_avg:nanms
step:6/500 train_loss:7.7457 train_time:75796ms step_avg:nanms
step:7/500 train_loss:7.3319 train_time:76064ms step_avg:nanms
step:8/500 train_loss:7.5482 train_time:76337ms step_avg:nanms
step:9/500 train_loss:7.2888 train_time:76603ms step_avg:nanms
step:10/500 train_loss:7.1080 train_time:76873ms step_avg:nanms
step:11/500 train_loss:7.0903 train_time:267ms step_avg:nanms
step:12/500 train_loss:7.0281 train_time:541ms step_avg:nanms
step:13/500 train_loss:6.8373 train_time:808ms step_avg:269.27ms
step:14/500 train_loss:6.8353 train_time:1078ms step_avg:269.40ms
step:15/500 train_loss:6.7961 train_time:1345ms step_avg:269.06ms
step:16/500 train_loss:6.7206 train_time:1615ms step_avg:269.11ms
step:17/500 train_loss:6.7329 train_time:1882ms step_avg:268.81ms
step:18/500 train_loss:6.7589 train_time:2149ms step_avg:268.63ms
step:19/500 train_loss:6.5902 train_time:2418ms step_avg:268.61ms
step:20/500 train_loss:6.5870 train_time:2684ms step_avg:268.43ms
step:21/500 train_loss:6.2557 train_time:2953ms step_avg:268.49ms
step:22/500 train_loss:6.6484 train_time:3222ms step_avg:268.47ms
step:23/500 train_loss:6.8862 train_time:3488ms step_avg:268.30ms
step:24/500 train_loss:6.5203 train_time:3757ms step_avg:268.36ms
step:25/500 train_loss:6.6403 train_time:4028ms step_avg:268.55ms
step:26/500 train_loss:6.3592 train_time:4300ms step_avg:268.73ms
step:27/500 train_loss:6.2758 train_time:4565ms step_avg:268.54ms
step:28/500 train_loss:6.4496 train_time:4833ms step_avg:268.48ms
step:29/500 train_loss:6.1027 train_time:5101ms step_avg:268.47ms
step:30/500 train_loss:6.3849 train_time:5366ms step_avg:268.29ms
step:31/500 train_loss:6.2265 train_time:5637ms step_avg:268.41ms
step:32/500 train_loss:6.1953 train_time:5904ms step_avg:268.35ms
step:33/500 train_loss:6.0084 train_time:6171ms step_avg:268.29ms
step:34/500 train_loss:6.3474 train_time:6442ms step_avg:268.42ms
step:35/500 train_loss:6.2583 train_time:6709ms step_avg:268.35ms
step:36/500 train_loss:6.4241 train_time:6978ms step_avg:268.37ms
step:37/500 train_loss:6.3352 train_time:7246ms step_avg:268.39ms
step:38/500 train_loss:6.2288 train_time:7517ms step_avg:268.47ms
step:39/500 train_loss:6.1219 train_time:7782ms step_avg:268.35ms
step:40/500 train_loss:6.1786 train_time:8054ms step_avg:268.46ms
step:41/500 train_loss:6.0770 train_time:8321ms step_avg:268.43ms
step:42/500 train_loss:6.1199 train_time:8586ms step_avg:268.33ms
step:43/500 train_loss:5.9904 train_time:8857ms step_avg:268.40ms
step:44/500 train_loss:6.0684 train_time:9125ms step_avg:268.38ms
step:45/500 train_loss:6.0658 train_time:9395ms step_avg:268.43ms
step:46/500 train_loss:6.2475 train_time:9664ms step_avg:268.44ms
step:47/500 train_loss:6.0452 train_time:9934ms step_avg:268.49ms
step:48/500 train_loss:5.8835 train_time:10204ms step_avg:268.52ms
step:49/500 train_loss:6.1337 train_time:10472ms step_avg:268.51ms
step:50/500 train_loss:6.0011 train_time:10743ms step_avg:268.57ms
step:51/500 train_loss:6.1647 train_time:11010ms step_avg:268.53ms w_mean:1.000 w_std:0.002 w_min:1.000 w_max:1.250
step:52/500 train_loss:6.0213 train_time:11281ms step_avg:268.59ms
step:53/500 train_loss:5.8719 train_time:11549ms step_avg:268.58ms
step:54/500 train_loss:5.9919 train_time:11819ms step_avg:268.61ms
step:55/500 train_loss:5.9147 train_time:12085ms step_avg:268.55ms
step:56/500 train_loss:6.2234 train_time:12358ms step_avg:268.64ms
step:57/500 train_loss:5.9014 train_time:12625ms step_avg:268.62ms
step:58/500 train_loss:5.7829 train_time:12895ms step_avg:268.64ms
step:59/500 train_loss:5.9446 train_time:13163ms step_avg:268.64ms
step:60/500 train_loss:5.8789 train_time:13434ms step_avg:268.69ms
step:61/500 train_loss:5.9752 train_time:13704ms step_avg:268.70ms
step:62/500 train_loss:5.7698 train_time:13973ms step_avg:268.70ms
step:63/500 train_loss:5.8792 train_time:14244ms step_avg:268.75ms
step:64/500 train_loss:5.8406 train_time:14516ms step_avg:268.82ms
step:65/500 train_loss:5.7980 train_time:14783ms step_avg:268.77ms
step:66/500 train_loss:5.6735 train_time:15053ms step_avg:268.81ms
step:67/500 train_loss:5.8453 train_time:15324ms step_avg:268.84ms
step:68/500 train_loss:5.7063 train_time:15593ms step_avg:268.85ms
step:69/500 train_loss:5.9513 train_time:15863ms step_avg:268.86ms
step:70/500 train_loss:5.6194 train_time:16135ms step_avg:268.91ms
step:71/500 train_loss:5.6358 train_time:16405ms step_avg:268.93ms
step:72/500 train_loss:5.8461 train_time:16675ms step_avg:268.95ms
step:73/500 train_loss:5.7908 train_time:16943ms step_avg:268.94ms
step:74/500 train_loss:5.6754 train_time:17213ms step_avg:268.96ms
step:75/500 train_loss:5.7874 train_time:17484ms step_avg:268.99ms
step:76/500 train_loss:5.7484 train_time:17755ms step_avg:269.01ms
step:77/500 train_loss:5.7193 train_time:18023ms step_avg:269.00ms
step:78/500 train_loss:5.7978 train_time:18293ms step_avg:269.02ms
step:79/500 train_loss:5.8296 train_time:18564ms step_avg:269.05ms
step:80/500 train_loss:5.6783 train_time:18837ms step_avg:269.10ms
step:81/500 train_loss:5.7851 train_time:19105ms step_avg:269.09ms
step:82/500 train_loss:5.5398 train_time:19377ms step_avg:269.12ms
step:83/500 train_loss:5.7149 train_time:19646ms step_avg:269.12ms
step:84/500 train_loss:5.6749 train_time:19915ms step_avg:269.13ms
step:85/500 train_loss:5.6397 train_time:20186ms step_avg:269.15ms
step:86/500 train_loss:5.5068 train_time:20457ms step_avg:269.18ms
step:87/500 train_loss:5.7154 train_time:20726ms step_avg:269.17ms
step:88/500 train_loss:5.6204 train_time:20998ms step_avg:269.20ms
step:89/500 train_loss:5.6754 train_time:21267ms step_avg:269.20ms
step:90/500 train_loss:5.6587 train_time:21541ms step_avg:269.26ms
step:91/500 train_loss:5.5704 train_time:21809ms step_avg:269.24ms
step:92/500 train_loss:5.5797 train_time:22083ms step_avg:269.31ms
step:93/500 train_loss:5.6692 train_time:22355ms step_avg:269.34ms
step:94/500 train_loss:5.5265 train_time:22626ms step_avg:269.35ms
step:95/500 train_loss:5.5076 train_time:22896ms step_avg:269.36ms
step:96/500 train_loss:5.5338 train_time:23168ms step_avg:269.39ms
step:97/500 train_loss:5.4509 train_time:23441ms step_avg:269.44ms
step:98/500 train_loss:5.5213 train_time:23710ms step_avg:269.43ms
step:99/500 train_loss:5.4457 train_time:23982ms step_avg:269.46ms
step:100/500 train_loss:5.5709 train_time:24253ms step_avg:269.48ms
step:101/500 train_loss:5.5265 train_time:24525ms step_avg:269.50ms w_mean:1.000 w_std:0.001 w_min:1.000 w_max:1.250
step:102/500 train_loss:5.4343 train_time:24795ms step_avg:269.51ms
step:103/500 train_loss:5.5366 train_time:25064ms step_avg:269.51ms
step:104/500 train_loss:5.4985 train_time:25339ms step_avg:269.57ms
step:105/500 train_loss:5.3291 train_time:25607ms step_avg:269.54ms
step:106/500 train_loss:5.4390 train_time:25883ms step_avg:269.61ms
step:107/500 train_loss:5.6436 train_time:26156ms step_avg:269.65ms
step:108/500 train_loss:5.4287 train_time:26426ms step_avg:269.66ms
step:109/500 train_loss:5.1817 train_time:26696ms step_avg:269.65ms
step:110/500 train_loss:5.3887 train_time:26968ms step_avg:269.68ms
step:111/500 train_loss:5.3582 train_time:27242ms step_avg:269.72ms
step:112/500 train_loss:5.3318 train_time:27512ms step_avg:269.73ms
step:113/500 train_loss:5.4343 train_time:27784ms step_avg:269.74ms
step:114/500 train_loss:5.3647 train_time:28059ms step_avg:269.80ms
step:115/500 train_loss:5.2195 train_time:28331ms step_avg:269.82ms
step:116/500 train_loss:5.3949 train_time:28603ms step_avg:269.84ms
step:117/500 train_loss:5.2460 train_time:28874ms step_avg:269.85ms
step:118/500 train_loss:5.2349 train_time:29146ms step_avg:269.87ms
step:119/500 train_loss:5.3556 train_time:29418ms step_avg:269.89ms
step:120/500 train_loss:5.3426 train_time:29685ms step_avg:269.86ms
step:121/500 train_loss:5.2719 train_time:29957ms step_avg:269.88ms
step:122/500 train_loss:5.1645 train_time:30227ms step_avg:269.89ms
step:123/500 train_loss:5.2607 train_time:30501ms step_avg:269.92ms
step:124/500 train_loss:5.1296 train_time:30770ms step_avg:269.92ms
step:125/500 train_loss:5.4301 train_time:31045ms step_avg:269.95ms
step:125/500 val_loss:5.2545 train_time:31046ms step_avg:269.97ms
step:126/500 train_loss:5.2790 train_time:31317ms step_avg:269.98ms
step:127/500 train_loss:5.2545 train_time:31593ms step_avg:270.03ms
step:128/500 train_loss:5.3230 train_time:31869ms step_avg:270.08ms
step:129/500 train_loss:5.1770 train_time:32135ms step_avg:270.05ms
step:130/500 train_loss:5.4472 train_time:32409ms step_avg:270.07ms
step:131/500 train_loss:5.2379 train_time:32679ms step_avg:270.08ms
step:132/500 train_loss:5.2261 train_time:32953ms step_avg:270.11ms
step:133/500 train_loss:5.1764 train_time:33222ms step_avg:270.09ms
step:134/500 train_loss:5.2236 train_time:33493ms step_avg:270.11ms
step:135/500 train_loss:5.1491 train_time:33767ms step_avg:270.14ms
step:136/500 train_loss:5.2174 train_time:34038ms step_avg:270.14ms
step:137/500 train_loss:5.0223 train_time:34310ms step_avg:270.15ms
step:138/500 train_loss:5.1845 train_time:34578ms step_avg:270.14ms
step:139/500 train_loss:5.1376 train_time:34852ms step_avg:270.17ms
step:140/500 train_loss:5.1516 train_time:35123ms step_avg:270.18ms
step:141/500 train_loss:5.2014 train_time:35393ms step_avg:270.17ms
step:142/500 train_loss:5.1102 train_time:35664ms step_avg:270.18ms
step:143/500 train_loss:5.1798 train_time:35934ms step_avg:270.18ms
step:144/500 train_loss:4.9928 train_time:36207ms step_avg:270.20ms
step:145/500 train_loss:5.1444 train_time:36475ms step_avg:270.18ms
step:146/500 train_loss:5.0906 train_time:36749ms step_avg:270.22ms
step:147/500 train_loss:4.9977 train_time:37020ms step_avg:270.22ms
step:148/500 train_loss:5.1261 train_time:37292ms step_avg:270.23ms
step:149/500 train_loss:5.0959 train_time:37563ms step_avg:270.24ms
step:150/500 train_loss:5.1544 train_time:37833ms step_avg:270.24ms
step:151/500 train_loss:5.1676 train_time:38108ms step_avg:270.27ms w_mean:1.000 w_std:0.002 w_min:1.000 w_max:1.250
step:152/500 train_loss:5.0865 train_time:38380ms step_avg:270.28ms
step:153/500 train_loss:5.0631 train_time:38653ms step_avg:270.30ms
step:154/500 train_loss:5.1442 train_time:38922ms step_avg:270.29ms
step:155/500 train_loss:5.0771 train_time:39194ms step_avg:270.30ms
step:156/500 train_loss:5.0603 train_time:39466ms step_avg:270.32ms
step:157/500 train_loss:5.0699 train_time:39736ms step_avg:270.31ms
step:158/500 train_loss:5.2008 train_time:40010ms step_avg:270.34ms
step:159/500 train_loss:4.9843 train_time:40282ms step_avg:270.35ms
step:160/500 train_loss:5.0475 train_time:40553ms step_avg:270.35ms
step:161/500 train_loss:4.9014 train_time:40823ms step_avg:270.35ms
step:162/500 train_loss:5.0513 train_time:41095ms step_avg:270.36ms
step:163/500 train_loss:5.0912 train_time:41370ms step_avg:270.39ms
step:164/500 train_loss:5.0763 train_time:41643ms step_avg:270.41ms
step:165/500 train_loss:4.9036 train_time:41913ms step_avg:270.40ms
step:166/500 train_loss:5.0183 train_time:42184ms step_avg:270.41ms
step:167/500 train_loss:5.1685 train_time:42456ms step_avg:270.42ms
step:168/500 train_loss:4.9534 train_time:42727ms step_avg:270.42ms
step:169/500 train_loss:5.0368 train_time:42994ms step_avg:270.40ms
step:170/500 train_loss:4.9069 train_time:43266ms step_avg:270.41ms
step:171/500 train_loss:4.8438 train_time:43537ms step_avg:270.42ms
step:172/500 train_loss:4.9523 train_time:43811ms step_avg:270.44ms
step:173/500 train_loss:4.9257 train_time:44081ms step_avg:270.44ms
step:174/500 train_loss:4.9820 train_time:44353ms step_avg:270.44ms
step:175/500 train_loss:5.1188 train_time:44622ms step_avg:270.44ms
step:176/500 train_loss:5.0086 train_time:44894ms step_avg:270.45ms
step:177/500 train_loss:4.8414 train_time:45169ms step_avg:270.47ms
step:178/500 train_loss:4.8231 train_time:45440ms step_avg:270.47ms
step:179/500 train_loss:4.8667 train_time:45710ms step_avg:270.47ms
step:180/500 train_loss:4.9084 train_time:45981ms step_avg:270.47ms
step:181/500 train_loss:4.8959 train_time:46255ms step_avg:270.50ms
step:182/500 train_loss:5.0121 train_time:46525ms step_avg:270.50ms
step:183/500 train_loss:4.8948 train_time:46792ms step_avg:270.47ms
step:184/500 train_loss:4.8258 train_time:47066ms step_avg:270.49ms
step:185/500 train_loss:4.8611 train_time:47338ms step_avg:270.50ms
step:186/500 train_loss:4.9778 train_time:47610ms step_avg:270.51ms
step:187/500 train_loss:4.8610 train_time:47877ms step_avg:270.49ms
step:188/500 train_loss:5.1057 train_time:48151ms step_avg:270.51ms
step:189/500 train_loss:4.8952 train_time:48677ms step_avg:271.94ms
step:190/500 train_loss:4.8136 train_time:49215ms step_avg:273.42ms
step:191/500 train_loss:4.9724 train_time:49482ms step_avg:273.38ms
step:192/500 train_loss:4.8169 train_time:49750ms step_avg:273.35ms
step:193/500 train_loss:4.7376 train_time:50017ms step_avg:273.31ms
step:194/500 train_loss:4.9363 train_time:50293ms step_avg:273.33ms
step:195/500 train_loss:4.8762 train_time:50565ms step_avg:273.32ms
step:196/500 train_loss:5.0560 train_time:50831ms step_avg:273.29ms
step:197/500 train_loss:4.9491 train_time:51102ms step_avg:273.27ms
step:198/500 train_loss:4.7908 train_time:51376ms step_avg:273.28ms
step:199/500 train_loss:4.8352 train_time:51651ms step_avg:273.29ms
step:200/500 train_loss:4.7276 train_time:51919ms step_avg:273.26ms
step:201/500 train_loss:4.8119 train_time:52190ms step_avg:273.24ms w_mean:1.000 w_std:0.002 w_min:1.000 w_max:1.250
step:202/500 train_loss:4.7255 train_time:52464ms step_avg:273.25ms
step:203/500 train_loss:4.9621 train_time:52735ms step_avg:273.24ms
step:204/500 train_loss:4.8659 train_time:53006ms step_avg:273.23ms
step:205/500 train_loss:4.8388 train_time:53274ms step_avg:273.20ms
step:206/500 train_loss:4.9911 train_time:53551ms step_avg:273.22ms
step:207/500 train_loss:4.6625 train_time:53819ms step_avg:273.19ms
step:208/500 train_loss:4.8105 train_time:54096ms step_avg:273.21ms
step:209/500 train_loss:4.7644 train_time:54366ms step_avg:273.19ms
step:210/500 train_loss:4.9296 train_time:54635ms step_avg:273.17ms
step:211/500 train_loss:4.8565 train_time:54910ms step_avg:273.18ms
step:212/500 train_loss:4.7421 train_time:55181ms step_avg:273.17ms
step:213/500 train_loss:4.8691 train_time:55454ms step_avg:273.17ms
step:214/500 train_loss:4.7098 train_time:55724ms step_avg:273.16ms
step:215/500 train_loss:4.8015 train_time:55996ms step_avg:273.15ms
step:216/500 train_loss:4.6590 train_time:56270ms step_avg:273.15ms
step:217/500 train_loss:4.7768 train_time:56541ms step_avg:273.15ms
step:218/500 train_loss:4.7619 train_time:56811ms step_avg:273.13ms
step:219/500 train_loss:4.7339 train_time:57083ms step_avg:273.12ms
step:220/500 train_loss:4.7467 train_time:57355ms step_avg:273.12ms
step:221/500 train_loss:4.7752 train_time:57626ms step_avg:273.11ms
step:222/500 train_loss:4.8138 train_time:57896ms step_avg:273.09ms
step:223/500 train_loss:4.7540 train_time:58171ms step_avg:273.10ms
step:224/500 train_loss:4.7523 train_time:58444ms step_avg:273.10ms
step:225/500 train_loss:4.8793 train_time:58714ms step_avg:273.09ms
step:226/500 train_loss:4.6237 train_time:58987ms step_avg:273.09ms
step:227/500 train_loss:4.6559 train_time:59256ms step_avg:273.07ms
step:228/500 train_loss:4.6424 train_time:59528ms step_avg:273.06ms
step:229/500 train_loss:4.8053 train_time:59796ms step_avg:273.04ms
step:230/500 train_loss:4.6380 train_time:60069ms step_avg:273.04ms
step:231/500 train_loss:4.7876 train_time:60342ms step_avg:273.04ms
step:232/500 train_loss:4.6448 train_time:60612ms step_avg:273.03ms
step:233/500 train_loss:4.6163 train_time:60882ms step_avg:273.01ms
step:234/500 train_loss:4.8248 train_time:61155ms step_avg:273.01ms
step:235/500 train_loss:4.6571 train_time:61425ms step_avg:273.00ms
step:236/500 train_loss:4.6049 train_time:61694ms step_avg:272.98ms
step:237/500 train_loss:4.8420 train_time:61965ms step_avg:272.97ms
step:238/500 train_loss:4.7259 train_time:62233ms step_avg:272.95ms
step:239/500 train_loss:4.6471 train_time:62505ms step_avg:272.95ms
step:240/500 train_loss:4.7808 train_time:62776ms step_avg:272.94ms
step:241/500 train_loss:4.7665 train_time:63050ms step_avg:272.95ms
step:242/500 train_loss:4.6710 train_time:63319ms step_avg:272.93ms
step:243/500 train_loss:4.8201 train_time:63589ms step_avg:272.92ms
step:244/500 train_loss:4.6643 train_time:63861ms step_avg:272.91ms
step:245/500 train_loss:4.6730 train_time:64132ms step_avg:272.90ms
step:246/500 train_loss:4.7433 train_time:64401ms step_avg:272.89ms
step:247/500 train_loss:4.6973 train_time:64673ms step_avg:272.88ms
step:248/500 train_loss:4.6574 train_time:64949ms step_avg:272.89ms
step:249/500 train_loss:4.8353 train_time:65218ms step_avg:272.88ms
step:250/500 train_loss:4.5643 train_time:65489ms step_avg:272.87ms
step:250/500 val_loss:4.6689 train_time:65490ms step_avg:272.88ms
step:251/500 train_loss:4.6034 train_time:65762ms step_avg:272.87ms w_mean:1.000 w_std:0.003 w_min:1.000 w_max:1.250
step:252/500 train_loss:4.7366 train_time:66040ms step_avg:272.89ms
step:253/500 train_loss:4.7253 train_time:66314ms step_avg:272.90ms
step:254/500 train_loss:4.6027 train_time:66580ms step_avg:272.87ms
step:255/500 train_loss:4.6196 train_time:66850ms step_avg:272.86ms
step:256/500 train_loss:4.7593 train_time:67123ms step_avg:272.86ms
step:257/500 train_loss:4.7031 train_time:67399ms step_avg:272.87ms
step:258/500 train_loss:4.6756 train_time:67668ms step_avg:272.85ms
step:259/500 train_loss:4.6056 train_time:67938ms step_avg:272.84ms
step:260/500 train_loss:4.6203 train_time:68209ms step_avg:272.84ms
step:261/500 train_loss:4.6921 train_time:68482ms step_avg:272.84ms
step:262/500 train_loss:4.6995 train_time:68751ms step_avg:272.82ms
step:263/500 train_loss:4.6050 train_time:69021ms step_avg:272.81ms
step:264/500 train_loss:4.5528 train_time:69294ms step_avg:272.81ms
step:265/500 train_loss:4.6036 train_time:69565ms step_avg:272.80ms
step:266/500 train_loss:4.4576 train_time:69836ms step_avg:272.80ms
step:267/500 train_loss:4.5200 train_time:70105ms step_avg:272.78ms
step:268/500 train_loss:4.5607 train_time:70378ms step_avg:272.78ms
step:269/500 train_loss:4.5222 train_time:70652ms step_avg:272.79ms
step:270/500 train_loss:4.4894 train_time:70921ms step_avg:272.77ms
step:271/500 train_loss:4.7079 train_time:71193ms step_avg:272.77ms
step:272/500 train_loss:4.6423 train_time:71462ms step_avg:272.75ms
step:273/500 train_loss:4.5015 train_time:71738ms step_avg:272.77ms
step:274/500 train_loss:4.5487 train_time:72008ms step_avg:272.76ms
step:275/500 train_loss:4.6665 train_time:72279ms step_avg:272.75ms
step:276/500 train_loss:4.6846 train_time:72549ms step_avg:272.74ms
step:277/500 train_loss:4.8854 train_time:72820ms step_avg:272.73ms
step:278/500 train_loss:4.6276 train_time:73092ms step_avg:272.73ms
step:279/500 train_loss:4.7581 train_time:73361ms step_avg:272.72ms
step:280/500 train_loss:4.5982 train_time:73642ms step_avg:272.75ms
step:281/500 train_loss:4.6748 train_time:73907ms step_avg:272.72ms
step:282/500 train_loss:4.5615 train_time:74179ms step_avg:272.72ms
step:283/500 train_loss:4.6860 train_time:74449ms step_avg:272.71ms
step:284/500 train_loss:4.5011 train_time:74720ms step_avg:272.70ms
step:285/500 train_loss:4.6665 train_time:74993ms step_avg:272.70ms
step:286/500 train_loss:4.6586 train_time:75262ms step_avg:272.69ms
step:287/500 train_loss:4.6872 train_time:75536ms step_avg:272.69ms
step:288/500 train_loss:4.5543 train_time:75806ms step_avg:272.69ms
step:289/500 train_loss:4.6150 train_time:76080ms step_avg:272.69ms
step:290/500 train_loss:4.4769 train_time:76350ms step_avg:272.68ms
step:291/500 train_loss:4.4662 train_time:76620ms step_avg:272.67ms
step:292/500 train_loss:4.5990 train_time:76892ms step_avg:272.67ms
step:293/500 train_loss:4.4833 train_time:77162ms step_avg:272.66ms
step:294/500 train_loss:4.5352 train_time:77437ms step_avg:272.66ms
step:295/500 train_loss:4.5561 train_time:77708ms step_avg:272.66ms
step:296/500 train_loss:4.4247 train_time:77980ms step_avg:272.66ms
step:297/500 train_loss:4.4130 train_time:78252ms step_avg:272.65ms
step:298/500 train_loss:4.4451 train_time:78520ms step_avg:272.64ms
step:299/500 train_loss:4.5481 train_time:78792ms step_avg:272.64ms
step:300/500 train_loss:4.4320 train_time:79064ms step_avg:272.63ms
step:301/500 train_loss:4.6149 train_time:79338ms step_avg:272.64ms w_mean:1.000 w_std:0.002 w_min:1.000 w_max:1.250
step:302/500 train_loss:4.5838 train_time:79608ms step_avg:272.63ms
step:303/500 train_loss:4.5084 train_time:79880ms step_avg:272.63ms
step:304/500 train_loss:4.5701 train_time:80152ms step_avg:272.62ms
step:305/500 train_loss:4.5577 train_time:80422ms step_avg:272.62ms
step:306/500 train_loss:5.0401 train_time:80693ms step_avg:272.61ms
step:307/500 train_loss:4.5136 train_time:80963ms step_avg:272.60ms
step:308/500 train_loss:4.4177 train_time:81238ms step_avg:272.61ms
step:309/500 train_loss:4.6050 train_time:81507ms step_avg:272.60ms
step:310/500 train_loss:4.4023 train_time:81779ms step_avg:272.60ms
step:311/500 train_loss:4.6384 train_time:82052ms step_avg:272.60ms
step:312/500 train_loss:4.5507 train_time:82320ms step_avg:272.58ms
step:313/500 train_loss:4.4645 train_time:82593ms step_avg:272.58ms
step:314/500 train_loss:4.5907 train_time:82861ms step_avg:272.57ms
step:315/500 train_loss:4.7228 train_time:83139ms step_avg:272.59ms
step:316/500 train_loss:4.5537 train_time:83409ms step_avg:272.58ms
step:317/500 train_loss:4.4448 train_time:83681ms step_avg:272.58ms
step:318/500 train_loss:4.4583 train_time:83951ms step_avg:272.57ms
step:319/500 train_loss:4.4735 train_time:84222ms step_avg:272.56ms
step:320/500 train_loss:4.4259 train_time:84496ms step_avg:272.57ms
step:321/500 train_loss:4.5135 train_time:84769ms step_avg:272.57ms
step:322/500 train_loss:4.5279 train_time:85039ms step_avg:272.56ms
step:323/500 train_loss:4.4932 train_time:85311ms step_avg:272.56ms
step:324/500 train_loss:4.5636 train_time:85580ms step_avg:272.55ms
step:325/500 train_loss:4.5651 train_time:85853ms step_avg:272.55ms
step:326/500 train_loss:4.6222 train_time:86122ms step_avg:272.54ms
step:327/500 train_loss:4.4756 train_time:86397ms step_avg:272.55ms
step:328/500 train_loss:4.9221 train_time:86669ms step_avg:272.54ms
step:329/500 train_loss:4.6327 train_time:86941ms step_avg:272.54ms
step:330/500 train_loss:4.4240 train_time:87211ms step_avg:272.53ms
step:331/500 train_loss:4.3838 train_time:87484ms step_avg:272.53ms
step:332/500 train_loss:4.5394 train_time:87756ms step_avg:272.54ms
step:333/500 train_loss:4.4561 train_time:88028ms step_avg:272.53ms
step:334/500 train_loss:4.4449 train_time:88299ms step_avg:272.53ms
step:335/500 train_loss:4.4043 train_time:88569ms step_avg:272.52ms
step:336/500 train_loss:4.5993 train_time:88842ms step_avg:272.52ms
step:337/500 train_loss:4.5304 train_time:89112ms step_avg:272.51ms
step:338/500 train_loss:5.0818 train_time:89381ms step_avg:272.50ms
step:339/500 train_loss:4.5048 train_time:89655ms step_avg:272.51ms
step:340/500 train_loss:4.4718 train_time:89928ms step_avg:272.51ms
step:341/500 train_loss:4.4582 train_time:90199ms step_avg:272.50ms
step:342/500 train_loss:4.4004 train_time:90469ms step_avg:272.50ms
step:343/500 train_loss:4.3700 train_time:90741ms step_avg:272.50ms
step:344/500 train_loss:4.4390 train_time:91015ms step_avg:272.50ms
step:345/500 train_loss:4.5295 train_time:91287ms step_avg:272.50ms
step:346/500 train_loss:4.4253 train_time:91560ms step_avg:272.50ms
step:347/500 train_loss:4.3646 train_time:91832ms step_avg:272.50ms
step:348/500 train_loss:4.4264 train_time:92103ms step_avg:272.49ms
step:349/500 train_loss:4.4226 train_time:92376ms step_avg:272.49ms
step:350/500 train_loss:4.3477 train_time:92645ms step_avg:272.49ms
step:351/500 train_loss:4.0307 train_time:92917ms step_avg:272.48ms w_mean:1.000 w_std:0.001 w_min:1.000 w_max:1.250
step:352/500 train_loss:4.3268 train_time:93189ms step_avg:272.48ms
step:353/500 train_loss:4.6724 train_time:93460ms step_avg:272.48ms
step:354/500 train_loss:4.2052 train_time:93732ms step_avg:272.48ms
step:355/500 train_loss:4.4546 train_time:94002ms step_avg:272.47ms
step:356/500 train_loss:4.3668 train_time:94274ms step_avg:272.47ms
step:357/500 train_loss:4.4582 train_time:94542ms step_avg:272.46ms
step:358/500 train_loss:4.4721 train_time:94815ms step_avg:272.46ms
step:359/500 train_loss:4.3766 train_time:95085ms step_avg:272.45ms
step:360/500 train_loss:4.6877 train_time:95359ms step_avg:272.45ms
step:361/500 train_loss:4.1114 train_time:95631ms step_avg:272.45ms
step:362/500 train_loss:4.5878 train_time:95902ms step_avg:272.45ms
step:363/500 train_loss:4.4850 train_time:96174ms step_avg:272.45ms
step:364/500 train_loss:4.3670 train_time:96443ms step_avg:272.44ms
step:365/500 train_loss:4.3021 train_time:96713ms step_avg:272.43ms
step:366/500 train_loss:4.4602 train_time:96984ms step_avg:272.43ms
step:367/500 train_loss:4.3901 train_time:97258ms step_avg:272.43ms
step:368/500 train_loss:4.3782 train_time:97529ms step_avg:272.43ms
step:369/500 train_loss:4.3837 train_time:97800ms step_avg:272.42ms
step:370/500 train_loss:4.2749 train_time:98073ms step_avg:272.43ms
step:371/500 train_loss:4.4218 train_time:98342ms step_avg:272.42ms
step:372/500 train_loss:4.3602 train_time:98613ms step_avg:272.41ms
step:373/500 train_loss:4.2397 train_time:98882ms step_avg:272.40ms
step:374/500 train_loss:4.4298 train_time:99156ms step_avg:272.41ms
step:375/500 train_loss:4.3553 train_time:99427ms step_avg:272.40ms
step:375/500 val_loss:4.3758 train_time:99429ms step_avg:272.41ms
step:376/500 train_loss:4.3596 train_time:99699ms step_avg:272.40ms
step:377/500 train_loss:4.4163 train_time:99975ms step_avg:272.41ms
step:378/500 train_loss:4.3042 train_time:100497ms step_avg:273.09ms
step:379/500 train_loss:4.3601 train_time:100763ms step_avg:273.07ms
step:380/500 train_loss:4.4312 train_time:101294ms step_avg:273.77ms
step:381/500 train_loss:4.4661 train_time:101560ms step_avg:273.75ms
step:382/500 train_loss:4.4087 train_time:101829ms step_avg:273.74ms
step:383/500 train_loss:4.3886 train_time:102096ms step_avg:273.72ms
step:384/500 train_loss:4.2926 train_time:102375ms step_avg:273.73ms
step:385/500 train_loss:4.3953 train_time:102645ms step_avg:273.72ms
step:386/500 train_loss:4.3065 train_time:102914ms step_avg:273.71ms
step:387/500 train_loss:4.4365 train_time:103184ms step_avg:273.70ms
step:388/500 train_loss:4.6397 train_time:103457ms step_avg:273.69ms
step:389/500 train_loss:4.3300 train_time:103732ms step_avg:273.70ms
step:390/500 train_loss:4.2841 train_time:103997ms step_avg:273.68ms
step:391/500 train_loss:4.4186 train_time:104271ms step_avg:273.68ms
step:392/500 train_loss:4.3365 train_time:104545ms step_avg:273.68ms
step:393/500 train_loss:4.4466 train_time:104816ms step_avg:273.67ms
step:394/500 train_loss:4.2647 train_time:105086ms step_avg:273.66ms
step:395/500 train_loss:4.3982 train_time:105358ms step_avg:273.66ms
step:396/500 train_loss:4.1908 train_time:105635ms step_avg:273.66ms
step:397/500 train_loss:4.3464 train_time:105904ms step_avg:273.65ms
step:398/500 train_loss:4.4430 train_time:106172ms step_avg:273.64ms
step:399/500 train_loss:4.3987 train_time:106444ms step_avg:273.64ms
step:400/500 train_loss:4.3107 train_time:106719ms step_avg:273.64ms
step:401/500 train_loss:4.3768 train_time:106987ms step_avg:273.62ms w_mean:1.000 w_std:0.001 w_min:1.000 w_max:1.250
step:402/500 train_loss:4.4132 train_time:107256ms step_avg:273.61ms
step:403/500 train_loss:4.3847 train_time:107529ms step_avg:273.61ms
step:404/500 train_loss:4.4766 train_time:107797ms step_avg:273.60ms
step:405/500 train_loss:4.2590 train_time:108071ms step_avg:273.60ms
step:406/500 train_loss:4.3027 train_time:108339ms step_avg:273.58ms
step:407/500 train_loss:4.5846 train_time:108611ms step_avg:273.58ms
step:408/500 train_loss:4.3426 train_time:108878ms step_avg:273.56ms
step:409/500 train_loss:4.3343 train_time:109152ms step_avg:273.56ms
step:410/500 train_loss:4.3857 train_time:109424ms step_avg:273.56ms
step:411/500 train_loss:4.2737 train_time:109696ms step_avg:273.56ms
step:412/500 train_loss:4.2886 train_time:109966ms step_avg:273.55ms
step:413/500 train_loss:4.7100 train_time:110238ms step_avg:273.54ms
step:414/500 train_loss:4.1573 train_time:110509ms step_avg:273.54ms
step:415/500 train_loss:4.5172 train_time:110777ms step_avg:273.52ms
step:416/500 train_loss:4.2919 train_time:111051ms step_avg:273.53ms
step:417/500 train_loss:4.2886 train_time:111325ms step_avg:273.53ms
step:418/500 train_loss:4.4726 train_time:111596ms step_avg:273.52ms
step:419/500 train_loss:4.2070 train_time:111867ms step_avg:273.51ms
step:420/500 train_loss:4.3069 train_time:112137ms step_avg:273.50ms
step:421/500 train_loss:4.2769 train_time:112407ms step_avg:273.50ms
step:422/500 train_loss:4.1669 train_time:112678ms step_avg:273.49ms
step:423/500 train_loss:4.2729 train_time:112953ms step_avg:273.49ms
step:424/500 train_loss:4.3848 train_time:113223ms step_avg:273.49ms
step:425/500 train_loss:4.1875 train_time:113494ms step_avg:273.48ms
step:426/500 train_loss:4.3534 train_time:113766ms step_avg:273.48ms
step:427/500 train_loss:4.2335 train_time:114038ms step_avg:273.47ms
step:428/500 train_loss:4.4141 train_time:114308ms step_avg:273.46ms
step:429/500 train_loss:4.3587 train_time:114577ms step_avg:273.45ms
step:430/500 train_loss:4.2675 train_time:114851ms step_avg:273.46ms
step:431/500 train_loss:4.2518 train_time:115124ms step_avg:273.45ms
step:432/500 train_loss:4.1990 train_time:115395ms step_avg:273.45ms
step:433/500 train_loss:4.2814 train_time:115666ms step_avg:273.44ms
step:434/500 train_loss:4.3581 train_time:115936ms step_avg:273.43ms
step:435/500 train_loss:4.2913 train_time:116209ms step_avg:273.43ms
step:436/500 train_loss:4.3321 train_time:116476ms step_avg:273.42ms
step:437/500 train_loss:4.3453 train_time:116750ms step_avg:273.42ms
step:438/500 train_loss:4.2326 train_time:117017ms step_avg:273.40ms
step:439/500 train_loss:4.2464 train_time:117296ms step_avg:273.42ms
step:440/500 train_loss:4.2137 train_time:117564ms step_avg:273.41ms
step:441/500 train_loss:4.4030 train_time:117838ms step_avg:273.40ms
step:442/500 train_loss:4.3024 train_time:118107ms step_avg:273.40ms
step:443/500 train_loss:4.2814 train_time:118379ms step_avg:273.39ms
step:444/500 train_loss:4.1735 train_time:118652ms step_avg:273.39ms
step:445/500 train_loss:4.4262 train_time:118924ms step_avg:273.39ms
step:446/500 train_loss:4.3492 train_time:119195ms step_avg:273.38ms
step:447/500 train_loss:4.3479 train_time:119464ms step_avg:273.37ms
step:448/500 train_loss:4.2765 train_time:119738ms step_avg:273.37ms
step:449/500 train_loss:4.3534 train_time:120009ms step_avg:273.37ms
step:450/500 train_loss:4.1897 train_time:120277ms step_avg:273.36ms
step:451/500 train_loss:4.2325 train_time:120549ms step_avg:273.35ms w_mean:1.000 w_std:0.002 w_min:1.000 w_max:1.250
step:452/500 train_loss:4.1212 train_time:120818ms step_avg:273.34ms
step:453/500 train_loss:4.2164 train_time:121092ms step_avg:273.35ms
step:454/500 train_loss:4.1961 train_time:121364ms step_avg:273.34ms
step:455/500 train_loss:4.1746 train_time:121638ms step_avg:273.34ms
step:456/500 train_loss:4.3807 train_time:121906ms step_avg:273.33ms
step:457/500 train_loss:4.2316 train_time:122177ms step_avg:273.33ms
step:458/500 train_loss:4.3212 train_time:122450ms step_avg:273.33ms
step:459/500 train_loss:4.3582 train_time:122721ms step_avg:273.32ms
step:460/500 train_loss:4.1582 train_time:122992ms step_avg:273.31ms
step:461/500 train_loss:4.3294 train_time:123259ms step_avg:273.30ms
step:462/500 train_loss:4.2320 train_time:123534ms step_avg:273.31ms
step:463/500 train_loss:4.2143 train_time:123806ms step_avg:273.30ms
step:464/500 train_loss:4.3050 train_time:124077ms step_avg:273.30ms
step:465/500 train_loss:4.2411 train_time:124347ms step_avg:273.29ms
step:466/500 train_loss:4.2396 train_time:124617ms step_avg:273.28ms
step:467/500 train_loss:4.3661 train_time:124892ms step_avg:273.29ms
step:468/500 train_loss:4.3688 train_time:125163ms step_avg:273.28ms
step:469/500 train_loss:4.3337 train_time:125436ms step_avg:273.28ms
step:470/500 train_loss:4.2419 train_time:125705ms step_avg:273.27ms
step:471/500 train_loss:4.3305 train_time:125978ms step_avg:273.27ms
step:472/500 train_loss:4.3792 train_time:126250ms step_avg:273.27ms
step:473/500 train_loss:4.2815 train_time:126518ms step_avg:273.26ms
step:474/500 train_loss:4.2541 train_time:126793ms step_avg:273.26ms
step:475/500 train_loss:4.1287 train_time:127064ms step_avg:273.26ms
step:476/500 train_loss:4.5667 train_time:127338ms step_avg:273.26ms
step:477/500 train_loss:4.3092 train_time:127607ms step_avg:273.25ms
step:478/500 train_loss:4.1185 train_time:127877ms step_avg:273.24ms
step:479/500 train_loss:4.3190 train_time:128151ms step_avg:273.24ms
step:480/500 train_loss:4.2982 train_time:128422ms step_avg:273.24ms
step:481/500 train_loss:4.4251 train_time:128693ms step_avg:273.23ms
step:482/500 train_loss:4.2556 train_time:128963ms step_avg:273.23ms
step:483/500 train_loss:4.0670 train_time:129237ms step_avg:273.23ms
step:484/500 train_loss:4.3463 train_time:129507ms step_avg:273.22ms
step:485/500 train_loss:4.1957 train_time:129779ms step_avg:273.22ms
step:486/500 train_loss:4.2197 train_time:130053ms step_avg:273.22ms
step:487/500 train_loss:4.1699 train_time:130325ms step_avg:273.22ms
step:488/500 train_loss:4.1936 train_time:130596ms step_avg:273.21ms
step:489/500 train_loss:4.4046 train_time:130867ms step_avg:273.21ms
step:490/500 train_loss:4.2585 train_time:131136ms step_avg:273.20ms
step:491/500 train_loss:4.1560 train_time:131407ms step_avg:273.20ms
step:492/500 train_loss:4.1650 train_time:131680ms step_avg:273.19ms
step:493/500 train_loss:4.2749 train_time:131953ms step_avg:273.20ms
step:494/500 train_loss:4.1211 train_time:132225ms step_avg:273.19ms
step:495/500 train_loss:4.2708 train_time:132496ms step_avg:273.19ms
step:496/500 train_loss:4.1866 train_time:132768ms step_avg:273.19ms
step:497/500 train_loss:4.1374 train_time:133041ms step_avg:273.18ms
step:498/500 train_loss:4.2723 train_time:133311ms step_avg:273.18ms
step:499/500 train_loss:4.3631 train_time:133580ms step_avg:273.17ms
step:500/500 train_loss:4.4119 train_time:133855ms step_avg:273.17ms
step:500/500 val_loss:4.2623 train_time:133856ms step_avg:273.18ms
