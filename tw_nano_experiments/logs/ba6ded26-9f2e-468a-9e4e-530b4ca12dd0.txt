====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # also save the seed for reproducibility
        if args.seed is not None:
            log['seed'] = args.seed
        # save token weighting config
        if tw_config.enabled:
            log['tw_config'] = {
                'function': tw_config.function,
                'clamp_min': tw_config.clamp_min,
                'clamp_max': tw_config.clamp_max,
                'schedule': tw_config.schedule,
            }
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 21:09:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   46C    P0             85W /  310W |    2363MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |     31%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   50C    P0             90W /  310W |    2363MiB /  81559MiB |     32%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   46C    P0             86W /  310W |    2363MiB /  81559MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   43C    P0             81W /  310W |    2363MiB /  81559MiB |     30%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   49C    P0             86W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   53C    P0             88W /  310W |    2363MiB /  81559MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           56251      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           56252      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           56253      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           56254      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           56255      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           56256      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           56257      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           56258      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 123 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: linear
  clamp: [0.5, 2.0]
  schedule: constant
====================================================================================================
step:0/500 val_loss:16.0073 train_time:277ms step_avg:nanms
step:1/500 train_loss:15.9981 train_time:52742ms step_avg:nanms w_mean:1.000 w_std:0.042 w_min:0.500 w_max:1.155
step:2/500 train_loss:9.3701 train_time:53460ms step_avg:nanms
step:3/500 train_loss:9.0167 train_time:53731ms step_avg:nanms
step:4/500 train_loss:8.7552 train_time:54002ms step_avg:nanms
step:5/500 train_loss:8.1530 train_time:54271ms step_avg:nanms
step:6/500 train_loss:7.7098 train_time:54544ms step_avg:nanms
step:7/500 train_loss:7.4021 train_time:54814ms step_avg:nanms
step:8/500 train_loss:7.5647 train_time:55085ms step_avg:nanms
step:9/500 train_loss:7.3006 train_time:55358ms step_avg:nanms
step:10/500 train_loss:7.0958 train_time:55634ms step_avg:nanms
step:11/500 train_loss:7.0840 train_time:271ms step_avg:nanms
step:12/500 train_loss:7.0430 train_time:543ms step_avg:nanms
step:13/500 train_loss:6.8363 train_time:813ms step_avg:271.15ms
step:14/500 train_loss:6.9163 train_time:1085ms step_avg:271.23ms
step:15/500 train_loss:6.8314 train_time:1358ms step_avg:271.57ms
step:16/500 train_loss:6.7935 train_time:1630ms step_avg:271.65ms
step:17/500 train_loss:6.7777 train_time:1902ms step_avg:271.78ms
step:18/500 train_loss:6.8096 train_time:2174ms step_avg:271.81ms
step:19/500 train_loss:6.6315 train_time:2447ms step_avg:271.86ms
step:20/500 train_loss:6.6469 train_time:2719ms step_avg:271.86ms
step:21/500 train_loss:6.3117 train_time:2989ms step_avg:271.76ms
step:22/500 train_loss:6.7027 train_time:3258ms step_avg:271.53ms
step:23/500 train_loss:6.9001 train_time:3531ms step_avg:271.65ms
step:24/500 train_loss:6.5952 train_time:3804ms step_avg:271.70ms
step:25/500 train_loss:6.6502 train_time:4079ms step_avg:271.91ms
step:26/500 train_loss:6.4302 train_time:4350ms step_avg:271.87ms
step:27/500 train_loss:6.3138 train_time:4626ms step_avg:272.10ms
step:28/500 train_loss:6.5076 train_time:4898ms step_avg:272.13ms
step:29/500 train_loss:6.1550 train_time:5171ms step_avg:272.14ms
step:30/500 train_loss:6.4350 train_time:5443ms step_avg:272.13ms
step:31/500 train_loss:6.2923 train_time:5714ms step_avg:272.08ms
step:32/500 train_loss:6.2561 train_time:5986ms step_avg:272.07ms
step:33/500 train_loss:6.0761 train_time:6260ms step_avg:272.15ms
step:34/500 train_loss:6.4068 train_time:6531ms step_avg:272.14ms
step:35/500 train_loss:6.3036 train_time:6803ms step_avg:272.13ms
step:36/500 train_loss:6.4567 train_time:7075ms step_avg:272.11ms
step:37/500 train_loss:6.3833 train_time:7348ms step_avg:272.14ms
step:38/500 train_loss:6.2736 train_time:7619ms step_avg:272.12ms
step:39/500 train_loss:6.1673 train_time:7892ms step_avg:272.12ms
step:40/500 train_loss:6.2271 train_time:8165ms step_avg:272.16ms
step:41/500 train_loss:6.1321 train_time:8437ms step_avg:272.17ms
step:42/500 train_loss:6.1601 train_time:8710ms step_avg:272.18ms
step:43/500 train_loss:6.0367 train_time:8982ms step_avg:272.17ms
step:44/500 train_loss:6.1253 train_time:9257ms step_avg:272.26ms
step:45/500 train_loss:6.1079 train_time:9529ms step_avg:272.27ms
step:46/500 train_loss:6.2947 train_time:9802ms step_avg:272.28ms
step:47/500 train_loss:6.0913 train_time:10075ms step_avg:272.29ms
step:48/500 train_loss:5.9619 train_time:10348ms step_avg:272.30ms
step:49/500 train_loss:6.1775 train_time:10619ms step_avg:272.29ms
step:50/500 train_loss:6.0520 train_time:10892ms step_avg:272.29ms
step:51/500 train_loss:6.1892 train_time:11164ms step_avg:272.30ms w_mean:1.000 w_std:0.426 w_min:0.487 w_max:1.949
step:52/500 train_loss:6.0678 train_time:11438ms step_avg:272.33ms
step:53/500 train_loss:5.9306 train_time:11708ms step_avg:272.28ms
step:54/500 train_loss:6.0263 train_time:11980ms step_avg:272.28ms
step:55/500 train_loss:5.9958 train_time:12253ms step_avg:272.29ms
step:56/500 train_loss:6.2350 train_time:12526ms step_avg:272.31ms
step:57/500 train_loss:5.9936 train_time:12800ms step_avg:272.33ms
step:58/500 train_loss:5.8285 train_time:13073ms step_avg:272.35ms
step:59/500 train_loss:6.0176 train_time:13346ms step_avg:272.36ms
step:60/500 train_loss:5.9272 train_time:13618ms step_avg:272.37ms
step:61/500 train_loss:6.0319 train_time:13891ms step_avg:272.37ms
step:62/500 train_loss:5.8393 train_time:14165ms step_avg:272.41ms
step:63/500 train_loss:5.9230 train_time:14438ms step_avg:272.41ms
step:64/500 train_loss:5.9172 train_time:14710ms step_avg:272.40ms
step:65/500 train_loss:5.7789 train_time:14980ms step_avg:272.37ms
step:66/500 train_loss:5.7536 train_time:15252ms step_avg:272.36ms
step:67/500 train_loss:5.8849 train_time:15526ms step_avg:272.38ms
step:68/500 train_loss:5.7878 train_time:15798ms step_avg:272.39ms
step:69/500 train_loss:5.9920 train_time:16070ms step_avg:272.38ms
step:70/500 train_loss:5.7008 train_time:16343ms step_avg:272.39ms
step:71/500 train_loss:5.7100 train_time:16615ms step_avg:272.38ms
step:72/500 train_loss:5.9075 train_time:16888ms step_avg:272.38ms
step:73/500 train_loss:5.8345 train_time:17161ms step_avg:272.40ms
step:74/500 train_loss:5.7497 train_time:17434ms step_avg:272.40ms
step:75/500 train_loss:5.8457 train_time:17707ms step_avg:272.42ms
step:76/500 train_loss:5.8066 train_time:17980ms step_avg:272.42ms
step:77/500 train_loss:5.7815 train_time:18252ms step_avg:272.42ms
step:78/500 train_loss:5.8628 train_time:18524ms step_avg:272.41ms
step:79/500 train_loss:5.8914 train_time:18796ms step_avg:272.40ms
step:80/500 train_loss:5.7469 train_time:19073ms step_avg:272.47ms
step:81/500 train_loss:5.8348 train_time:19346ms step_avg:272.48ms
step:82/500 train_loss:5.6029 train_time:19619ms step_avg:272.48ms
step:83/500 train_loss:5.7862 train_time:19890ms step_avg:272.46ms
step:84/500 train_loss:5.7368 train_time:20164ms step_avg:272.49ms
step:85/500 train_loss:5.7276 train_time:20438ms step_avg:272.51ms
step:86/500 train_loss:5.5559 train_time:20711ms step_avg:272.51ms
step:87/500 train_loss:5.7983 train_time:20982ms step_avg:272.49ms
step:88/500 train_loss:5.6680 train_time:21254ms step_avg:272.49ms
step:89/500 train_loss:5.7606 train_time:21528ms step_avg:272.51ms
step:90/500 train_loss:5.7236 train_time:21801ms step_avg:272.51ms
step:91/500 train_loss:5.6473 train_time:22074ms step_avg:272.51ms
step:92/500 train_loss:5.6593 train_time:22346ms step_avg:272.51ms
step:93/500 train_loss:5.7302 train_time:22619ms step_avg:272.51ms
step:94/500 train_loss:5.6180 train_time:22892ms step_avg:272.53ms
step:95/500 train_loss:5.5553 train_time:23166ms step_avg:272.54ms
step:96/500 train_loss:5.6424 train_time:23438ms step_avg:272.54ms
step:97/500 train_loss:5.5046 train_time:23711ms step_avg:272.54ms
step:98/500 train_loss:5.6271 train_time:23983ms step_avg:272.54ms
step:99/500 train_loss:5.5025 train_time:24257ms step_avg:272.55ms
step:100/500 train_loss:5.6618 train_time:24531ms step_avg:272.56ms
step:101/500 train_loss:5.5866 train_time:24804ms step_avg:272.57ms w_mean:1.000 w_std:0.450 w_min:0.486 w_max:1.943
step:102/500 train_loss:5.5165 train_time:25077ms step_avg:272.58ms
step:103/500 train_loss:5.5887 train_time:25349ms step_avg:272.57ms
step:104/500 train_loss:5.5817 train_time:25621ms step_avg:272.56ms
step:105/500 train_loss:5.4085 train_time:25893ms step_avg:272.56ms
step:106/500 train_loss:5.5412 train_time:26166ms step_avg:272.56ms
step:107/500 train_loss:5.6801 train_time:26439ms step_avg:272.56ms
step:108/500 train_loss:5.5290 train_time:26711ms step_avg:272.56ms
step:109/500 train_loss:5.2638 train_time:26984ms step_avg:272.56ms
step:110/500 train_loss:5.4944 train_time:27257ms step_avg:272.57ms
step:111/500 train_loss:5.4328 train_time:27531ms step_avg:272.58ms
step:112/500 train_loss:5.4390 train_time:27805ms step_avg:272.59ms
step:113/500 train_loss:5.4976 train_time:28077ms step_avg:272.59ms
step:114/500 train_loss:5.4722 train_time:28350ms step_avg:272.59ms
step:115/500 train_loss:5.2894 train_time:28622ms step_avg:272.59ms
step:116/500 train_loss:5.5007 train_time:28894ms step_avg:272.59ms
step:117/500 train_loss:5.3141 train_time:29168ms step_avg:272.60ms
step:118/500 train_loss:5.3623 train_time:29441ms step_avg:272.60ms
step:119/500 train_loss:5.4294 train_time:29713ms step_avg:272.60ms
step:120/500 train_loss:5.4572 train_time:29986ms step_avg:272.60ms
step:121/500 train_loss:5.3600 train_time:30259ms step_avg:272.61ms
step:122/500 train_loss:5.2853 train_time:30532ms step_avg:272.61ms
step:123/500 train_loss:5.3343 train_time:30806ms step_avg:272.62ms
step:124/500 train_loss:5.2554 train_time:31078ms step_avg:272.62ms
step:125/500 train_loss:5.4951 train_time:31354ms step_avg:272.64ms
step:125/500 val_loss:5.3725 train_time:31355ms step_avg:272.65ms
step:126/500 train_loss:5.3845 train_time:31627ms step_avg:272.65ms
step:127/500 train_loss:5.3319 train_time:31907ms step_avg:272.71ms
step:128/500 train_loss:5.4369 train_time:32182ms step_avg:272.73ms
step:129/500 train_loss:5.2613 train_time:32453ms step_avg:272.71ms
step:130/500 train_loss:5.5475 train_time:32725ms step_avg:272.70ms
step:131/500 train_loss:5.3195 train_time:32998ms step_avg:272.71ms
step:132/500 train_loss:5.3408 train_time:33272ms step_avg:272.72ms
step:133/500 train_loss:5.2608 train_time:33545ms step_avg:272.72ms
step:134/500 train_loss:5.3302 train_time:33817ms step_avg:272.72ms
step:135/500 train_loss:5.2289 train_time:34090ms step_avg:272.72ms
step:136/500 train_loss:5.3308 train_time:34362ms step_avg:272.71ms
step:137/500 train_loss:5.1119 train_time:34635ms step_avg:272.72ms
step:138/500 train_loss:5.2797 train_time:34909ms step_avg:272.73ms
step:139/500 train_loss:5.2386 train_time:35182ms step_avg:272.73ms
step:140/500 train_loss:5.2451 train_time:35455ms step_avg:272.73ms
step:141/500 train_loss:5.2848 train_time:35728ms step_avg:272.73ms
step:142/500 train_loss:5.2022 train_time:36001ms step_avg:272.73ms
step:143/500 train_loss:5.2836 train_time:36275ms step_avg:272.75ms
step:144/500 train_loss:5.1087 train_time:36548ms step_avg:272.74ms
step:145/500 train_loss:5.2429 train_time:36821ms step_avg:272.74ms
step:146/500 train_loss:5.1928 train_time:37093ms step_avg:272.74ms
step:147/500 train_loss:5.1031 train_time:37367ms step_avg:272.75ms
step:148/500 train_loss:5.2054 train_time:37641ms step_avg:272.76ms
step:149/500 train_loss:5.1964 train_time:37914ms step_avg:272.76ms
step:150/500 train_loss:5.2310 train_time:38186ms step_avg:272.76ms
step:151/500 train_loss:5.2754 train_time:38460ms step_avg:272.76ms w_mean:1.000 w_std:0.457 w_min:0.487 w_max:1.947
step:152/500 train_loss:5.1713 train_time:38733ms step_avg:272.77ms
step:153/500 train_loss:5.1757 train_time:39008ms step_avg:272.78ms
step:154/500 train_loss:5.2112 train_time:39281ms step_avg:272.78ms
step:155/500 train_loss:5.1831 train_time:39555ms step_avg:272.79ms
step:156/500 train_loss:5.1586 train_time:39829ms step_avg:272.80ms
step:157/500 train_loss:5.1786 train_time:40102ms step_avg:272.80ms
step:158/500 train_loss:5.2946 train_time:40376ms step_avg:272.81ms
step:159/500 train_loss:5.0872 train_time:40648ms step_avg:272.81ms
step:160/500 train_loss:5.1576 train_time:40923ms step_avg:272.82ms
step:161/500 train_loss:4.9929 train_time:41195ms step_avg:272.82ms
step:162/500 train_loss:5.1655 train_time:41467ms step_avg:272.81ms
step:163/500 train_loss:5.1662 train_time:41741ms step_avg:272.82ms
step:164/500 train_loss:5.1932 train_time:42014ms step_avg:272.82ms
step:165/500 train_loss:4.9980 train_time:42286ms step_avg:272.82ms
step:166/500 train_loss:5.1303 train_time:42559ms step_avg:272.81ms
step:167/500 train_loss:5.2552 train_time:42833ms step_avg:272.82ms
step:168/500 train_loss:5.0668 train_time:43107ms step_avg:272.83ms
step:169/500 train_loss:5.1094 train_time:43381ms step_avg:272.83ms
step:170/500 train_loss:5.0112 train_time:43653ms step_avg:272.83ms
step:171/500 train_loss:4.9453 train_time:43927ms step_avg:272.84ms
step:172/500 train_loss:5.0583 train_time:44201ms step_avg:272.84ms
step:173/500 train_loss:5.0187 train_time:44475ms step_avg:272.85ms
step:174/500 train_loss:5.0884 train_time:44749ms step_avg:272.86ms
step:175/500 train_loss:5.2001 train_time:45022ms step_avg:272.86ms
step:176/500 train_loss:5.1114 train_time:45295ms step_avg:272.86ms
step:177/500 train_loss:4.9485 train_time:45568ms step_avg:272.86ms
step:178/500 train_loss:4.9225 train_time:45843ms step_avg:272.88ms
step:179/500 train_loss:4.9693 train_time:46116ms step_avg:272.88ms
step:180/500 train_loss:4.9965 train_time:46388ms step_avg:272.87ms
step:181/500 train_loss:5.0020 train_time:46660ms step_avg:272.87ms
step:182/500 train_loss:5.0895 train_time:46933ms step_avg:272.86ms
step:183/500 train_loss:5.0132 train_time:47207ms step_avg:272.87ms
step:184/500 train_loss:4.9171 train_time:47481ms step_avg:272.88ms
step:185/500 train_loss:4.9607 train_time:47755ms step_avg:272.88ms
step:186/500 train_loss:5.0688 train_time:48029ms step_avg:272.89ms
step:187/500 train_loss:4.9658 train_time:48302ms step_avg:272.89ms
step:188/500 train_loss:5.1818 train_time:48575ms step_avg:272.89ms
step:189/500 train_loss:5.0008 train_time:49111ms step_avg:274.36ms
step:190/500 train_loss:4.9133 train_time:49720ms step_avg:276.22ms
step:191/500 train_loss:5.0694 train_time:49992ms step_avg:276.20ms
step:192/500 train_loss:4.9179 train_time:50263ms step_avg:276.17ms
step:193/500 train_loss:4.8344 train_time:50535ms step_avg:276.15ms
step:194/500 train_loss:5.0329 train_time:50811ms step_avg:276.15ms
step:195/500 train_loss:4.9648 train_time:51083ms step_avg:276.13ms
step:196/500 train_loss:5.1578 train_time:51355ms step_avg:276.10ms
step:197/500 train_loss:5.0321 train_time:51626ms step_avg:276.08ms
step:198/500 train_loss:4.9049 train_time:51899ms step_avg:276.06ms
step:199/500 train_loss:4.9233 train_time:52176ms step_avg:276.06ms
step:200/500 train_loss:4.8436 train_time:52448ms step_avg:276.04ms
step:201/500 train_loss:4.9063 train_time:52720ms step_avg:276.02ms w_mean:1.000 w_std:0.474 w_min:0.485 w_max:1.942
step:202/500 train_loss:4.8557 train_time:52992ms step_avg:276.00ms
step:203/500 train_loss:5.0315 train_time:53265ms step_avg:275.99ms
step:204/500 train_loss:4.9843 train_time:53538ms step_avg:275.97ms
step:205/500 train_loss:4.9033 train_time:53811ms step_avg:275.95ms
step:206/500 train_loss:5.0862 train_time:54083ms step_avg:275.93ms
step:207/500 train_loss:4.7478 train_time:54354ms step_avg:275.91ms
step:208/500 train_loss:4.9215 train_time:54628ms step_avg:275.90ms
step:209/500 train_loss:4.8509 train_time:54901ms step_avg:275.88ms
step:210/500 train_loss:5.0310 train_time:55174ms step_avg:275.87ms
step:211/500 train_loss:4.9544 train_time:55448ms step_avg:275.86ms
step:212/500 train_loss:4.8425 train_time:55723ms step_avg:275.86ms
step:213/500 train_loss:4.9723 train_time:55995ms step_avg:275.84ms
step:214/500 train_loss:4.8132 train_time:56267ms step_avg:275.82ms
step:215/500 train_loss:4.8992 train_time:56540ms step_avg:275.80ms
step:216/500 train_loss:4.7818 train_time:56813ms step_avg:275.79ms
step:217/500 train_loss:4.8826 train_time:57085ms step_avg:275.77ms
step:218/500 train_loss:4.8666 train_time:57357ms step_avg:275.75ms
step:219/500 train_loss:4.8230 train_time:57630ms step_avg:275.74ms
step:220/500 train_loss:4.8546 train_time:57904ms step_avg:275.73ms
step:221/500 train_loss:4.8662 train_time:58177ms step_avg:275.72ms
step:222/500 train_loss:4.9095 train_time:58450ms step_avg:275.71ms
step:223/500 train_loss:4.8442 train_time:58725ms step_avg:275.70ms
step:224/500 train_loss:4.8625 train_time:58996ms step_avg:275.68ms
step:225/500 train_loss:4.9588 train_time:59268ms step_avg:275.66ms
step:226/500 train_loss:4.7240 train_time:59541ms step_avg:275.65ms
step:227/500 train_loss:4.7483 train_time:59814ms step_avg:275.64ms
step:228/500 train_loss:4.7536 train_time:60087ms step_avg:275.63ms
step:229/500 train_loss:4.8915 train_time:60361ms step_avg:275.62ms
step:230/500 train_loss:4.7471 train_time:60634ms step_avg:275.61ms
step:231/500 train_loss:4.8696 train_time:60908ms step_avg:275.60ms
step:232/500 train_loss:4.7707 train_time:61181ms step_avg:275.59ms
step:233/500 train_loss:4.6945 train_time:61453ms step_avg:275.57ms
step:234/500 train_loss:4.9229 train_time:61727ms step_avg:275.57ms
step:235/500 train_loss:4.7462 train_time:62001ms step_avg:275.56ms
step:236/500 train_loss:4.6992 train_time:62274ms step_avg:275.55ms
step:237/500 train_loss:4.9409 train_time:62548ms step_avg:275.54ms
step:238/500 train_loss:4.8160 train_time:62820ms step_avg:275.53ms
step:239/500 train_loss:4.7450 train_time:63092ms step_avg:275.51ms
step:240/500 train_loss:4.8630 train_time:63366ms step_avg:275.51ms
step:241/500 train_loss:4.8660 train_time:63639ms step_avg:275.49ms
step:242/500 train_loss:4.7565 train_time:63912ms step_avg:275.48ms
step:243/500 train_loss:4.9261 train_time:64184ms step_avg:275.47ms
step:244/500 train_loss:4.7490 train_time:64456ms step_avg:275.45ms
step:245/500 train_loss:4.7843 train_time:64729ms step_avg:275.44ms
step:246/500 train_loss:4.8269 train_time:65002ms step_avg:275.43ms
step:247/500 train_loss:4.8003 train_time:65275ms step_avg:275.42ms
step:248/500 train_loss:4.7456 train_time:65548ms step_avg:275.41ms
step:249/500 train_loss:4.9232 train_time:65820ms step_avg:275.40ms
step:250/500 train_loss:4.6610 train_time:66094ms step_avg:275.39ms
step:250/500 val_loss:4.7576 train_time:66095ms step_avg:275.40ms
step:251/500 train_loss:4.6926 train_time:66364ms step_avg:275.37ms w_mean:1.000 w_std:0.482 w_min:0.485 w_max:1.941
step:252/500 train_loss:4.8309 train_time:66639ms step_avg:275.37ms
step:253/500 train_loss:4.8045 train_time:66915ms step_avg:275.37ms
step:254/500 train_loss:4.7136 train_time:67186ms step_avg:275.35ms
step:255/500 train_loss:4.7177 train_time:67459ms step_avg:275.34ms
step:256/500 train_loss:4.8409 train_time:67731ms step_avg:275.33ms
step:257/500 train_loss:4.7888 train_time:68006ms step_avg:275.33ms
step:258/500 train_loss:4.7745 train_time:68279ms step_avg:275.32ms
step:259/500 train_loss:4.6988 train_time:68552ms step_avg:275.31ms
step:260/500 train_loss:4.7209 train_time:68825ms step_avg:275.30ms
step:261/500 train_loss:4.7779 train_time:69098ms step_avg:275.29ms
step:262/500 train_loss:4.7803 train_time:69370ms step_avg:275.28ms
step:263/500 train_loss:4.6995 train_time:69644ms step_avg:275.27ms
step:264/500 train_loss:4.6487 train_time:69920ms step_avg:275.27ms
step:265/500 train_loss:4.6984 train_time:70192ms step_avg:275.26ms
step:266/500 train_loss:4.5747 train_time:70464ms step_avg:275.25ms
step:267/500 train_loss:4.6077 train_time:70736ms step_avg:275.24ms
step:268/500 train_loss:4.6646 train_time:71009ms step_avg:275.23ms
step:269/500 train_loss:4.6086 train_time:71284ms step_avg:275.23ms
step:270/500 train_loss:4.6019 train_time:71556ms step_avg:275.22ms
step:271/500 train_loss:4.7860 train_time:71830ms step_avg:275.21ms
step:272/500 train_loss:4.7437 train_time:72103ms step_avg:275.20ms
step:273/500 train_loss:4.5959 train_time:72375ms step_avg:275.19ms
step:274/500 train_loss:4.6564 train_time:72646ms step_avg:275.18ms
step:275/500 train_loss:4.7544 train_time:72919ms step_avg:275.17ms
step:276/500 train_loss:4.7705 train_time:73193ms step_avg:275.16ms
step:277/500 train_loss:4.9652 train_time:73465ms step_avg:275.15ms
step:278/500 train_loss:4.7141 train_time:73737ms step_avg:275.14ms
step:279/500 train_loss:4.8510 train_time:74013ms step_avg:275.14ms
step:280/500 train_loss:4.6971 train_time:74286ms step_avg:275.13ms
step:281/500 train_loss:4.7519 train_time:74558ms step_avg:275.12ms
step:282/500 train_loss:4.6642 train_time:74829ms step_avg:275.11ms
step:283/500 train_loss:4.7692 train_time:75106ms step_avg:275.11ms
step:284/500 train_loss:4.5961 train_time:75379ms step_avg:275.11ms
step:285/500 train_loss:4.7529 train_time:75652ms step_avg:275.10ms
step:286/500 train_loss:4.7474 train_time:75925ms step_avg:275.09ms
step:287/500 train_loss:4.7672 train_time:76198ms step_avg:275.08ms
step:288/500 train_loss:4.6430 train_time:76471ms step_avg:275.08ms
step:289/500 train_loss:4.7040 train_time:76744ms step_avg:275.07ms
step:290/500 train_loss:4.5647 train_time:77017ms step_avg:275.06ms
step:291/500 train_loss:4.5742 train_time:77291ms step_avg:275.06ms
step:292/500 train_loss:4.6941 train_time:77564ms step_avg:275.05ms
step:293/500 train_loss:4.5837 train_time:77838ms step_avg:275.05ms
step:294/500 train_loss:4.6315 train_time:78111ms step_avg:275.04ms
step:295/500 train_loss:4.6545 train_time:78385ms step_avg:275.04ms
step:296/500 train_loss:4.5230 train_time:78659ms step_avg:275.03ms
step:297/500 train_loss:4.5149 train_time:78934ms step_avg:275.03ms
step:298/500 train_loss:4.5385 train_time:79206ms step_avg:275.02ms
step:299/500 train_loss:4.6396 train_time:79479ms step_avg:275.02ms
step:300/500 train_loss:4.5257 train_time:79753ms step_avg:275.01ms
step:301/500 train_loss:4.7104 train_time:80027ms step_avg:275.01ms w_mean:1.000 w_std:0.479 w_min:0.486 w_max:1.944
step:302/500 train_loss:4.6662 train_time:80299ms step_avg:275.00ms
step:303/500 train_loss:4.6037 train_time:80578ms step_avg:275.01ms
step:304/500 train_loss:4.6571 train_time:80850ms step_avg:275.00ms
step:305/500 train_loss:4.6445 train_time:81123ms step_avg:274.99ms
step:306/500 train_loss:5.0939 train_time:81395ms step_avg:274.98ms
step:307/500 train_loss:4.6103 train_time:81668ms step_avg:274.98ms
step:308/500 train_loss:4.5098 train_time:81940ms step_avg:274.97ms
step:309/500 train_loss:4.6876 train_time:82213ms step_avg:274.96ms
step:310/500 train_loss:4.5092 train_time:82487ms step_avg:274.96ms
step:311/500 train_loss:4.7181 train_time:82760ms step_avg:274.95ms
step:312/500 train_loss:4.6530 train_time:83033ms step_avg:274.94ms
step:313/500 train_loss:4.5479 train_time:83306ms step_avg:274.94ms
step:314/500 train_loss:4.6842 train_time:83579ms step_avg:274.93ms
step:315/500 train_loss:4.8059 train_time:83856ms step_avg:274.94ms
step:316/500 train_loss:4.6578 train_time:84130ms step_avg:274.93ms
step:317/500 train_loss:4.5400 train_time:84403ms step_avg:274.93ms
step:318/500 train_loss:4.5552 train_time:84677ms step_avg:274.92ms
step:319/500 train_loss:4.5673 train_time:84949ms step_avg:274.91ms
step:320/500 train_loss:4.5236 train_time:85222ms step_avg:274.91ms
step:321/500 train_loss:4.6115 train_time:85495ms step_avg:274.90ms
step:322/500 train_loss:4.6200 train_time:85767ms step_avg:274.90ms
step:323/500 train_loss:4.5841 train_time:86042ms step_avg:274.89ms
step:324/500 train_loss:4.6514 train_time:86312ms step_avg:274.88ms
step:325/500 train_loss:4.6423 train_time:86586ms step_avg:274.88ms
step:326/500 train_loss:4.7128 train_time:86859ms step_avg:274.87ms
step:327/500 train_loss:4.5643 train_time:87132ms step_avg:274.87ms
step:328/500 train_loss:4.9981 train_time:87405ms step_avg:274.86ms
step:329/500 train_loss:4.7163 train_time:87680ms step_avg:274.86ms
step:330/500 train_loss:4.5200 train_time:87951ms step_avg:274.85ms
step:331/500 train_loss:4.4848 train_time:88225ms step_avg:274.84ms
step:332/500 train_loss:4.6260 train_time:88498ms step_avg:274.84ms
step:333/500 train_loss:4.5585 train_time:88770ms step_avg:274.83ms
step:334/500 train_loss:4.5357 train_time:89043ms step_avg:274.82ms
step:335/500 train_loss:4.5026 train_time:89315ms step_avg:274.82ms
step:336/500 train_loss:4.6807 train_time:89589ms step_avg:274.81ms
step:337/500 train_loss:4.6372 train_time:89862ms step_avg:274.81ms
step:338/500 train_loss:5.1300 train_time:90134ms step_avg:274.80ms
step:339/500 train_loss:4.6038 train_time:90408ms step_avg:274.80ms
step:340/500 train_loss:4.5643 train_time:90681ms step_avg:274.79ms
step:341/500 train_loss:4.5508 train_time:90954ms step_avg:274.79ms
step:342/500 train_loss:4.4970 train_time:91227ms step_avg:274.78ms
step:343/500 train_loss:4.4783 train_time:91500ms step_avg:274.77ms
step:344/500 train_loss:4.5341 train_time:91772ms step_avg:274.77ms
step:345/500 train_loss:4.6176 train_time:92045ms step_avg:274.76ms
step:346/500 train_loss:4.5168 train_time:92319ms step_avg:274.76ms
step:347/500 train_loss:4.4666 train_time:92592ms step_avg:274.75ms
step:348/500 train_loss:4.5308 train_time:92864ms step_avg:274.75ms
step:349/500 train_loss:4.5171 train_time:93136ms step_avg:274.74ms
step:350/500 train_loss:4.4424 train_time:93409ms step_avg:274.73ms
step:351/500 train_loss:4.1458 train_time:93685ms step_avg:274.74ms w_mean:1.000 w_std:0.492 w_min:0.490 w_max:1.960
step:352/500 train_loss:4.4224 train_time:93958ms step_avg:274.73ms
step:353/500 train_loss:4.7531 train_time:94231ms step_avg:274.73ms
step:354/500 train_loss:4.3190 train_time:94504ms step_avg:274.72ms
step:355/500 train_loss:4.5450 train_time:94777ms step_avg:274.72ms
step:356/500 train_loss:4.4664 train_time:95049ms step_avg:274.71ms
step:357/500 train_loss:4.5488 train_time:95322ms step_avg:274.70ms
step:358/500 train_loss:4.5676 train_time:95594ms step_avg:274.70ms
step:359/500 train_loss:4.4615 train_time:95866ms step_avg:274.69ms
step:360/500 train_loss:4.8122 train_time:96139ms step_avg:274.68ms
step:361/500 train_loss:4.2105 train_time:96413ms step_avg:274.68ms
step:362/500 train_loss:4.6734 train_time:96691ms step_avg:274.69ms
step:363/500 train_loss:4.5767 train_time:96958ms step_avg:274.67ms
step:364/500 train_loss:4.4681 train_time:97231ms step_avg:274.67ms
step:365/500 train_loss:4.4048 train_time:97506ms step_avg:274.66ms
step:366/500 train_loss:4.5603 train_time:97779ms step_avg:274.66ms
step:367/500 train_loss:4.4839 train_time:98053ms step_avg:274.66ms
step:368/500 train_loss:4.4740 train_time:98326ms step_avg:274.65ms
step:369/500 train_loss:4.4781 train_time:98599ms step_avg:274.65ms
step:370/500 train_loss:4.3838 train_time:98872ms step_avg:274.65ms
step:371/500 train_loss:4.5070 train_time:99146ms step_avg:274.64ms
step:372/500 train_loss:4.4651 train_time:99419ms step_avg:274.64ms
step:373/500 train_loss:4.3319 train_time:99692ms step_avg:274.63ms
step:374/500 train_loss:4.5181 train_time:99964ms step_avg:274.63ms
step:375/500 train_loss:4.4619 train_time:100236ms step_avg:274.62ms
step:375/500 val_loss:4.4717 train_time:100237ms step_avg:274.62ms
step:376/500 train_loss:4.4489 train_time:100507ms step_avg:274.61ms
step:377/500 train_loss:4.5136 train_time:100785ms step_avg:274.62ms
step:378/500 train_loss:4.4105 train_time:101314ms step_avg:275.31ms
step:379/500 train_loss:4.4401 train_time:101586ms step_avg:275.30ms
step:380/500 train_loss:4.5471 train_time:102185ms step_avg:276.18ms
step:381/500 train_loss:4.5490 train_time:102456ms step_avg:276.16ms
step:382/500 train_loss:4.5109 train_time:102729ms step_avg:276.15ms
step:383/500 train_loss:4.4928 train_time:102999ms step_avg:276.14ms
step:384/500 train_loss:4.3915 train_time:103275ms step_avg:276.14ms
step:385/500 train_loss:4.4857 train_time:103547ms step_avg:276.13ms
step:386/500 train_loss:4.4068 train_time:103819ms step_avg:276.11ms
step:387/500 train_loss:4.5316 train_time:104091ms step_avg:276.10ms
step:388/500 train_loss:4.7247 train_time:104364ms step_avg:276.09ms
step:389/500 train_loss:4.4256 train_time:104636ms step_avg:276.09ms
step:390/500 train_loss:4.3870 train_time:104910ms step_avg:276.08ms
step:391/500 train_loss:4.5090 train_time:105182ms step_avg:276.07ms
step:392/500 train_loss:4.4423 train_time:105455ms step_avg:276.06ms
step:393/500 train_loss:4.5328 train_time:105727ms step_avg:276.05ms
step:394/500 train_loss:4.3737 train_time:106000ms step_avg:276.04ms
step:395/500 train_loss:4.4966 train_time:106273ms step_avg:276.03ms
step:396/500 train_loss:4.2873 train_time:106547ms step_avg:276.03ms
step:397/500 train_loss:4.4429 train_time:106819ms step_avg:276.02ms
step:398/500 train_loss:4.5438 train_time:107092ms step_avg:276.01ms
step:399/500 train_loss:4.4785 train_time:107365ms step_avg:276.00ms
step:400/500 train_loss:4.4149 train_time:107638ms step_avg:275.99ms
step:401/500 train_loss:4.4874 train_time:107910ms step_avg:275.99ms w_mean:1.000 w_std:0.488 w_min:0.486 w_max:1.943
step:402/500 train_loss:4.5069 train_time:108183ms step_avg:275.98ms
step:403/500 train_loss:4.4801 train_time:108456ms step_avg:275.97ms
step:404/500 train_loss:4.5655 train_time:108728ms step_avg:275.96ms
step:405/500 train_loss:4.3631 train_time:109002ms step_avg:275.96ms
step:406/500 train_loss:4.4037 train_time:109275ms step_avg:275.95ms
step:407/500 train_loss:4.6719 train_time:109547ms step_avg:275.94ms
step:408/500 train_loss:4.4423 train_time:109822ms step_avg:275.93ms
step:409/500 train_loss:4.4304 train_time:110096ms step_avg:275.93ms
step:410/500 train_loss:4.4850 train_time:110369ms step_avg:275.92ms
step:411/500 train_loss:4.3694 train_time:110641ms step_avg:275.91ms
step:412/500 train_loss:4.3850 train_time:110912ms step_avg:275.90ms
step:413/500 train_loss:4.7990 train_time:111186ms step_avg:275.90ms
step:414/500 train_loss:4.2652 train_time:111461ms step_avg:275.89ms
step:415/500 train_loss:4.6069 train_time:111734ms step_avg:275.89ms
step:416/500 train_loss:4.4005 train_time:112010ms step_avg:275.89ms
step:417/500 train_loss:4.3848 train_time:112282ms step_avg:275.88ms
step:418/500 train_loss:4.5574 train_time:112556ms step_avg:275.87ms
step:419/500 train_loss:4.3098 train_time:112830ms step_avg:275.87ms
step:420/500 train_loss:4.4035 train_time:113102ms step_avg:275.86ms
step:421/500 train_loss:4.3877 train_time:113375ms step_avg:275.85ms
step:422/500 train_loss:4.2680 train_time:113647ms step_avg:275.84ms
step:423/500 train_loss:4.3678 train_time:113921ms step_avg:275.84ms
step:424/500 train_loss:4.4781 train_time:114194ms step_avg:275.83ms
step:425/500 train_loss:4.3070 train_time:114467ms step_avg:275.83ms
step:426/500 train_loss:4.4593 train_time:114740ms step_avg:275.82ms
step:427/500 train_loss:4.3346 train_time:115011ms step_avg:275.81ms
step:428/500 train_loss:4.5007 train_time:115283ms step_avg:275.80ms
step:429/500 train_loss:4.4629 train_time:115556ms step_avg:275.79ms
step:430/500 train_loss:4.3696 train_time:115829ms step_avg:275.78ms
step:431/500 train_loss:4.3550 train_time:116103ms step_avg:275.78ms
step:432/500 train_loss:4.3203 train_time:116376ms step_avg:275.77ms
step:433/500 train_loss:4.3816 train_time:116648ms step_avg:275.76ms
step:434/500 train_loss:4.4600 train_time:116923ms step_avg:275.76ms
step:435/500 train_loss:4.3922 train_time:117197ms step_avg:275.76ms
step:436/500 train_loss:4.4308 train_time:117470ms step_avg:275.75ms
step:437/500 train_loss:4.4489 train_time:117743ms step_avg:275.74ms
step:438/500 train_loss:4.3422 train_time:118014ms step_avg:275.73ms
step:439/500 train_loss:4.3520 train_time:118289ms step_avg:275.73ms
step:440/500 train_loss:4.2995 train_time:118562ms step_avg:275.73ms
step:441/500 train_loss:4.4970 train_time:118836ms step_avg:275.72ms
step:442/500 train_loss:4.4075 train_time:119110ms step_avg:275.72ms
step:443/500 train_loss:4.3714 train_time:119384ms step_avg:275.71ms
step:444/500 train_loss:4.2708 train_time:119656ms step_avg:275.71ms
step:445/500 train_loss:4.5179 train_time:119929ms step_avg:275.70ms
step:446/500 train_loss:4.4493 train_time:120203ms step_avg:275.69ms
step:447/500 train_loss:4.4464 train_time:120477ms step_avg:275.69ms
step:448/500 train_loss:4.3713 train_time:120749ms step_avg:275.68ms
step:449/500 train_loss:4.4431 train_time:121022ms step_avg:275.68ms
step:450/500 train_loss:4.2912 train_time:121296ms step_avg:275.67ms
step:451/500 train_loss:4.3364 train_time:121568ms step_avg:275.66ms w_mean:1.000 w_std:0.493 w_min:0.487 w_max:1.948
step:452/500 train_loss:4.2263 train_time:121840ms step_avg:275.66ms
step:453/500 train_loss:4.3263 train_time:122113ms step_avg:275.65ms
step:454/500 train_loss:4.3011 train_time:122386ms step_avg:275.64ms
step:455/500 train_loss:4.2733 train_time:122661ms step_avg:275.64ms
step:456/500 train_loss:4.4722 train_time:122935ms step_avg:275.64ms
step:457/500 train_loss:4.3311 train_time:123210ms step_avg:275.64ms
step:458/500 train_loss:4.4206 train_time:123485ms step_avg:275.64ms
step:459/500 train_loss:4.4579 train_time:123757ms step_avg:275.63ms
step:460/500 train_loss:4.2623 train_time:124030ms step_avg:275.62ms
step:461/500 train_loss:4.4271 train_time:124303ms step_avg:275.62ms
step:462/500 train_loss:4.3385 train_time:124576ms step_avg:275.61ms
step:463/500 train_loss:4.3213 train_time:124849ms step_avg:275.60ms
step:464/500 train_loss:4.4047 train_time:125122ms step_avg:275.60ms
step:465/500 train_loss:4.3385 train_time:125396ms step_avg:275.60ms
step:466/500 train_loss:4.3427 train_time:125668ms step_avg:275.59ms
step:467/500 train_loss:4.4661 train_time:125940ms step_avg:275.58ms
step:468/500 train_loss:4.4722 train_time:126212ms step_avg:275.57ms
step:469/500 train_loss:4.4268 train_time:126486ms step_avg:275.57ms
step:470/500 train_loss:4.3425 train_time:126758ms step_avg:275.56ms
step:471/500 train_loss:4.4368 train_time:127032ms step_avg:275.56ms
step:472/500 train_loss:4.4775 train_time:127304ms step_avg:275.55ms
step:473/500 train_loss:4.3822 train_time:127578ms step_avg:275.55ms
step:474/500 train_loss:4.3560 train_time:127850ms step_avg:275.54ms
step:475/500 train_loss:4.2363 train_time:128123ms step_avg:275.53ms
step:476/500 train_loss:4.6472 train_time:128396ms step_avg:275.53ms
step:477/500 train_loss:4.4057 train_time:128669ms step_avg:275.52ms
step:478/500 train_loss:4.2259 train_time:128942ms step_avg:275.52ms
step:479/500 train_loss:4.4083 train_time:129214ms step_avg:275.51ms
step:480/500 train_loss:4.3980 train_time:129489ms step_avg:275.51ms
step:481/500 train_loss:4.5154 train_time:129763ms step_avg:275.50ms
step:482/500 train_loss:4.3583 train_time:130035ms step_avg:275.50ms
step:483/500 train_loss:4.1813 train_time:130308ms step_avg:275.49ms
step:484/500 train_loss:4.4466 train_time:130583ms step_avg:275.49ms
step:485/500 train_loss:4.3010 train_time:130857ms step_avg:275.49ms
step:486/500 train_loss:4.3220 train_time:131130ms step_avg:275.48ms
step:487/500 train_loss:4.2779 train_time:131404ms step_avg:275.48ms
step:488/500 train_loss:4.2911 train_time:131677ms step_avg:275.48ms
step:489/500 train_loss:4.4978 train_time:131950ms step_avg:275.47ms
step:490/500 train_loss:4.3604 train_time:132222ms step_avg:275.46ms
step:491/500 train_loss:4.2610 train_time:132496ms step_avg:275.46ms
step:492/500 train_loss:4.2679 train_time:132769ms step_avg:275.45ms
step:493/500 train_loss:4.3761 train_time:133041ms step_avg:275.45ms
step:494/500 train_loss:4.2267 train_time:133314ms step_avg:275.44ms
step:495/500 train_loss:4.3702 train_time:133587ms step_avg:275.44ms
step:496/500 train_loss:4.2935 train_time:133861ms step_avg:275.44ms
step:497/500 train_loss:4.2491 train_time:134133ms step_avg:275.43ms
step:498/500 train_loss:4.3730 train_time:134407ms step_avg:275.42ms
step:499/500 train_loss:4.4603 train_time:134682ms step_avg:275.42ms
step:500/500 train_loss:4.5140 train_time:134955ms step_avg:275.42ms
step:500/500 val_loss:4.3623 train_time:134956ms step_avg:275.42ms
