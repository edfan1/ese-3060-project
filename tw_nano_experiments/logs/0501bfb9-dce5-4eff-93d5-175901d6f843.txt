====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # also save the seed for reproducibility
        if args.seed is not None:
            log['seed'] = args.seed
        # save token weighting config
        if tw_config.enabled:
            log['tw_config'] = {
                'function': tw_config.function,
                'clamp_min': tw_config.clamp_min,
                'clamp_max': tw_config.clamp_max,
                'schedule': tw_config.schedule,
            }
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 21:40:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   45C    P0             83W /  310W |    2363MiB /  81559MiB |     14%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             85W /  310W |    2363MiB /  81559MiB |     32%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   48C    P0             88W /  310W |    2363MiB /  81559MiB |      6%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             86W /  310W |    2363MiB /  81559MiB |     22%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   44C    P0             81W /  310W |    2363MiB /  81559MiB |     35%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0             86W /  310W |    2363MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   52C    P0             87W /  310W |    2363MiB /  81559MiB |     21%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   45C    P0             85W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           68563      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           68564      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           68565      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           68566      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           68567      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           68568      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           68569      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           68570      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 456 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: log
  clamp: [0.8, 1.3]
  schedule: constant
====================================================================================================
step:0/500 val_loss:16.0011 train_time:264ms step_avg:nanms
step:1/500 train_loss:15.9963 train_time:60524ms step_avg:nanms w_mean:1.000 w_std:0.000 w_min:1.000 w_max:1.000
step:2/500 train_loss:9.3784 train_time:61108ms step_avg:nanms
step:3/500 train_loss:8.9730 train_time:61390ms step_avg:nanms
step:4/500 train_loss:8.6926 train_time:61672ms step_avg:nanms
step:5/500 train_loss:8.2171 train_time:61955ms step_avg:nanms
step:6/500 train_loss:7.7159 train_time:62237ms step_avg:nanms
step:7/500 train_loss:7.3418 train_time:62521ms step_avg:nanms
step:8/500 train_loss:7.5474 train_time:62803ms step_avg:nanms
step:9/500 train_loss:7.3343 train_time:63086ms step_avg:nanms
step:10/500 train_loss:7.0993 train_time:63367ms step_avg:nanms
step:11/500 train_loss:7.0796 train_time:282ms step_avg:nanms
step:12/500 train_loss:7.0157 train_time:564ms step_avg:nanms
step:13/500 train_loss:6.8342 train_time:846ms step_avg:282.04ms
step:14/500 train_loss:6.8161 train_time:1129ms step_avg:282.35ms
step:15/500 train_loss:6.8031 train_time:1415ms step_avg:282.99ms
step:16/500 train_loss:6.7201 train_time:1697ms step_avg:282.79ms
step:17/500 train_loss:6.7284 train_time:1981ms step_avg:283.00ms
step:18/500 train_loss:6.7461 train_time:2264ms step_avg:282.95ms
step:19/500 train_loss:6.5814 train_time:2545ms step_avg:282.77ms
step:20/500 train_loss:6.5892 train_time:2829ms step_avg:282.89ms
step:21/500 train_loss:6.2502 train_time:3113ms step_avg:283.03ms
step:22/500 train_loss:6.6500 train_time:3397ms step_avg:283.05ms
step:23/500 train_loss:6.8799 train_time:3682ms step_avg:283.22ms
step:24/500 train_loss:6.5254 train_time:3965ms step_avg:283.23ms
step:25/500 train_loss:6.6315 train_time:4248ms step_avg:283.22ms
step:26/500 train_loss:6.3646 train_time:4533ms step_avg:283.29ms
step:27/500 train_loss:6.2774 train_time:4815ms step_avg:283.24ms
step:28/500 train_loss:6.4557 train_time:5099ms step_avg:283.29ms
step:29/500 train_loss:6.1073 train_time:5383ms step_avg:283.29ms
step:30/500 train_loss:6.3911 train_time:5665ms step_avg:283.25ms
step:31/500 train_loss:6.2340 train_time:5948ms step_avg:283.25ms
step:32/500 train_loss:6.1960 train_time:6233ms step_avg:283.31ms
step:33/500 train_loss:6.0118 train_time:6520ms step_avg:283.48ms
step:34/500 train_loss:6.3457 train_time:6803ms step_avg:283.44ms
step:35/500 train_loss:6.2569 train_time:7086ms step_avg:283.42ms
step:36/500 train_loss:6.4228 train_time:7369ms step_avg:283.41ms
step:37/500 train_loss:6.3434 train_time:7656ms step_avg:283.54ms
step:38/500 train_loss:6.2266 train_time:7939ms step_avg:283.54ms
step:39/500 train_loss:6.1229 train_time:8223ms step_avg:283.57ms
step:40/500 train_loss:6.1770 train_time:8506ms step_avg:283.54ms
step:41/500 train_loss:6.0823 train_time:8790ms step_avg:283.55ms
step:42/500 train_loss:6.1161 train_time:9078ms step_avg:283.68ms
step:43/500 train_loss:5.9954 train_time:9361ms step_avg:283.67ms
step:44/500 train_loss:6.0729 train_time:9645ms step_avg:283.68ms
step:45/500 train_loss:6.0588 train_time:9929ms step_avg:283.70ms
step:46/500 train_loss:6.2394 train_time:10214ms step_avg:283.72ms
step:47/500 train_loss:6.0493 train_time:10499ms step_avg:283.74ms
step:48/500 train_loss:5.8867 train_time:10783ms step_avg:283.78ms
step:49/500 train_loss:6.1343 train_time:11066ms step_avg:283.75ms
step:50/500 train_loss:6.0007 train_time:11351ms step_avg:283.78ms
step:51/500 train_loss:6.1545 train_time:11636ms step_avg:283.80ms w_mean:1.000 w_std:0.104 w_min:0.941 w_max:1.528
step:52/500 train_loss:6.0154 train_time:11922ms step_avg:283.86ms
step:53/500 train_loss:5.8712 train_time:12208ms step_avg:283.90ms
step:54/500 train_loss:5.9906 train_time:12493ms step_avg:283.92ms
step:55/500 train_loss:5.9196 train_time:12780ms step_avg:284.00ms
step:56/500 train_loss:6.2143 train_time:13064ms step_avg:284.01ms
step:57/500 train_loss:5.9126 train_time:13348ms step_avg:284.00ms
step:58/500 train_loss:5.7920 train_time:13632ms step_avg:283.99ms
step:59/500 train_loss:5.9493 train_time:13920ms step_avg:284.09ms
step:60/500 train_loss:5.8860 train_time:14205ms step_avg:284.10ms
step:61/500 train_loss:5.9804 train_time:14490ms step_avg:284.11ms
step:62/500 train_loss:5.7823 train_time:14776ms step_avg:284.15ms
step:63/500 train_loss:5.8762 train_time:15062ms step_avg:284.19ms
step:64/500 train_loss:5.8496 train_time:15347ms step_avg:284.20ms
step:65/500 train_loss:5.8206 train_time:15631ms step_avg:284.21ms
step:66/500 train_loss:5.6822 train_time:15919ms step_avg:284.27ms
step:67/500 train_loss:5.8450 train_time:16204ms step_avg:284.28ms
step:68/500 train_loss:5.7067 train_time:16488ms step_avg:284.28ms
step:69/500 train_loss:5.9563 train_time:16774ms step_avg:284.31ms
step:70/500 train_loss:5.6229 train_time:17062ms step_avg:284.37ms
step:71/500 train_loss:5.6447 train_time:17346ms step_avg:284.37ms
step:72/500 train_loss:5.8500 train_time:17632ms step_avg:284.38ms
step:73/500 train_loss:5.7973 train_time:17919ms step_avg:284.43ms
step:74/500 train_loss:5.6850 train_time:18204ms step_avg:284.44ms
step:75/500 train_loss:5.8043 train_time:18489ms step_avg:284.44ms
step:76/500 train_loss:5.7515 train_time:18775ms step_avg:284.48ms
step:77/500 train_loss:5.7259 train_time:19062ms step_avg:284.51ms
step:78/500 train_loss:5.8012 train_time:19345ms step_avg:284.49ms
step:79/500 train_loss:5.8276 train_time:19630ms step_avg:284.50ms
step:80/500 train_loss:5.6940 train_time:19918ms step_avg:284.55ms
step:81/500 train_loss:5.7820 train_time:20205ms step_avg:284.57ms
step:82/500 train_loss:5.5546 train_time:20489ms step_avg:284.57ms
step:83/500 train_loss:5.7187 train_time:20777ms step_avg:284.61ms
step:84/500 train_loss:5.6835 train_time:21064ms step_avg:284.64ms
step:85/500 train_loss:5.6436 train_time:21347ms step_avg:284.63ms
step:86/500 train_loss:5.5151 train_time:21633ms step_avg:284.65ms
step:87/500 train_loss:5.7223 train_time:21922ms step_avg:284.70ms
step:88/500 train_loss:5.6271 train_time:22207ms step_avg:284.70ms
step:89/500 train_loss:5.6872 train_time:22493ms step_avg:284.72ms
step:90/500 train_loss:5.6611 train_time:22783ms step_avg:284.79ms
step:91/500 train_loss:5.5863 train_time:23067ms step_avg:284.77ms
step:92/500 train_loss:5.5721 train_time:23353ms step_avg:284.80ms
step:93/500 train_loss:5.6895 train_time:23640ms step_avg:284.81ms
step:94/500 train_loss:5.5308 train_time:23925ms step_avg:284.83ms
step:95/500 train_loss:5.5271 train_time:24210ms step_avg:284.82ms
step:96/500 train_loss:5.5385 train_time:24496ms step_avg:284.83ms
step:97/500 train_loss:5.4623 train_time:24783ms step_avg:284.86ms
step:98/500 train_loss:5.5296 train_time:25068ms step_avg:284.86ms
step:99/500 train_loss:5.4556 train_time:25355ms step_avg:284.88ms
step:100/500 train_loss:5.5744 train_time:25641ms step_avg:284.90ms
step:101/500 train_loss:5.5421 train_time:25927ms step_avg:284.91ms w_mean:1.000 w_std:0.118 w_min:0.932 w_max:1.515
step:102/500 train_loss:5.4402 train_time:26213ms step_avg:284.93ms
step:103/500 train_loss:5.5475 train_time:26500ms step_avg:284.94ms
step:104/500 train_loss:5.5057 train_time:26785ms step_avg:284.95ms
step:105/500 train_loss:5.3432 train_time:27069ms step_avg:284.94ms
step:106/500 train_loss:5.4484 train_time:27356ms step_avg:284.96ms
step:107/500 train_loss:5.6467 train_time:27643ms step_avg:284.98ms
step:108/500 train_loss:5.4383 train_time:27927ms step_avg:284.97ms
step:109/500 train_loss:5.2050 train_time:28215ms step_avg:285.00ms
step:110/500 train_loss:5.4045 train_time:28502ms step_avg:285.02ms
step:111/500 train_loss:5.3756 train_time:28786ms step_avg:285.01ms
step:112/500 train_loss:5.3421 train_time:29071ms step_avg:285.01ms
step:113/500 train_loss:5.4582 train_time:29359ms step_avg:285.04ms
step:114/500 train_loss:5.3744 train_time:29644ms step_avg:285.04ms
step:115/500 train_loss:5.2424 train_time:29929ms step_avg:285.04ms
step:116/500 train_loss:5.3988 train_time:30216ms step_avg:285.06ms
step:117/500 train_loss:5.2753 train_time:30502ms step_avg:285.07ms
step:118/500 train_loss:5.2538 train_time:30787ms step_avg:285.07ms
step:119/500 train_loss:5.3747 train_time:31072ms step_avg:285.06ms
step:120/500 train_loss:5.3583 train_time:31360ms step_avg:285.09ms
step:121/500 train_loss:5.2970 train_time:31644ms step_avg:285.08ms
step:122/500 train_loss:5.1850 train_time:31929ms step_avg:285.08ms
step:123/500 train_loss:5.2837 train_time:32219ms step_avg:285.12ms
step:124/500 train_loss:5.1449 train_time:32503ms step_avg:285.11ms
step:125/500 train_loss:5.4450 train_time:32787ms step_avg:285.11ms
step:125/500 val_loss:5.2738 train_time:32788ms step_avg:285.12ms
step:126/500 train_loss:5.3029 train_time:33065ms step_avg:285.04ms
step:127/500 train_loss:5.2787 train_time:33354ms step_avg:285.08ms
step:128/500 train_loss:5.3342 train_time:33643ms step_avg:285.11ms
step:129/500 train_loss:5.1966 train_time:33930ms step_avg:285.12ms
step:130/500 train_loss:5.4631 train_time:34214ms step_avg:285.12ms
step:131/500 train_loss:5.2544 train_time:34502ms step_avg:285.14ms
step:132/500 train_loss:5.2596 train_time:34790ms step_avg:285.16ms
step:133/500 train_loss:5.1916 train_time:35076ms step_avg:285.17ms
step:134/500 train_loss:5.2440 train_time:35363ms step_avg:285.18ms
step:135/500 train_loss:5.1691 train_time:35649ms step_avg:285.19ms
step:136/500 train_loss:5.2376 train_time:35934ms step_avg:285.19ms
step:137/500 train_loss:5.0355 train_time:36220ms step_avg:285.20ms
step:138/500 train_loss:5.2024 train_time:36509ms step_avg:285.22ms
step:139/500 train_loss:5.1608 train_time:36793ms step_avg:285.22ms
step:140/500 train_loss:5.1668 train_time:37079ms step_avg:285.22ms
step:141/500 train_loss:5.2191 train_time:37369ms step_avg:285.26ms
step:142/500 train_loss:5.1184 train_time:37653ms step_avg:285.25ms
step:143/500 train_loss:5.1986 train_time:37939ms step_avg:285.25ms
step:144/500 train_loss:5.0171 train_time:38228ms step_avg:285.28ms
step:145/500 train_loss:5.1719 train_time:38513ms step_avg:285.28ms
step:146/500 train_loss:5.1086 train_time:38799ms step_avg:285.28ms
step:147/500 train_loss:5.0230 train_time:39088ms step_avg:285.31ms
step:148/500 train_loss:5.1411 train_time:39373ms step_avg:285.31ms
step:149/500 train_loss:5.1132 train_time:39659ms step_avg:285.31ms
step:150/500 train_loss:5.1698 train_time:39945ms step_avg:285.32ms
step:151/500 train_loss:5.1806 train_time:40231ms step_avg:285.33ms w_mean:1.000 w_std:0.129 w_min:0.926 w_max:1.504
step:152/500 train_loss:5.1091 train_time:40515ms step_avg:285.32ms
step:153/500 train_loss:5.0801 train_time:40805ms step_avg:285.35ms
step:154/500 train_loss:5.1620 train_time:41103ms step_avg:285.44ms
step:155/500 train_loss:5.0961 train_time:41375ms step_avg:285.34ms
step:156/500 train_loss:5.0791 train_time:41662ms step_avg:285.35ms
step:157/500 train_loss:5.0888 train_time:41948ms step_avg:285.36ms
step:158/500 train_loss:5.2150 train_time:42235ms step_avg:285.37ms
step:159/500 train_loss:5.0052 train_time:42523ms step_avg:285.39ms
step:160/500 train_loss:5.0610 train_time:42809ms step_avg:285.40ms
step:161/500 train_loss:4.9183 train_time:43093ms step_avg:285.38ms
step:162/500 train_loss:5.0701 train_time:43379ms step_avg:285.39ms
step:163/500 train_loss:5.1004 train_time:43667ms step_avg:285.40ms
step:164/500 train_loss:5.0937 train_time:43951ms step_avg:285.40ms
step:165/500 train_loss:4.9282 train_time:44237ms step_avg:285.40ms
step:166/500 train_loss:5.0314 train_time:44526ms step_avg:285.42ms
step:167/500 train_loss:5.1843 train_time:44811ms step_avg:285.42ms
step:168/500 train_loss:4.9694 train_time:45097ms step_avg:285.43ms
step:169/500 train_loss:5.0411 train_time:45386ms step_avg:285.44ms
step:170/500 train_loss:4.9186 train_time:45671ms step_avg:285.45ms
step:171/500 train_loss:4.8570 train_time:45957ms step_avg:285.44ms
step:172/500 train_loss:4.9674 train_time:46243ms step_avg:285.45ms
step:173/500 train_loss:4.9387 train_time:46531ms step_avg:285.47ms
step:174/500 train_loss:5.0013 train_time:46815ms step_avg:285.46ms
step:175/500 train_loss:5.1292 train_time:47103ms step_avg:285.47ms
step:176/500 train_loss:5.0245 train_time:47389ms step_avg:285.47ms
step:177/500 train_loss:4.8540 train_time:47674ms step_avg:285.47ms
step:178/500 train_loss:4.8422 train_time:47960ms step_avg:285.48ms
step:179/500 train_loss:4.8750 train_time:48246ms step_avg:285.48ms
step:180/500 train_loss:4.9246 train_time:48533ms step_avg:285.49ms
step:181/500 train_loss:4.9078 train_time:48818ms step_avg:285.49ms
step:182/500 train_loss:5.0191 train_time:49106ms step_avg:285.50ms
step:183/500 train_loss:4.9108 train_time:49392ms step_avg:285.50ms
step:184/500 train_loss:4.8448 train_time:49678ms step_avg:285.51ms
step:185/500 train_loss:4.8685 train_time:49968ms step_avg:285.53ms
step:186/500 train_loss:4.9949 train_time:50252ms step_avg:285.52ms
step:187/500 train_loss:4.8729 train_time:50538ms step_avg:285.53ms
step:188/500 train_loss:5.1189 train_time:50827ms step_avg:285.54ms
step:189/500 train_loss:4.9141 train_time:51426ms step_avg:287.29ms
step:190/500 train_loss:4.8291 train_time:52023ms step_avg:289.01ms
step:191/500 train_loss:4.9872 train_time:52309ms step_avg:289.00ms
step:192/500 train_loss:4.8291 train_time:52593ms step_avg:288.97ms
step:193/500 train_loss:4.7482 train_time:52876ms step_avg:288.94ms
step:194/500 train_loss:4.9487 train_time:53163ms step_avg:288.93ms
step:195/500 train_loss:4.8881 train_time:53449ms step_avg:288.91ms
step:196/500 train_loss:5.0696 train_time:53733ms step_avg:288.89ms
step:197/500 train_loss:4.9618 train_time:54018ms step_avg:288.87ms
step:198/500 train_loss:4.8086 train_time:54305ms step_avg:288.85ms
step:199/500 train_loss:4.8435 train_time:54591ms step_avg:288.84ms
step:200/500 train_loss:4.7426 train_time:54877ms step_avg:288.82ms
step:201/500 train_loss:4.8228 train_time:55163ms step_avg:288.81ms w_mean:1.000 w_std:0.139 w_min:0.921 w_max:1.496
step:202/500 train_loss:4.7496 train_time:55449ms step_avg:288.80ms
step:203/500 train_loss:4.9623 train_time:55734ms step_avg:288.78ms
step:204/500 train_loss:4.8763 train_time:56021ms step_avg:288.77ms
step:205/500 train_loss:4.8377 train_time:56306ms step_avg:288.75ms
step:206/500 train_loss:4.9990 train_time:56592ms step_avg:288.73ms
step:207/500 train_loss:4.6740 train_time:56876ms step_avg:288.71ms
step:208/500 train_loss:4.8219 train_time:57164ms step_avg:288.71ms
step:209/500 train_loss:4.7762 train_time:57449ms step_avg:288.69ms
step:210/500 train_loss:4.9348 train_time:57734ms step_avg:288.67ms
step:211/500 train_loss:4.8587 train_time:58020ms step_avg:288.66ms
step:212/500 train_loss:4.7446 train_time:58307ms step_avg:288.65ms
step:213/500 train_loss:4.8808 train_time:58591ms step_avg:288.63ms
step:214/500 train_loss:4.7210 train_time:58876ms step_avg:288.61ms
step:215/500 train_loss:4.8143 train_time:59163ms step_avg:288.60ms
step:216/500 train_loss:4.6720 train_time:59449ms step_avg:288.59ms
step:217/500 train_loss:4.7904 train_time:59733ms step_avg:288.57ms
step:218/500 train_loss:4.7742 train_time:60020ms step_avg:288.56ms
step:219/500 train_loss:4.7460 train_time:60308ms step_avg:288.55ms
step:220/500 train_loss:4.7574 train_time:60592ms step_avg:288.54ms
step:221/500 train_loss:4.7866 train_time:60877ms step_avg:288.52ms
step:222/500 train_loss:4.8170 train_time:61165ms step_avg:288.52ms
step:223/500 train_loss:4.7620 train_time:61453ms step_avg:288.51ms
step:224/500 train_loss:4.7636 train_time:61736ms step_avg:288.49ms
step:225/500 train_loss:4.8911 train_time:62025ms step_avg:288.49ms
step:226/500 train_loss:4.6271 train_time:62311ms step_avg:288.48ms
step:227/500 train_loss:4.6655 train_time:62595ms step_avg:288.46ms
step:228/500 train_loss:4.6541 train_time:62881ms step_avg:288.45ms
step:229/500 train_loss:4.8165 train_time:63171ms step_avg:288.45ms
step:230/500 train_loss:4.6515 train_time:63456ms step_avg:288.44ms
step:231/500 train_loss:4.7906 train_time:63742ms step_avg:288.43ms
step:232/500 train_loss:4.6617 train_time:64031ms step_avg:288.43ms
step:233/500 train_loss:4.6226 train_time:64315ms step_avg:288.41ms
step:234/500 train_loss:4.8270 train_time:64603ms step_avg:288.40ms
step:235/500 train_loss:4.6682 train_time:64888ms step_avg:288.39ms
step:236/500 train_loss:4.6072 train_time:65174ms step_avg:288.38ms
step:237/500 train_loss:4.8550 train_time:65459ms step_avg:288.36ms
step:238/500 train_loss:4.7362 train_time:65746ms step_avg:288.36ms
step:239/500 train_loss:4.6539 train_time:66032ms step_avg:288.35ms
step:240/500 train_loss:4.7906 train_time:66317ms step_avg:288.33ms
step:241/500 train_loss:4.7772 train_time:66605ms step_avg:288.33ms
step:242/500 train_loss:4.6830 train_time:66890ms step_avg:288.32ms
step:243/500 train_loss:4.8298 train_time:67174ms step_avg:288.30ms
step:244/500 train_loss:4.6720 train_time:67460ms step_avg:288.29ms
step:245/500 train_loss:4.6833 train_time:67747ms step_avg:288.28ms
step:246/500 train_loss:4.7506 train_time:68032ms step_avg:288.27ms
step:247/500 train_loss:4.7099 train_time:68317ms step_avg:288.26ms
step:248/500 train_loss:4.6674 train_time:68605ms step_avg:288.26ms
step:249/500 train_loss:4.8275 train_time:68891ms step_avg:288.25ms
step:250/500 train_loss:4.5722 train_time:69176ms step_avg:288.23ms
step:250/500 val_loss:4.6716 train_time:69177ms step_avg:288.24ms
step:251/500 train_loss:4.6054 train_time:69447ms step_avg:288.16ms w_mean:1.000 w_std:0.145 w_min:0.917 w_max:1.490
step:252/500 train_loss:4.7438 train_time:69739ms step_avg:288.18ms
step:253/500 train_loss:4.7302 train_time:70026ms step_avg:288.17ms
step:254/500 train_loss:4.6185 train_time:70312ms step_avg:288.16ms
step:255/500 train_loss:4.6272 train_time:70598ms step_avg:288.16ms
step:256/500 train_loss:4.7617 train_time:70885ms step_avg:288.15ms
step:257/500 train_loss:4.7032 train_time:71171ms step_avg:288.14ms
step:258/500 train_loss:4.6854 train_time:71459ms step_avg:288.14ms
step:259/500 train_loss:4.6117 train_time:71743ms step_avg:288.13ms
step:260/500 train_loss:4.6315 train_time:72029ms step_avg:288.12ms
step:261/500 train_loss:4.6974 train_time:72319ms step_avg:288.13ms
step:262/500 train_loss:4.7057 train_time:72604ms step_avg:288.11ms
step:263/500 train_loss:4.6105 train_time:72890ms step_avg:288.10ms
step:264/500 train_loss:4.5566 train_time:73178ms step_avg:288.10ms
step:265/500 train_loss:4.6157 train_time:73464ms step_avg:288.09ms
step:266/500 train_loss:4.4660 train_time:73750ms step_avg:288.08ms
step:267/500 train_loss:4.5268 train_time:74039ms step_avg:288.09ms
step:268/500 train_loss:4.5698 train_time:74324ms step_avg:288.08ms
step:269/500 train_loss:4.5260 train_time:74611ms step_avg:288.07ms
step:270/500 train_loss:4.5003 train_time:74897ms step_avg:288.07ms
step:271/500 train_loss:4.7111 train_time:75182ms step_avg:288.05ms
step:272/500 train_loss:4.6494 train_time:75469ms step_avg:288.05ms
step:273/500 train_loss:4.5069 train_time:75756ms step_avg:288.05ms
step:274/500 train_loss:4.5618 train_time:76042ms step_avg:288.04ms
step:275/500 train_loss:4.6693 train_time:76326ms step_avg:288.02ms
step:276/500 train_loss:4.6879 train_time:76614ms step_avg:288.02ms
step:277/500 train_loss:4.8716 train_time:76901ms step_avg:288.02ms
step:278/500 train_loss:4.6306 train_time:77189ms step_avg:288.02ms
step:279/500 train_loss:4.7653 train_time:77479ms step_avg:288.02ms
step:280/500 train_loss:4.6025 train_time:77764ms step_avg:288.01ms
step:281/500 train_loss:4.6764 train_time:78050ms step_avg:288.01ms
step:282/500 train_loss:4.5708 train_time:78334ms step_avg:287.99ms
step:283/500 train_loss:4.6859 train_time:78622ms step_avg:287.99ms
step:284/500 train_loss:4.5107 train_time:78906ms step_avg:287.98ms
step:285/500 train_loss:4.6681 train_time:79193ms step_avg:287.97ms
step:286/500 train_loss:4.6576 train_time:79482ms step_avg:287.98ms
step:287/500 train_loss:4.6878 train_time:79766ms step_avg:287.96ms
step:288/500 train_loss:4.5542 train_time:80054ms step_avg:287.96ms
step:289/500 train_loss:4.6164 train_time:80340ms step_avg:287.96ms
step:290/500 train_loss:4.4794 train_time:80625ms step_avg:287.95ms
step:291/500 train_loss:4.4756 train_time:80910ms step_avg:287.94ms
step:292/500 train_loss:4.5991 train_time:81198ms step_avg:287.94ms
step:293/500 train_loss:4.4940 train_time:81483ms step_avg:287.93ms
step:294/500 train_loss:4.5358 train_time:81768ms step_avg:287.92ms
step:295/500 train_loss:4.5628 train_time:82055ms step_avg:287.91ms
step:296/500 train_loss:4.4279 train_time:82342ms step_avg:287.91ms
step:297/500 train_loss:4.4207 train_time:82626ms step_avg:287.90ms
step:298/500 train_loss:4.4466 train_time:82913ms step_avg:287.89ms
step:299/500 train_loss:4.5508 train_time:83201ms step_avg:287.89ms
step:300/500 train_loss:4.4410 train_time:83486ms step_avg:287.88ms
step:301/500 train_loss:4.6103 train_time:83773ms step_avg:287.88ms w_mean:1.000 w_std:0.145 w_min:0.917 w_max:1.491
step:302/500 train_loss:4.5885 train_time:84061ms step_avg:287.88ms
step:303/500 train_loss:4.5039 train_time:84345ms step_avg:287.87ms
step:304/500 train_loss:4.5794 train_time:84633ms step_avg:287.87ms
step:305/500 train_loss:4.5596 train_time:84921ms step_avg:287.87ms
step:306/500 train_loss:5.0271 train_time:85205ms step_avg:287.85ms
step:307/500 train_loss:4.5144 train_time:85491ms step_avg:287.85ms
step:308/500 train_loss:4.4193 train_time:85779ms step_avg:287.85ms
step:309/500 train_loss:4.6084 train_time:86063ms step_avg:287.84ms
step:310/500 train_loss:4.4112 train_time:86350ms step_avg:287.83ms
step:311/500 train_loss:4.6417 train_time:86637ms step_avg:287.83ms
step:312/500 train_loss:4.5531 train_time:86923ms step_avg:287.83ms
step:313/500 train_loss:4.4635 train_time:87207ms step_avg:287.81ms
step:314/500 train_loss:4.5932 train_time:87495ms step_avg:287.81ms
step:315/500 train_loss:4.7192 train_time:87782ms step_avg:287.81ms
step:316/500 train_loss:4.5602 train_time:88067ms step_avg:287.80ms
step:317/500 train_loss:4.4452 train_time:88353ms step_avg:287.80ms
step:318/500 train_loss:4.4618 train_time:88638ms step_avg:287.79ms
step:319/500 train_loss:4.4820 train_time:88925ms step_avg:287.78ms
step:320/500 train_loss:4.4285 train_time:89210ms step_avg:287.77ms
step:321/500 train_loss:4.5154 train_time:89498ms step_avg:287.77ms
step:322/500 train_loss:4.5307 train_time:89783ms step_avg:287.77ms
step:323/500 train_loss:4.4946 train_time:90069ms step_avg:287.76ms
step:324/500 train_loss:4.5640 train_time:90357ms step_avg:287.76ms
step:325/500 train_loss:4.5540 train_time:90642ms step_avg:287.75ms
step:326/500 train_loss:4.6309 train_time:90927ms step_avg:287.74ms
step:327/500 train_loss:4.4760 train_time:91219ms step_avg:287.76ms
step:328/500 train_loss:4.9204 train_time:91503ms step_avg:287.75ms
step:329/500 train_loss:4.6287 train_time:91789ms step_avg:287.74ms
step:330/500 train_loss:4.4182 train_time:92077ms step_avg:287.74ms
step:331/500 train_loss:4.3869 train_time:92363ms step_avg:287.73ms
step:332/500 train_loss:4.5324 train_time:92648ms step_avg:287.73ms
step:333/500 train_loss:4.4599 train_time:92935ms step_avg:287.72ms
step:334/500 train_loss:4.4445 train_time:93221ms step_avg:287.72ms
step:335/500 train_loss:4.4110 train_time:93505ms step_avg:287.71ms
step:336/500 train_loss:4.5954 train_time:93792ms step_avg:287.71ms
step:337/500 train_loss:4.5368 train_time:94081ms step_avg:287.71ms
step:338/500 train_loss:5.0671 train_time:94365ms step_avg:287.70ms
step:339/500 train_loss:4.5075 train_time:94651ms step_avg:287.69ms
step:340/500 train_loss:4.4807 train_time:94939ms step_avg:287.69ms
step:341/500 train_loss:4.4587 train_time:95224ms step_avg:287.69ms
step:342/500 train_loss:4.3984 train_time:95510ms step_avg:287.68ms
step:343/500 train_loss:4.3748 train_time:95798ms step_avg:287.68ms
step:344/500 train_loss:4.4407 train_time:96084ms step_avg:287.68ms
step:345/500 train_loss:4.5281 train_time:96369ms step_avg:287.67ms
step:346/500 train_loss:4.4269 train_time:96657ms step_avg:287.67ms
step:347/500 train_loss:4.3695 train_time:96944ms step_avg:287.67ms
step:348/500 train_loss:4.4272 train_time:97229ms step_avg:287.66ms
step:349/500 train_loss:4.4182 train_time:97519ms step_avg:287.67ms
step:350/500 train_loss:4.3501 train_time:97803ms step_avg:287.66ms
step:351/500 train_loss:4.0434 train_time:98089ms step_avg:287.65ms w_mean:1.000 w_std:0.156 w_min:0.914 w_max:1.485
step:352/500 train_loss:4.3290 train_time:98377ms step_avg:287.65ms
step:353/500 train_loss:4.6702 train_time:98663ms step_avg:287.65ms
step:354/500 train_loss:4.2131 train_time:98949ms step_avg:287.64ms
step:355/500 train_loss:4.4573 train_time:99235ms step_avg:287.64ms
step:356/500 train_loss:4.3603 train_time:99522ms step_avg:287.64ms
step:357/500 train_loss:4.4583 train_time:99806ms step_avg:287.63ms
step:358/500 train_loss:4.4621 train_time:100094ms step_avg:287.63ms
step:359/500 train_loss:4.3697 train_time:100382ms step_avg:287.63ms
step:360/500 train_loss:4.6711 train_time:100666ms step_avg:287.62ms
step:361/500 train_loss:4.1063 train_time:100953ms step_avg:287.62ms
step:362/500 train_loss:4.5841 train_time:101240ms step_avg:287.61ms
step:363/500 train_loss:4.4830 train_time:101526ms step_avg:287.61ms
step:364/500 train_loss:4.3741 train_time:101812ms step_avg:287.60ms
step:365/500 train_loss:4.3066 train_time:102099ms step_avg:287.60ms
step:366/500 train_loss:4.4614 train_time:102384ms step_avg:287.60ms
step:367/500 train_loss:4.3952 train_time:102670ms step_avg:287.59ms
step:368/500 train_loss:4.3789 train_time:102956ms step_avg:287.59ms
step:369/500 train_loss:4.3835 train_time:103242ms step_avg:287.58ms
step:370/500 train_loss:4.2741 train_time:103526ms step_avg:287.57ms
step:371/500 train_loss:4.4260 train_time:103814ms step_avg:287.57ms
step:372/500 train_loss:4.3591 train_time:104100ms step_avg:287.57ms
step:373/500 train_loss:4.2370 train_time:104385ms step_avg:287.56ms
step:374/500 train_loss:4.4294 train_time:104670ms step_avg:287.56ms
step:375/500 train_loss:4.3562 train_time:104958ms step_avg:287.56ms
step:375/500 val_loss:4.3750 train_time:104959ms step_avg:287.56ms
step:376/500 train_loss:4.3555 train_time:105231ms step_avg:287.52ms
step:377/500 train_loss:4.4152 train_time:105523ms step_avg:287.53ms
step:378/500 train_loss:4.3098 train_time:106123ms step_avg:288.38ms
step:379/500 train_loss:4.3625 train_time:106408ms step_avg:288.37ms
step:380/500 train_loss:4.4382 train_time:107008ms step_avg:289.21ms
step:381/500 train_loss:4.4658 train_time:107292ms step_avg:289.20ms
step:382/500 train_loss:4.4156 train_time:107576ms step_avg:289.18ms
step:383/500 train_loss:4.3931 train_time:107865ms step_avg:289.18ms
step:384/500 train_loss:4.2968 train_time:108149ms step_avg:289.17ms
step:385/500 train_loss:4.3946 train_time:108434ms step_avg:289.16ms
step:386/500 train_loss:4.3087 train_time:108725ms step_avg:289.16ms
step:387/500 train_loss:4.4356 train_time:109008ms step_avg:289.15ms
step:388/500 train_loss:4.6322 train_time:109294ms step_avg:289.14ms
step:389/500 train_loss:4.3335 train_time:109580ms step_avg:289.13ms
step:390/500 train_loss:4.2922 train_time:109866ms step_avg:289.12ms
step:391/500 train_loss:4.4119 train_time:110150ms step_avg:289.11ms
step:392/500 train_loss:4.3413 train_time:110438ms step_avg:289.11ms
step:393/500 train_loss:4.4453 train_time:110726ms step_avg:289.10ms
step:394/500 train_loss:4.2721 train_time:111011ms step_avg:289.09ms
step:395/500 train_loss:4.4026 train_time:111296ms step_avg:289.08ms
step:396/500 train_loss:4.1859 train_time:111584ms step_avg:289.08ms
step:397/500 train_loss:4.3506 train_time:111869ms step_avg:289.07ms
step:398/500 train_loss:4.4389 train_time:112153ms step_avg:289.05ms
step:399/500 train_loss:4.3908 train_time:112440ms step_avg:289.05ms
step:400/500 train_loss:4.3153 train_time:112727ms step_avg:289.04ms
step:401/500 train_loss:4.3785 train_time:113010ms step_avg:289.03ms w_mean:1.000 w_std:0.151 w_min:0.914 w_max:1.485
step:402/500 train_loss:4.4139 train_time:113296ms step_avg:289.02ms
step:403/500 train_loss:4.3809 train_time:113584ms step_avg:289.02ms
step:404/500 train_loss:4.4747 train_time:113870ms step_avg:289.01ms
step:405/500 train_loss:4.2556 train_time:114155ms step_avg:289.00ms
step:406/500 train_loss:4.3064 train_time:114441ms step_avg:288.99ms
step:407/500 train_loss:4.5777 train_time:114727ms step_avg:288.99ms
step:408/500 train_loss:4.3402 train_time:115014ms step_avg:288.98ms
step:409/500 train_loss:4.3373 train_time:115302ms step_avg:288.98ms
step:410/500 train_loss:4.3894 train_time:115588ms step_avg:288.97ms
step:411/500 train_loss:4.2719 train_time:115873ms step_avg:288.96ms
step:412/500 train_loss:4.2886 train_time:116161ms step_avg:288.96ms
step:413/500 train_loss:4.6981 train_time:116446ms step_avg:288.95ms
step:414/500 train_loss:4.1579 train_time:116731ms step_avg:288.94ms
step:415/500 train_loss:4.5193 train_time:117018ms step_avg:288.93ms
step:416/500 train_loss:4.2966 train_time:117305ms step_avg:288.93ms
step:417/500 train_loss:4.2877 train_time:117590ms step_avg:288.92ms
step:418/500 train_loss:4.4718 train_time:117874ms step_avg:288.91ms
step:419/500 train_loss:4.2088 train_time:118167ms step_avg:288.92ms
step:420/500 train_loss:4.3054 train_time:118450ms step_avg:288.90ms
step:421/500 train_loss:4.2797 train_time:118736ms step_avg:288.90ms
step:422/500 train_loss:4.1679 train_time:119024ms step_avg:288.89ms
step:423/500 train_loss:4.2792 train_time:119310ms step_avg:288.89ms
step:424/500 train_loss:4.3886 train_time:119599ms step_avg:288.89ms
step:425/500 train_loss:4.1875 train_time:119886ms step_avg:288.88ms
step:426/500 train_loss:4.3543 train_time:120170ms step_avg:288.87ms
step:427/500 train_loss:4.2346 train_time:120455ms step_avg:288.86ms
step:428/500 train_loss:4.4133 train_time:120741ms step_avg:288.86ms
step:429/500 train_loss:4.3610 train_time:121027ms step_avg:288.85ms
step:430/500 train_loss:4.2768 train_time:121312ms step_avg:288.84ms
step:431/500 train_loss:4.2474 train_time:121597ms step_avg:288.83ms
step:432/500 train_loss:4.1960 train_time:121884ms step_avg:288.83ms
step:433/500 train_loss:4.2889 train_time:122170ms step_avg:288.82ms
step:434/500 train_loss:4.3650 train_time:122454ms step_avg:288.81ms
step:435/500 train_loss:4.2899 train_time:122742ms step_avg:288.80ms
step:436/500 train_loss:4.3370 train_time:123028ms step_avg:288.80ms
step:437/500 train_loss:4.3463 train_time:123313ms step_avg:288.79ms
step:438/500 train_loss:4.2289 train_time:123599ms step_avg:288.78ms
step:439/500 train_loss:4.2545 train_time:123886ms step_avg:288.78ms
step:440/500 train_loss:4.2213 train_time:124170ms step_avg:288.77ms
step:441/500 train_loss:4.4060 train_time:124460ms step_avg:288.77ms
step:442/500 train_loss:4.3060 train_time:124746ms step_avg:288.76ms
step:443/500 train_loss:4.2820 train_time:125030ms step_avg:288.75ms
step:444/500 train_loss:4.1789 train_time:125317ms step_avg:288.75ms
step:445/500 train_loss:4.4264 train_time:125604ms step_avg:288.74ms
step:446/500 train_loss:4.3472 train_time:125889ms step_avg:288.74ms
step:447/500 train_loss:4.3537 train_time:126176ms step_avg:288.73ms
step:448/500 train_loss:4.2700 train_time:126466ms step_avg:288.73ms
step:449/500 train_loss:4.3569 train_time:126751ms step_avg:288.73ms
step:450/500 train_loss:4.1916 train_time:127036ms step_avg:288.72ms
step:451/500 train_loss:4.2345 train_time:127324ms step_avg:288.72ms w_mean:1.000 w_std:0.153 w_min:0.913 w_max:1.483
step:452/500 train_loss:4.1245 train_time:127609ms step_avg:288.71ms
step:453/500 train_loss:4.2232 train_time:127896ms step_avg:288.70ms
step:454/500 train_loss:4.2003 train_time:128183ms step_avg:288.70ms
step:455/500 train_loss:4.1788 train_time:128468ms step_avg:288.69ms
step:456/500 train_loss:4.3829 train_time:128753ms step_avg:288.68ms
step:457/500 train_loss:4.2370 train_time:129039ms step_avg:288.68ms
step:458/500 train_loss:4.3264 train_time:129328ms step_avg:288.68ms
step:459/500 train_loss:4.3593 train_time:129613ms step_avg:288.67ms
step:460/500 train_loss:4.1628 train_time:129902ms step_avg:288.67ms
step:461/500 train_loss:4.3354 train_time:130187ms step_avg:288.66ms
step:462/500 train_loss:4.2342 train_time:130471ms step_avg:288.65ms
step:463/500 train_loss:4.2197 train_time:130759ms step_avg:288.65ms
step:464/500 train_loss:4.3128 train_time:131046ms step_avg:288.65ms
step:465/500 train_loss:4.2403 train_time:131331ms step_avg:288.64ms
step:466/500 train_loss:4.2422 train_time:131617ms step_avg:288.63ms
step:467/500 train_loss:4.3728 train_time:131906ms step_avg:288.63ms
step:468/500 train_loss:4.3749 train_time:132191ms step_avg:288.63ms
step:469/500 train_loss:4.3341 train_time:132475ms step_avg:288.62ms
step:470/500 train_loss:4.2443 train_time:132765ms step_avg:288.62ms
step:471/500 train_loss:4.3354 train_time:133050ms step_avg:288.61ms
step:472/500 train_loss:4.3797 train_time:133335ms step_avg:288.60ms
step:473/500 train_loss:4.2804 train_time:133624ms step_avg:288.60ms
step:474/500 train_loss:4.2554 train_time:133908ms step_avg:288.60ms
step:475/500 train_loss:4.1323 train_time:134196ms step_avg:288.59ms
step:476/500 train_loss:4.5531 train_time:134481ms step_avg:288.59ms
step:477/500 train_loss:4.3092 train_time:134767ms step_avg:288.58ms
step:478/500 train_loss:4.1169 train_time:135051ms step_avg:288.57ms
step:479/500 train_loss:4.3172 train_time:135339ms step_avg:288.57ms
step:480/500 train_loss:4.3017 train_time:135627ms step_avg:288.57ms
step:481/500 train_loss:4.4270 train_time:135911ms step_avg:288.56ms
step:482/500 train_loss:4.2579 train_time:136198ms step_avg:288.55ms
step:483/500 train_loss:4.0836 train_time:136485ms step_avg:288.55ms
step:484/500 train_loss:4.3483 train_time:136770ms step_avg:288.54ms
step:485/500 train_loss:4.1946 train_time:137056ms step_avg:288.54ms
step:486/500 train_loss:4.2219 train_time:137342ms step_avg:288.53ms
step:487/500 train_loss:4.1729 train_time:137629ms step_avg:288.53ms
step:488/500 train_loss:4.1967 train_time:137915ms step_avg:288.53ms
step:489/500 train_loss:4.4045 train_time:138204ms step_avg:288.53ms
step:490/500 train_loss:4.2594 train_time:138488ms step_avg:288.52ms
step:491/500 train_loss:4.1601 train_time:138772ms step_avg:288.51ms
step:492/500 train_loss:4.1658 train_time:139060ms step_avg:288.51ms
step:493/500 train_loss:4.2800 train_time:139347ms step_avg:288.50ms
step:494/500 train_loss:4.1222 train_time:139632ms step_avg:288.50ms
step:495/500 train_loss:4.2734 train_time:139920ms step_avg:288.49ms
step:496/500 train_loss:4.1917 train_time:140206ms step_avg:288.49ms
step:497/500 train_loss:4.1386 train_time:140491ms step_avg:288.48ms
step:498/500 train_loss:4.2826 train_time:140777ms step_avg:288.48ms
step:499/500 train_loss:4.3637 train_time:141067ms step_avg:288.48ms
step:500/500 train_loss:4.4191 train_time:141351ms step_avg:288.47ms
step:500/500 val_loss:4.2636 train_time:141353ms step_avg:288.47ms
