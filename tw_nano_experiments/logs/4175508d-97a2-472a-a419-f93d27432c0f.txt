====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # also save the seed for reproducibility
        if args.seed is not None:
            log['seed'] = args.seed
        # save token weighting config
        if tw_config.enabled:
            log['tw_config'] = {
                'function': tw_config.function,
                'clamp_min': tw_config.clamp_min,
                'clamp_max': tw_config.clamp_max,
                'schedule': tw_config.schedule,
            }
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 21:17:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   46C    P0             83W /  310W |    2363MiB /  81559MiB |     14%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     29%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   49C    P0             89W /  310W |    2363MiB /  81559MiB |     31%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             86W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   44C    P0             82W /  310W |    2363MiB /  81559MiB |     18%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   49C    P0             86W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   52C    P0             88W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           60433      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           60434      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           60435      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           60436      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           60437      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           60438      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           60439      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           60440      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 42 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: sqrt
  clamp: [0.7, 1.5]
  schedule: constant
====================================================================================================
step:0/500 val_loss:15.9787 train_time:273ms step_avg:nanms
step:1/500 train_loss:15.9775 train_time:63251ms step_avg:nanms w_mean:1.000 w_std:0.021 w_min:0.700 w_max:1.076
step:2/500 train_loss:9.3666 train_time:63518ms step_avg:nanms
step:3/500 train_loss:8.9352 train_time:63785ms step_avg:nanms
step:4/500 train_loss:8.7466 train_time:64051ms step_avg:nanms
step:5/500 train_loss:8.2503 train_time:64324ms step_avg:nanms
step:6/500 train_loss:7.7270 train_time:64591ms step_avg:nanms
step:7/500 train_loss:7.3842 train_time:64858ms step_avg:nanms
step:8/500 train_loss:7.5397 train_time:65127ms step_avg:nanms
step:9/500 train_loss:7.2968 train_time:65395ms step_avg:nanms
step:10/500 train_loss:7.1056 train_time:65660ms step_avg:nanms
step:11/500 train_loss:7.0640 train_time:267ms step_avg:nanms
step:12/500 train_loss:7.0243 train_time:535ms step_avg:nanms
step:13/500 train_loss:6.8286 train_time:802ms step_avg:267.45ms
step:14/500 train_loss:6.8415 train_time:1071ms step_avg:267.63ms
step:15/500 train_loss:6.8061 train_time:1338ms step_avg:267.63ms
step:16/500 train_loss:6.7309 train_time:1607ms step_avg:267.81ms
step:17/500 train_loss:6.7329 train_time:1875ms step_avg:267.81ms
step:18/500 train_loss:6.7833 train_time:2142ms step_avg:267.73ms
step:19/500 train_loss:6.5919 train_time:2413ms step_avg:268.13ms
step:20/500 train_loss:6.6119 train_time:2680ms step_avg:268.00ms
step:21/500 train_loss:6.2639 train_time:2950ms step_avg:268.15ms
step:22/500 train_loss:6.6628 train_time:3219ms step_avg:268.27ms
step:23/500 train_loss:6.8757 train_time:3488ms step_avg:268.31ms
step:24/500 train_loss:6.5347 train_time:3755ms step_avg:268.24ms
step:25/500 train_loss:6.6333 train_time:4024ms step_avg:268.29ms
step:26/500 train_loss:6.3673 train_time:4293ms step_avg:268.32ms
step:27/500 train_loss:6.2725 train_time:4562ms step_avg:268.34ms
step:28/500 train_loss:6.4531 train_time:4832ms step_avg:268.47ms
step:29/500 train_loss:6.1085 train_time:5102ms step_avg:268.55ms
step:30/500 train_loss:6.3869 train_time:5373ms step_avg:268.67ms
step:31/500 train_loss:6.2374 train_time:5642ms step_avg:268.65ms
step:32/500 train_loss:6.2009 train_time:5911ms step_avg:268.68ms
step:33/500 train_loss:6.0351 train_time:6179ms step_avg:268.67ms
step:34/500 train_loss:6.3507 train_time:6449ms step_avg:268.73ms
step:35/500 train_loss:6.2700 train_time:6719ms step_avg:268.75ms
step:36/500 train_loss:6.4102 train_time:6988ms step_avg:268.78ms
step:37/500 train_loss:6.3448 train_time:7258ms step_avg:268.80ms
step:38/500 train_loss:6.2225 train_time:7528ms step_avg:268.85ms
step:39/500 train_loss:6.1296 train_time:7798ms step_avg:268.89ms
step:40/500 train_loss:6.1800 train_time:8068ms step_avg:268.93ms
step:41/500 train_loss:6.0822 train_time:8339ms step_avg:269.00ms
step:42/500 train_loss:6.1169 train_time:8610ms step_avg:269.08ms
step:43/500 train_loss:5.9929 train_time:8880ms step_avg:269.10ms
step:44/500 train_loss:6.0715 train_time:9150ms step_avg:269.11ms
step:45/500 train_loss:6.0683 train_time:9420ms step_avg:269.15ms
step:46/500 train_loss:6.2401 train_time:9692ms step_avg:269.22ms
step:47/500 train_loss:6.0460 train_time:9963ms step_avg:269.27ms
step:48/500 train_loss:5.8991 train_time:10232ms step_avg:269.27ms
step:49/500 train_loss:6.1309 train_time:10503ms step_avg:269.31ms
step:50/500 train_loss:6.0094 train_time:10774ms step_avg:269.34ms
step:51/500 train_loss:6.1524 train_time:11046ms step_avg:269.41ms w_mean:1.000 w_std:0.233 w_min:0.706 w_max:1.514
step:52/500 train_loss:6.0320 train_time:11314ms step_avg:269.38ms
step:53/500 train_loss:5.8683 train_time:11584ms step_avg:269.38ms
step:54/500 train_loss:6.0018 train_time:11852ms step_avg:269.37ms
step:55/500 train_loss:5.9065 train_time:12124ms step_avg:269.42ms
step:56/500 train_loss:6.2273 train_time:12392ms step_avg:269.40ms
step:57/500 train_loss:5.9014 train_time:12665ms step_avg:269.46ms
step:58/500 train_loss:5.8007 train_time:12931ms step_avg:269.40ms
step:59/500 train_loss:5.9418 train_time:13205ms step_avg:269.48ms
step:60/500 train_loss:5.8985 train_time:13472ms step_avg:269.45ms
step:61/500 train_loss:5.9708 train_time:13743ms step_avg:269.48ms
step:62/500 train_loss:5.7957 train_time:14013ms step_avg:269.48ms
step:63/500 train_loss:5.8725 train_time:14282ms step_avg:269.47ms
step:64/500 train_loss:5.8617 train_time:14553ms step_avg:269.51ms
step:65/500 train_loss:5.7507 train_time:14825ms step_avg:269.54ms
step:66/500 train_loss:5.7020 train_time:15094ms step_avg:269.53ms
step:67/500 train_loss:5.8398 train_time:15366ms step_avg:269.57ms
step:68/500 train_loss:5.7185 train_time:15637ms step_avg:269.60ms
step:69/500 train_loss:5.9516 train_time:15909ms step_avg:269.65ms
step:70/500 train_loss:5.6359 train_time:16179ms step_avg:269.65ms
step:71/500 train_loss:5.6413 train_time:16448ms step_avg:269.64ms
step:72/500 train_loss:5.8448 train_time:16720ms step_avg:269.69ms
step:73/500 train_loss:5.7948 train_time:16992ms step_avg:269.71ms
step:74/500 train_loss:5.6787 train_time:17263ms step_avg:269.74ms
step:75/500 train_loss:5.8049 train_time:17532ms step_avg:269.72ms
step:76/500 train_loss:5.7455 train_time:17805ms step_avg:269.77ms
step:77/500 train_loss:5.7332 train_time:18074ms step_avg:269.76ms
step:78/500 train_loss:5.7996 train_time:18347ms step_avg:269.81ms
step:79/500 train_loss:5.8328 train_time:18617ms step_avg:269.81ms
step:80/500 train_loss:5.6863 train_time:18889ms step_avg:269.84ms
step:81/500 train_loss:5.7808 train_time:19159ms step_avg:269.84ms
step:82/500 train_loss:5.5473 train_time:19431ms step_avg:269.87ms
step:83/500 train_loss:5.7216 train_time:19702ms step_avg:269.90ms
step:84/500 train_loss:5.6786 train_time:19971ms step_avg:269.87ms
step:85/500 train_loss:5.6534 train_time:20242ms step_avg:269.89ms
step:86/500 train_loss:5.5002 train_time:20514ms step_avg:269.92ms
step:87/500 train_loss:5.7375 train_time:20784ms step_avg:269.93ms
step:88/500 train_loss:5.6170 train_time:21052ms step_avg:269.89ms
step:89/500 train_loss:5.7024 train_time:21325ms step_avg:269.93ms
step:90/500 train_loss:5.6622 train_time:21595ms step_avg:269.94ms
step:91/500 train_loss:5.5874 train_time:21870ms step_avg:270.00ms
step:92/500 train_loss:5.5804 train_time:22140ms step_avg:270.00ms
step:93/500 train_loss:5.6803 train_time:22417ms step_avg:270.08ms
step:94/500 train_loss:5.5254 train_time:22687ms step_avg:270.08ms
step:95/500 train_loss:5.5180 train_time:22957ms step_avg:270.08ms
step:96/500 train_loss:5.5406 train_time:23228ms step_avg:270.10ms
step:97/500 train_loss:5.4658 train_time:23501ms step_avg:270.13ms
step:98/500 train_loss:5.5309 train_time:23772ms step_avg:270.14ms
step:99/500 train_loss:5.4506 train_time:24044ms step_avg:270.15ms
step:100/500 train_loss:5.5767 train_time:24312ms step_avg:270.13ms
step:101/500 train_loss:5.5287 train_time:24582ms step_avg:270.13ms w_mean:1.000 w_std:0.244 w_min:0.706 w_max:1.513
step:102/500 train_loss:5.4554 train_time:24853ms step_avg:270.14ms
step:103/500 train_loss:5.5281 train_time:25125ms step_avg:270.16ms
step:104/500 train_loss:5.5179 train_time:25395ms step_avg:270.16ms
step:105/500 train_loss:5.3273 train_time:25668ms step_avg:270.19ms
step:106/500 train_loss:5.4659 train_time:25938ms step_avg:270.18ms
step:107/500 train_loss:5.6307 train_time:26216ms step_avg:270.27ms
step:108/500 train_loss:5.4559 train_time:26488ms step_avg:270.28ms
step:109/500 train_loss:5.1884 train_time:26760ms step_avg:270.30ms
step:110/500 train_loss:5.4065 train_time:27031ms step_avg:270.31ms
step:111/500 train_loss:5.3668 train_time:27306ms step_avg:270.36ms
step:112/500 train_loss:5.3522 train_time:27577ms step_avg:270.36ms
step:113/500 train_loss:5.4389 train_time:27849ms step_avg:270.37ms
step:114/500 train_loss:5.3802 train_time:28120ms step_avg:270.38ms
step:115/500 train_loss:5.2331 train_time:28393ms step_avg:270.40ms
step:116/500 train_loss:5.4003 train_time:28665ms step_avg:270.43ms
step:117/500 train_loss:5.2577 train_time:28935ms step_avg:270.42ms
step:118/500 train_loss:5.2486 train_time:29209ms step_avg:270.46ms
step:119/500 train_loss:5.3739 train_time:29479ms step_avg:270.45ms
step:120/500 train_loss:5.3560 train_time:29750ms step_avg:270.46ms
step:121/500 train_loss:5.3009 train_time:30022ms step_avg:270.47ms
step:122/500 train_loss:5.1666 train_time:30292ms step_avg:270.47ms
step:123/500 train_loss:5.2898 train_time:30565ms step_avg:270.49ms
step:124/500 train_loss:5.1291 train_time:30837ms step_avg:270.50ms
step:125/500 train_loss:5.4528 train_time:31111ms step_avg:270.53ms
step:125/500 val_loss:5.2578 train_time:31112ms step_avg:270.54ms
step:126/500 train_loss:5.2799 train_time:31385ms step_avg:270.56ms
step:127/500 train_loss:5.2743 train_time:31662ms step_avg:270.61ms
step:128/500 train_loss:5.3251 train_time:31936ms step_avg:270.64ms
step:129/500 train_loss:5.1976 train_time:32203ms step_avg:270.61ms
step:130/500 train_loss:5.4490 train_time:32473ms step_avg:270.61ms
step:131/500 train_loss:5.2616 train_time:32747ms step_avg:270.64ms
step:132/500 train_loss:5.2348 train_time:33019ms step_avg:270.65ms
step:133/500 train_loss:5.2038 train_time:33289ms step_avg:270.64ms
step:134/500 train_loss:5.2208 train_time:33559ms step_avg:270.63ms
step:135/500 train_loss:5.1746 train_time:33833ms step_avg:270.67ms
step:136/500 train_loss:5.2218 train_time:34103ms step_avg:270.66ms
step:137/500 train_loss:5.0523 train_time:34377ms step_avg:270.68ms
step:138/500 train_loss:5.1874 train_time:34648ms step_avg:270.69ms
step:139/500 train_loss:5.1614 train_time:34922ms step_avg:270.71ms
step:140/500 train_loss:5.1605 train_time:35191ms step_avg:270.70ms
step:141/500 train_loss:5.2107 train_time:35461ms step_avg:270.69ms
step:142/500 train_loss:5.1166 train_time:35734ms step_avg:270.71ms
step:143/500 train_loss:5.2011 train_time:36004ms step_avg:270.71ms
step:144/500 train_loss:5.0126 train_time:36278ms step_avg:270.73ms
step:145/500 train_loss:5.1688 train_time:36548ms step_avg:270.73ms
step:146/500 train_loss:5.1064 train_time:36820ms step_avg:270.74ms
step:147/500 train_loss:5.0103 train_time:37091ms step_avg:270.74ms
step:148/500 train_loss:5.1453 train_time:37362ms step_avg:270.74ms
step:149/500 train_loss:5.1020 train_time:37635ms step_avg:270.75ms
step:150/500 train_loss:5.1738 train_time:37904ms step_avg:270.74ms
step:151/500 train_loss:5.1736 train_time:38177ms step_avg:270.76ms w_mean:1.000 w_std:0.253 w_min:0.707 w_max:1.514
step:152/500 train_loss:5.1113 train_time:38449ms step_avg:270.77ms
step:153/500 train_loss:5.0768 train_time:38721ms step_avg:270.78ms
step:154/500 train_loss:5.1586 train_time:38992ms step_avg:270.78ms
step:155/500 train_loss:5.0964 train_time:39266ms step_avg:270.80ms
step:156/500 train_loss:5.0765 train_time:39539ms step_avg:270.81ms
step:157/500 train_loss:5.0931 train_time:39809ms step_avg:270.81ms
step:158/500 train_loss:5.2065 train_time:40080ms step_avg:270.81ms
step:159/500 train_loss:5.0107 train_time:40353ms step_avg:270.82ms
step:160/500 train_loss:5.0513 train_time:40623ms step_avg:270.82ms
step:161/500 train_loss:4.9260 train_time:40894ms step_avg:270.82ms
step:162/500 train_loss:5.0667 train_time:41164ms step_avg:270.81ms
step:163/500 train_loss:5.1078 train_time:41438ms step_avg:270.84ms
step:164/500 train_loss:5.0959 train_time:41710ms step_avg:270.84ms
step:165/500 train_loss:4.9195 train_time:41981ms step_avg:270.84ms
step:166/500 train_loss:5.0441 train_time:42253ms step_avg:270.85ms
step:167/500 train_loss:5.1760 train_time:42524ms step_avg:270.85ms
step:168/500 train_loss:4.9827 train_time:42795ms step_avg:270.86ms
step:169/500 train_loss:5.0457 train_time:43065ms step_avg:270.85ms
step:170/500 train_loss:4.9301 train_time:43340ms step_avg:270.87ms
step:171/500 train_loss:4.8676 train_time:43613ms step_avg:270.89ms
step:172/500 train_loss:4.9769 train_time:43887ms step_avg:270.90ms
step:173/500 train_loss:4.9396 train_time:44160ms step_avg:270.92ms
step:174/500 train_loss:5.0029 train_time:44431ms step_avg:270.92ms
step:175/500 train_loss:5.1270 train_time:44701ms step_avg:270.92ms
step:176/500 train_loss:5.0329 train_time:44975ms step_avg:270.94ms
step:177/500 train_loss:4.8609 train_time:45244ms step_avg:270.92ms
step:178/500 train_loss:4.8525 train_time:45519ms step_avg:270.95ms
step:179/500 train_loss:4.8777 train_time:45788ms step_avg:270.93ms
step:180/500 train_loss:4.9326 train_time:46061ms step_avg:270.95ms
step:181/500 train_loss:4.9070 train_time:46334ms step_avg:270.96ms
step:182/500 train_loss:5.0336 train_time:46604ms step_avg:270.95ms
step:183/500 train_loss:4.9099 train_time:46876ms step_avg:270.96ms
step:184/500 train_loss:4.8516 train_time:47149ms step_avg:270.97ms
step:185/500 train_loss:4.8734 train_time:47421ms step_avg:270.98ms
step:186/500 train_loss:5.0036 train_time:47694ms step_avg:270.99ms
step:187/500 train_loss:4.8728 train_time:47962ms step_avg:270.97ms
step:188/500 train_loss:5.1213 train_time:48235ms step_avg:270.98ms
step:189/500 train_loss:4.9128 train_time:48761ms step_avg:272.41ms
step:190/500 train_loss:4.8357 train_time:49300ms step_avg:273.89ms
step:191/500 train_loss:4.9870 train_time:49566ms step_avg:273.85ms
step:192/500 train_loss:4.8363 train_time:49834ms step_avg:273.81ms
step:193/500 train_loss:4.7502 train_time:50101ms step_avg:273.78ms
step:194/500 train_loss:4.9555 train_time:50381ms step_avg:273.81ms
step:195/500 train_loss:4.8908 train_time:50653ms step_avg:273.80ms
step:196/500 train_loss:5.0783 train_time:50921ms step_avg:273.77ms
step:197/500 train_loss:4.9597 train_time:51190ms step_avg:273.75ms
step:198/500 train_loss:4.8204 train_time:51463ms step_avg:273.74ms
step:199/500 train_loss:4.8495 train_time:51737ms step_avg:273.74ms
step:200/500 train_loss:4.7593 train_time:52008ms step_avg:273.72ms
step:201/500 train_loss:4.8258 train_time:52277ms step_avg:273.70ms w_mean:1.000 w_std:0.259 w_min:0.707 w_max:1.515
step:202/500 train_loss:4.7538 train_time:52551ms step_avg:273.70ms
step:203/500 train_loss:4.9699 train_time:52822ms step_avg:273.69ms
step:204/500 train_loss:4.8984 train_time:53093ms step_avg:273.68ms
step:205/500 train_loss:4.8514 train_time:53362ms step_avg:273.65ms
step:206/500 train_loss:5.0117 train_time:53636ms step_avg:273.66ms
step:207/500 train_loss:4.6825 train_time:53908ms step_avg:273.65ms
step:208/500 train_loss:4.8386 train_time:54180ms step_avg:273.64ms
step:209/500 train_loss:4.7828 train_time:54450ms step_avg:273.62ms
step:210/500 train_loss:4.9481 train_time:54722ms step_avg:273.61ms
step:211/500 train_loss:4.8786 train_time:54992ms step_avg:273.59ms
step:212/500 train_loss:4.7626 train_time:55264ms step_avg:273.58ms
step:213/500 train_loss:4.8900 train_time:55537ms step_avg:273.58ms
step:214/500 train_loss:4.7322 train_time:55810ms step_avg:273.58ms
step:215/500 train_loss:4.8246 train_time:56081ms step_avg:273.57ms
step:216/500 train_loss:4.6818 train_time:56354ms step_avg:273.57ms
step:217/500 train_loss:4.8056 train_time:56623ms step_avg:273.54ms
step:218/500 train_loss:4.7805 train_time:56896ms step_avg:273.54ms
step:219/500 train_loss:4.7588 train_time:57167ms step_avg:273.52ms
step:220/500 train_loss:4.7667 train_time:57440ms step_avg:273.52ms
step:221/500 train_loss:4.7975 train_time:57712ms step_avg:273.52ms
step:222/500 train_loss:4.8251 train_time:57982ms step_avg:273.50ms
step:223/500 train_loss:4.7775 train_time:58256ms step_avg:273.50ms
step:224/500 train_loss:4.7711 train_time:58528ms step_avg:273.49ms
step:225/500 train_loss:4.8974 train_time:58798ms step_avg:273.48ms
step:226/500 train_loss:4.6375 train_time:59069ms step_avg:273.47ms
step:227/500 train_loss:4.6803 train_time:59343ms step_avg:273.47ms
step:228/500 train_loss:4.6646 train_time:59618ms step_avg:273.48ms
step:229/500 train_loss:4.8217 train_time:59888ms step_avg:273.46ms
step:230/500 train_loss:4.6648 train_time:60160ms step_avg:273.45ms
step:231/500 train_loss:4.7981 train_time:60432ms step_avg:273.45ms
step:232/500 train_loss:4.6769 train_time:60703ms step_avg:273.44ms
step:233/500 train_loss:4.6348 train_time:60975ms step_avg:273.43ms
step:234/500 train_loss:4.8444 train_time:61243ms step_avg:273.41ms
step:235/500 train_loss:4.6752 train_time:61518ms step_avg:273.41ms
step:236/500 train_loss:4.6313 train_time:61789ms step_avg:273.40ms
step:237/500 train_loss:4.8591 train_time:62062ms step_avg:273.40ms
step:238/500 train_loss:4.7504 train_time:62333ms step_avg:273.39ms
step:239/500 train_loss:4.6625 train_time:62603ms step_avg:273.38ms
step:240/500 train_loss:4.8086 train_time:62877ms step_avg:273.38ms
step:241/500 train_loss:4.7763 train_time:63151ms step_avg:273.38ms
step:242/500 train_loss:4.7027 train_time:63422ms step_avg:273.37ms
step:243/500 train_loss:4.8324 train_time:63693ms step_avg:273.36ms
step:244/500 train_loss:4.6915 train_time:63963ms step_avg:273.35ms
step:245/500 train_loss:4.6849 train_time:64238ms step_avg:273.35ms
step:246/500 train_loss:4.7675 train_time:64510ms step_avg:273.35ms
step:247/500 train_loss:4.7134 train_time:64781ms step_avg:273.34ms
step:248/500 train_loss:4.6870 train_time:65055ms step_avg:273.34ms
step:249/500 train_loss:4.8445 train_time:65328ms step_avg:273.34ms
step:250/500 train_loss:4.5890 train_time:65598ms step_avg:273.33ms
step:250/500 val_loss:4.6815 train_time:65600ms step_avg:273.33ms
step:251/500 train_loss:4.6183 train_time:65872ms step_avg:273.33ms w_mean:1.000 w_std:0.265 w_min:0.707 w_max:1.515
step:252/500 train_loss:4.7594 train_time:66153ms step_avg:273.36ms
step:253/500 train_loss:4.7374 train_time:66427ms step_avg:273.36ms
step:254/500 train_loss:4.6308 train_time:66698ms step_avg:273.35ms
step:255/500 train_loss:4.6372 train_time:66968ms step_avg:273.34ms
step:256/500 train_loss:4.7724 train_time:67243ms step_avg:273.34ms
step:257/500 train_loss:4.7180 train_time:67517ms step_avg:273.35ms
step:258/500 train_loss:4.6988 train_time:67786ms step_avg:273.33ms
step:259/500 train_loss:4.6267 train_time:68056ms step_avg:273.32ms
step:260/500 train_loss:4.6410 train_time:68330ms step_avg:273.32ms
step:261/500 train_loss:4.7157 train_time:68603ms step_avg:273.32ms
step:262/500 train_loss:4.7081 train_time:68873ms step_avg:273.31ms
step:263/500 train_loss:4.6281 train_time:69147ms step_avg:273.31ms
step:264/500 train_loss:4.5662 train_time:69420ms step_avg:273.31ms
step:265/500 train_loss:4.6301 train_time:69691ms step_avg:273.30ms
step:266/500 train_loss:4.4818 train_time:69961ms step_avg:273.28ms
step:267/500 train_loss:4.5453 train_time:70235ms step_avg:273.29ms
step:268/500 train_loss:4.5763 train_time:70510ms step_avg:273.30ms
step:269/500 train_loss:4.5434 train_time:70780ms step_avg:273.28ms
step:270/500 train_loss:4.5185 train_time:71052ms step_avg:273.28ms
step:271/500 train_loss:4.7200 train_time:71326ms step_avg:273.28ms
step:272/500 train_loss:4.6672 train_time:71600ms step_avg:273.28ms
step:273/500 train_loss:4.5179 train_time:71871ms step_avg:273.28ms
step:274/500 train_loss:4.5772 train_time:72143ms step_avg:273.27ms
step:275/500 train_loss:4.6790 train_time:72416ms step_avg:273.27ms
step:276/500 train_loss:4.7089 train_time:72688ms step_avg:273.26ms
step:277/500 train_loss:4.8898 train_time:72959ms step_avg:273.25ms
step:278/500 train_loss:4.6501 train_time:73233ms step_avg:273.26ms
step:279/500 train_loss:4.7766 train_time:73507ms step_avg:273.26ms
step:280/500 train_loss:4.6187 train_time:73778ms step_avg:273.25ms
step:281/500 train_loss:4.6838 train_time:74049ms step_avg:273.24ms
step:282/500 train_loss:4.5840 train_time:74321ms step_avg:273.24ms
step:283/500 train_loss:4.7001 train_time:74595ms step_avg:273.24ms
step:284/500 train_loss:4.5213 train_time:74868ms step_avg:273.24ms
step:285/500 train_loss:4.6795 train_time:75140ms step_avg:273.24ms
step:286/500 train_loss:4.6756 train_time:75413ms step_avg:273.23ms
step:287/500 train_loss:4.6991 train_time:75684ms step_avg:273.23ms
step:288/500 train_loss:4.5701 train_time:75954ms step_avg:273.21ms
step:289/500 train_loss:4.6337 train_time:76227ms step_avg:273.21ms
step:290/500 train_loss:4.4921 train_time:76498ms step_avg:273.21ms
step:291/500 train_loss:4.4912 train_time:76771ms step_avg:273.21ms
step:292/500 train_loss:4.6131 train_time:77041ms step_avg:273.19ms
step:293/500 train_loss:4.5064 train_time:77313ms step_avg:273.19ms
step:294/500 train_loss:4.5556 train_time:77583ms step_avg:273.18ms
step:295/500 train_loss:4.5788 train_time:77857ms step_avg:273.18ms
step:296/500 train_loss:4.4475 train_time:78131ms step_avg:273.19ms
step:297/500 train_loss:4.4376 train_time:78403ms step_avg:273.18ms
step:298/500 train_loss:4.4746 train_time:78672ms step_avg:273.17ms
step:299/500 train_loss:4.5587 train_time:78946ms step_avg:273.17ms
step:300/500 train_loss:4.4603 train_time:79219ms step_avg:273.17ms
step:301/500 train_loss:4.6253 train_time:79490ms step_avg:273.16ms w_mean:1.000 w_std:0.265 w_min:0.707 w_max:1.516
step:302/500 train_loss:4.6029 train_time:79759ms step_avg:273.15ms
step:303/500 train_loss:4.5203 train_time:80035ms step_avg:273.16ms
step:304/500 train_loss:4.5899 train_time:80309ms step_avg:273.16ms
step:305/500 train_loss:4.5713 train_time:80578ms step_avg:273.15ms
step:306/500 train_loss:5.0422 train_time:80850ms step_avg:273.14ms
step:307/500 train_loss:4.5289 train_time:81124ms step_avg:273.14ms
step:308/500 train_loss:4.4412 train_time:81394ms step_avg:273.13ms
step:309/500 train_loss:4.6172 train_time:81667ms step_avg:273.13ms
step:310/500 train_loss:4.4301 train_time:81939ms step_avg:273.13ms
step:311/500 train_loss:4.6518 train_time:82213ms step_avg:273.13ms
step:312/500 train_loss:4.5704 train_time:82484ms step_avg:273.12ms
step:313/500 train_loss:4.4769 train_time:82755ms step_avg:273.12ms
step:314/500 train_loss:4.6034 train_time:83030ms step_avg:273.13ms
step:315/500 train_loss:4.7344 train_time:83304ms step_avg:273.13ms
step:316/500 train_loss:4.5751 train_time:83573ms step_avg:273.11ms
step:317/500 train_loss:4.4617 train_time:83846ms step_avg:273.11ms
step:318/500 train_loss:4.4746 train_time:84116ms step_avg:273.10ms
step:319/500 train_loss:4.4912 train_time:84389ms step_avg:273.10ms
step:320/500 train_loss:4.4461 train_time:84660ms step_avg:273.10ms
step:321/500 train_loss:4.5377 train_time:84932ms step_avg:273.09ms
step:322/500 train_loss:4.5419 train_time:85205ms step_avg:273.09ms
step:323/500 train_loss:4.5078 train_time:85475ms step_avg:273.08ms
step:324/500 train_loss:4.5797 train_time:85748ms step_avg:273.08ms
step:325/500 train_loss:4.5770 train_time:86020ms step_avg:273.08ms
step:326/500 train_loss:4.6368 train_time:86292ms step_avg:273.08ms
step:327/500 train_loss:4.4879 train_time:86565ms step_avg:273.08ms
step:328/500 train_loss:4.9283 train_time:86834ms step_avg:273.06ms
step:329/500 train_loss:4.6442 train_time:87109ms step_avg:273.07ms
step:330/500 train_loss:4.4423 train_time:87380ms step_avg:273.06ms
step:331/500 train_loss:4.3984 train_time:87653ms step_avg:273.06ms
step:332/500 train_loss:4.5550 train_time:87925ms step_avg:273.06ms
step:333/500 train_loss:4.4762 train_time:88195ms step_avg:273.05ms
step:334/500 train_loss:4.4624 train_time:88467ms step_avg:273.05ms
step:335/500 train_loss:4.4264 train_time:88742ms step_avg:273.05ms
step:336/500 train_loss:4.6106 train_time:89017ms step_avg:273.06ms
step:337/500 train_loss:4.5542 train_time:89287ms step_avg:273.05ms
step:338/500 train_loss:5.0737 train_time:89557ms step_avg:273.04ms
step:339/500 train_loss:4.5264 train_time:89832ms step_avg:273.05ms
step:340/500 train_loss:4.4847 train_time:90108ms step_avg:273.05ms
step:341/500 train_loss:4.4775 train_time:90378ms step_avg:273.04ms
step:342/500 train_loss:4.4154 train_time:90650ms step_avg:273.04ms
step:343/500 train_loss:4.3998 train_time:90922ms step_avg:273.04ms
step:344/500 train_loss:4.4579 train_time:91194ms step_avg:273.04ms
step:345/500 train_loss:4.5443 train_time:91468ms step_avg:273.04ms
step:346/500 train_loss:4.4358 train_time:91740ms step_avg:273.04ms
step:347/500 train_loss:4.3856 train_time:92013ms step_avg:273.04ms
step:348/500 train_loss:4.4405 train_time:92284ms step_avg:273.03ms
step:349/500 train_loss:4.4404 train_time:92555ms step_avg:273.02ms
step:350/500 train_loss:4.3670 train_time:92830ms step_avg:273.03ms
step:351/500 train_loss:4.0645 train_time:93102ms step_avg:273.03ms w_mean:1.000 w_std:0.271 w_min:0.711 w_max:1.523
step:352/500 train_loss:4.3491 train_time:93373ms step_avg:273.02ms
step:353/500 train_loss:4.6825 train_time:93646ms step_avg:273.02ms
step:354/500 train_loss:4.2333 train_time:93920ms step_avg:273.02ms
step:355/500 train_loss:4.4704 train_time:94192ms step_avg:273.02ms
step:356/500 train_loss:4.3855 train_time:94465ms step_avg:273.02ms
step:357/500 train_loss:4.4733 train_time:94736ms step_avg:273.01ms
step:358/500 train_loss:4.4865 train_time:95010ms step_avg:273.02ms
step:359/500 train_loss:4.3920 train_time:95279ms step_avg:273.01ms
step:360/500 train_loss:4.7072 train_time:95553ms step_avg:273.01ms
step:361/500 train_loss:4.1323 train_time:95826ms step_avg:273.01ms
step:362/500 train_loss:4.6024 train_time:96097ms step_avg:273.00ms
step:363/500 train_loss:4.4999 train_time:96368ms step_avg:273.00ms
step:364/500 train_loss:4.3873 train_time:96641ms step_avg:273.00ms
step:365/500 train_loss:4.3261 train_time:96914ms step_avg:273.00ms
step:366/500 train_loss:4.4791 train_time:97186ms step_avg:272.99ms
step:367/500 train_loss:4.4079 train_time:97454ms step_avg:272.98ms
step:368/500 train_loss:4.3974 train_time:97729ms step_avg:272.99ms
step:369/500 train_loss:4.4010 train_time:98002ms step_avg:272.99ms
step:370/500 train_loss:4.3005 train_time:98274ms step_avg:272.98ms
step:371/500 train_loss:4.4372 train_time:98546ms step_avg:272.98ms
step:372/500 train_loss:4.3789 train_time:98819ms step_avg:272.98ms
step:373/500 train_loss:4.2577 train_time:99091ms step_avg:272.98ms
step:374/500 train_loss:4.4458 train_time:99362ms step_avg:272.97ms
step:375/500 train_loss:4.3807 train_time:99636ms step_avg:272.98ms
step:375/500 val_loss:4.3954 train_time:99638ms step_avg:272.98ms
step:376/500 train_loss:4.3757 train_time:99911ms step_avg:272.98ms
step:377/500 train_loss:4.4401 train_time:100192ms step_avg:273.00ms
step:378/500 train_loss:4.3271 train_time:100716ms step_avg:273.69ms
step:379/500 train_loss:4.3749 train_time:100987ms step_avg:273.68ms
step:380/500 train_loss:4.4557 train_time:101517ms step_avg:274.37ms
step:381/500 train_loss:4.4803 train_time:101785ms step_avg:274.35ms
step:382/500 train_loss:4.4294 train_time:102054ms step_avg:274.34ms
step:383/500 train_loss:4.4110 train_time:102324ms step_avg:274.33ms
step:384/500 train_loss:4.3156 train_time:102601ms step_avg:274.33ms
step:385/500 train_loss:4.4147 train_time:102870ms step_avg:274.32ms
step:386/500 train_loss:4.3263 train_time:103140ms step_avg:274.31ms
step:387/500 train_loss:4.4538 train_time:103411ms step_avg:274.30ms
step:388/500 train_loss:4.6539 train_time:103687ms step_avg:274.30ms
step:389/500 train_loss:4.3496 train_time:103958ms step_avg:274.30ms
step:390/500 train_loss:4.3078 train_time:104229ms step_avg:274.29ms
step:391/500 train_loss:4.4334 train_time:104499ms step_avg:274.28ms
step:392/500 train_loss:4.3604 train_time:104771ms step_avg:274.27ms
step:393/500 train_loss:4.4603 train_time:105045ms step_avg:274.27ms
step:394/500 train_loss:4.2893 train_time:105315ms step_avg:274.26ms
step:395/500 train_loss:4.4227 train_time:105588ms step_avg:274.26ms
step:396/500 train_loss:4.2147 train_time:105859ms step_avg:274.25ms
step:397/500 train_loss:4.3673 train_time:106133ms step_avg:274.24ms
step:398/500 train_loss:4.4640 train_time:106406ms step_avg:274.24ms
step:399/500 train_loss:4.4079 train_time:106678ms step_avg:274.24ms
step:400/500 train_loss:4.3335 train_time:106949ms step_avg:274.23ms
step:401/500 train_loss:4.3989 train_time:107220ms step_avg:274.22ms w_mean:1.000 w_std:0.270 w_min:0.708 w_max:1.516
step:402/500 train_loss:4.4325 train_time:107492ms step_avg:274.21ms
step:403/500 train_loss:4.4040 train_time:107762ms step_avg:274.20ms
step:404/500 train_loss:4.4970 train_time:108032ms step_avg:274.19ms
step:405/500 train_loss:4.2755 train_time:108302ms step_avg:274.18ms
step:406/500 train_loss:4.3241 train_time:108572ms step_avg:274.17ms
step:407/500 train_loss:4.5939 train_time:108846ms step_avg:274.17ms
step:408/500 train_loss:4.3664 train_time:109118ms step_avg:274.17ms
step:409/500 train_loss:4.3532 train_time:109391ms step_avg:274.16ms
step:410/500 train_loss:4.4073 train_time:109660ms step_avg:274.15ms
step:411/500 train_loss:4.2962 train_time:109932ms step_avg:274.15ms
step:412/500 train_loss:4.3098 train_time:110204ms step_avg:274.14ms
step:413/500 train_loss:4.7206 train_time:110474ms step_avg:274.13ms
step:414/500 train_loss:4.1845 train_time:110747ms step_avg:274.13ms
step:415/500 train_loss:4.5329 train_time:111020ms step_avg:274.12ms
step:416/500 train_loss:4.3143 train_time:111292ms step_avg:274.12ms
step:417/500 train_loss:4.3068 train_time:111564ms step_avg:274.11ms
step:418/500 train_loss:4.4865 train_time:111833ms step_avg:274.10ms
step:419/500 train_loss:4.2266 train_time:112106ms step_avg:274.10ms
step:420/500 train_loss:4.3255 train_time:112377ms step_avg:274.09ms
step:421/500 train_loss:4.3055 train_time:112649ms step_avg:274.09ms
step:422/500 train_loss:4.1915 train_time:112919ms step_avg:274.07ms
step:423/500 train_loss:4.2975 train_time:113194ms step_avg:274.08ms
step:424/500 train_loss:4.4028 train_time:113463ms step_avg:274.07ms
step:425/500 train_loss:4.2111 train_time:113735ms step_avg:274.06ms
step:426/500 train_loss:4.3807 train_time:114006ms step_avg:274.05ms
step:427/500 train_loss:4.2527 train_time:114280ms step_avg:274.05ms
step:428/500 train_loss:4.4243 train_time:114549ms step_avg:274.04ms
step:429/500 train_loss:4.3835 train_time:114823ms step_avg:274.04ms
step:430/500 train_loss:4.2928 train_time:115093ms step_avg:274.03ms
step:431/500 train_loss:4.2765 train_time:115364ms step_avg:274.02ms
step:432/500 train_loss:4.2245 train_time:115634ms step_avg:274.01ms
step:433/500 train_loss:4.3034 train_time:115908ms step_avg:274.01ms
step:434/500 train_loss:4.3823 train_time:116181ms step_avg:274.01ms
step:435/500 train_loss:4.3115 train_time:116452ms step_avg:274.00ms
step:436/500 train_loss:4.3563 train_time:116725ms step_avg:274.00ms
step:437/500 train_loss:4.3678 train_time:116998ms step_avg:274.00ms
step:438/500 train_loss:4.2580 train_time:117269ms step_avg:273.99ms
step:439/500 train_loss:4.2736 train_time:117540ms step_avg:273.99ms
step:440/500 train_loss:4.2294 train_time:117813ms step_avg:273.98ms
step:441/500 train_loss:4.4219 train_time:118088ms step_avg:273.99ms
step:442/500 train_loss:4.3256 train_time:118358ms step_avg:273.98ms
step:443/500 train_loss:4.3021 train_time:118630ms step_avg:273.97ms
step:444/500 train_loss:4.2001 train_time:118904ms step_avg:273.97ms
step:445/500 train_loss:4.4402 train_time:119174ms step_avg:273.96ms
step:446/500 train_loss:4.3730 train_time:119446ms step_avg:273.96ms
step:447/500 train_loss:4.3692 train_time:119716ms step_avg:273.95ms
step:448/500 train_loss:4.2956 train_time:119990ms step_avg:273.95ms
step:449/500 train_loss:4.3712 train_time:120261ms step_avg:273.94ms
step:450/500 train_loss:4.2109 train_time:120531ms step_avg:273.93ms
step:451/500 train_loss:4.2630 train_time:120806ms step_avg:273.94ms w_mean:1.000 w_std:0.271 w_min:0.709 w_max:1.519
step:452/500 train_loss:4.1459 train_time:121077ms step_avg:273.93ms
step:453/500 train_loss:4.2392 train_time:121349ms step_avg:273.93ms
step:454/500 train_loss:4.2209 train_time:121621ms step_avg:273.92ms
step:455/500 train_loss:4.2001 train_time:121894ms step_avg:273.92ms
step:456/500 train_loss:4.3964 train_time:122165ms step_avg:273.91ms
step:457/500 train_loss:4.2524 train_time:122435ms step_avg:273.90ms
step:458/500 train_loss:4.3466 train_time:122710ms step_avg:273.91ms
step:459/500 train_loss:4.3814 train_time:122982ms step_avg:273.90ms
step:460/500 train_loss:4.1819 train_time:123253ms step_avg:273.90ms
step:461/500 train_loss:4.3510 train_time:123526ms step_avg:273.89ms
step:462/500 train_loss:4.2572 train_time:123797ms step_avg:273.89ms
step:463/500 train_loss:4.2381 train_time:124070ms step_avg:273.88ms
step:464/500 train_loss:4.3345 train_time:124341ms step_avg:273.88ms
step:465/500 train_loss:4.2607 train_time:124613ms step_avg:273.87ms
step:466/500 train_loss:4.2647 train_time:124887ms step_avg:273.88ms
step:467/500 train_loss:4.3887 train_time:125157ms step_avg:273.87ms
step:468/500 train_loss:4.3869 train_time:125430ms step_avg:273.87ms
step:469/500 train_loss:4.3514 train_time:125703ms step_avg:273.86ms
step:470/500 train_loss:4.2630 train_time:125973ms step_avg:273.85ms
step:471/500 train_loss:4.3527 train_time:126247ms step_avg:273.85ms
step:472/500 train_loss:4.4000 train_time:126518ms step_avg:273.85ms
step:473/500 train_loss:4.3087 train_time:126791ms step_avg:273.85ms
step:474/500 train_loss:4.2752 train_time:127061ms step_avg:273.84ms
step:475/500 train_loss:4.1547 train_time:127334ms step_avg:273.84ms
step:476/500 train_loss:4.5846 train_time:127609ms step_avg:273.84ms
step:477/500 train_loss:4.3312 train_time:127881ms step_avg:273.84ms
step:478/500 train_loss:4.1424 train_time:128151ms step_avg:273.83ms
step:479/500 train_loss:4.3387 train_time:128426ms step_avg:273.83ms
step:480/500 train_loss:4.3225 train_time:128700ms step_avg:273.83ms
step:481/500 train_loss:4.4433 train_time:128970ms step_avg:273.82ms
step:482/500 train_loss:4.2743 train_time:129240ms step_avg:273.81ms
step:483/500 train_loss:4.0988 train_time:129515ms step_avg:273.82ms
step:484/500 train_loss:4.3678 train_time:129788ms step_avg:273.82ms
step:485/500 train_loss:4.2190 train_time:130058ms step_avg:273.81ms
step:486/500 train_loss:4.2446 train_time:130328ms step_avg:273.80ms
step:487/500 train_loss:4.1966 train_time:130602ms step_avg:273.80ms
step:488/500 train_loss:4.2172 train_time:130873ms step_avg:273.79ms
step:489/500 train_loss:4.4215 train_time:131147ms step_avg:273.79ms
step:490/500 train_loss:4.2831 train_time:131416ms step_avg:273.78ms
step:491/500 train_loss:4.1824 train_time:131690ms step_avg:273.78ms
step:492/500 train_loss:4.1877 train_time:131961ms step_avg:273.78ms
step:493/500 train_loss:4.2983 train_time:132232ms step_avg:273.77ms
step:494/500 train_loss:4.1438 train_time:132506ms step_avg:273.77ms
step:495/500 train_loss:4.2938 train_time:132777ms step_avg:273.77ms
step:496/500 train_loss:4.2159 train_time:133050ms step_avg:273.76ms
step:497/500 train_loss:4.1643 train_time:133321ms step_avg:273.76ms
step:498/500 train_loss:4.2950 train_time:133593ms step_avg:273.76ms
step:499/500 train_loss:4.3869 train_time:133864ms step_avg:273.75ms
step:500/500 train_loss:4.4327 train_time:134135ms step_avg:273.74ms
step:500/500 val_loss:4.2852 train_time:134136ms step_avg:273.75ms
