====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # also save the seed for reproducibility
        if args.seed is not None:
            log['seed'] = args.seed
        # save token weighting config
        if tw_config.enabled:
            log['tw_config'] = {
                'function': tw_config.function,
                'clamp_min': tw_config.clamp_min,
                'clamp_max': tw_config.clamp_max,
                'schedule': tw_config.schedule,
            }
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 21:36:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   46C    P0             83W /  310W |    2363MiB /  81559MiB |      4%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     30%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   50C    P0             89W /  310W |    2363MiB /  81559MiB |     28%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             86W /  310W |    2363MiB /  81559MiB |     21%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   43C    P0             80W /  310W |    2363MiB /  81559MiB |     31%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0             86W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   52C    P0             87W /  310W |    2363MiB /  81559MiB |     19%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   45C    P0             83W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           67353      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           67354      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           67355      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           67356      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           67357      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           67358      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           67359      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           67360      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 123 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: log
  clamp: [0.8, 1.3]
  schedule: constant
====================================================================================================
step:0/500 val_loss:16.0073 train_time:270ms step_avg:nanms
step:1/500 train_loss:15.9981 train_time:55560ms step_avg:nanms w_mean:1.000 w_std:0.000 w_min:1.000 w_max:1.000
step:2/500 train_loss:9.3706 train_time:56153ms step_avg:nanms
step:3/500 train_loss:8.9747 train_time:56418ms step_avg:nanms
step:4/500 train_loss:8.6425 train_time:56687ms step_avg:nanms
step:5/500 train_loss:8.1127 train_time:56954ms step_avg:nanms
step:6/500 train_loss:7.8040 train_time:57229ms step_avg:nanms
step:7/500 train_loss:7.3947 train_time:57495ms step_avg:nanms
step:8/500 train_loss:7.5211 train_time:57768ms step_avg:nanms
step:9/500 train_loss:7.2809 train_time:58033ms step_avg:nanms
step:10/500 train_loss:7.1196 train_time:58303ms step_avg:nanms
step:11/500 train_loss:7.0874 train_time:267ms step_avg:nanms
step:12/500 train_loss:7.0096 train_time:535ms step_avg:nanms
step:13/500 train_loss:6.8239 train_time:803ms step_avg:267.60ms
step:14/500 train_loss:6.8579 train_time:1070ms step_avg:267.59ms
step:15/500 train_loss:6.8257 train_time:1341ms step_avg:268.22ms
step:16/500 train_loss:6.7390 train_time:1608ms step_avg:268.03ms
step:17/500 train_loss:6.7443 train_time:1878ms step_avg:268.30ms
step:18/500 train_loss:6.7773 train_time:2146ms step_avg:268.23ms
step:19/500 train_loss:6.5898 train_time:2415ms step_avg:268.39ms
step:20/500 train_loss:6.5994 train_time:2686ms step_avg:268.58ms
step:21/500 train_loss:6.2630 train_time:2961ms step_avg:269.15ms
step:22/500 train_loss:6.6516 train_time:3225ms step_avg:268.74ms
step:23/500 train_loss:6.8886 train_time:3497ms step_avg:268.96ms
step:24/500 train_loss:6.5287 train_time:3765ms step_avg:268.95ms
step:25/500 train_loss:6.6409 train_time:4038ms step_avg:269.18ms
step:26/500 train_loss:6.3857 train_time:4306ms step_avg:269.09ms
step:27/500 train_loss:6.2812 train_time:4576ms step_avg:269.20ms
step:28/500 train_loss:6.4545 train_time:4845ms step_avg:269.16ms
step:29/500 train_loss:6.1071 train_time:5118ms step_avg:269.34ms
step:30/500 train_loss:6.3878 train_time:5390ms step_avg:269.48ms
step:31/500 train_loss:6.2388 train_time:5663ms step_avg:269.65ms
step:32/500 train_loss:6.1983 train_time:5930ms step_avg:269.56ms
step:33/500 train_loss:6.0157 train_time:6203ms step_avg:269.71ms
step:34/500 train_loss:6.3488 train_time:6473ms step_avg:269.72ms
step:35/500 train_loss:6.2668 train_time:6745ms step_avg:269.79ms
step:36/500 train_loss:6.4143 train_time:7014ms step_avg:269.76ms
step:37/500 train_loss:6.3439 train_time:7285ms step_avg:269.83ms
step:38/500 train_loss:6.2268 train_time:7559ms step_avg:269.97ms
step:39/500 train_loss:6.1246 train_time:7829ms step_avg:269.97ms
step:40/500 train_loss:6.1731 train_time:8101ms step_avg:270.04ms
step:41/500 train_loss:6.0808 train_time:8370ms step_avg:270.00ms
step:42/500 train_loss:6.1183 train_time:8643ms step_avg:270.08ms
step:43/500 train_loss:5.9916 train_time:8911ms step_avg:270.03ms
step:44/500 train_loss:6.0667 train_time:9185ms step_avg:270.16ms
step:45/500 train_loss:6.0631 train_time:9458ms step_avg:270.23ms
step:46/500 train_loss:6.2436 train_time:9729ms step_avg:270.26ms
step:47/500 train_loss:6.0468 train_time:10001ms step_avg:270.30ms
step:48/500 train_loss:5.8875 train_time:10273ms step_avg:270.33ms
step:49/500 train_loss:6.1395 train_time:10542ms step_avg:270.31ms
step:50/500 train_loss:6.0028 train_time:10811ms step_avg:270.27ms
step:51/500 train_loss:6.1584 train_time:11082ms step_avg:270.30ms w_mean:1.000 w_std:0.105 w_min:0.939 w_max:1.527
step:52/500 train_loss:6.0189 train_time:11353ms step_avg:270.32ms
step:53/500 train_loss:5.8719 train_time:11625ms step_avg:270.34ms
step:54/500 train_loss:5.9969 train_time:11896ms step_avg:270.37ms
step:55/500 train_loss:5.9191 train_time:12166ms step_avg:270.36ms
step:56/500 train_loss:6.2180 train_time:12435ms step_avg:270.34ms
step:57/500 train_loss:5.9129 train_time:12706ms step_avg:270.34ms
step:58/500 train_loss:5.7889 train_time:12979ms step_avg:270.39ms
step:59/500 train_loss:5.9509 train_time:13250ms step_avg:270.41ms
step:60/500 train_loss:5.8868 train_time:13520ms step_avg:270.40ms
step:61/500 train_loss:5.9765 train_time:13790ms step_avg:270.39ms
step:62/500 train_loss:5.7774 train_time:14062ms step_avg:270.43ms
step:63/500 train_loss:5.8789 train_time:14334ms step_avg:270.46ms
step:64/500 train_loss:5.8441 train_time:14603ms step_avg:270.43ms
step:65/500 train_loss:5.7599 train_time:14876ms step_avg:270.47ms
step:66/500 train_loss:5.6734 train_time:15145ms step_avg:270.44ms
step:67/500 train_loss:5.8588 train_time:15419ms step_avg:270.51ms
step:68/500 train_loss:5.7095 train_time:15691ms step_avg:270.54ms
step:69/500 train_loss:5.9616 train_time:15965ms step_avg:270.59ms
step:70/500 train_loss:5.6223 train_time:16233ms step_avg:270.55ms
step:71/500 train_loss:5.6633 train_time:16504ms step_avg:270.56ms
step:72/500 train_loss:5.8503 train_time:16779ms step_avg:270.64ms
step:73/500 train_loss:5.8022 train_time:17047ms step_avg:270.59ms
step:74/500 train_loss:5.6870 train_time:17319ms step_avg:270.61ms
step:75/500 train_loss:5.7980 train_time:17589ms step_avg:270.60ms
step:76/500 train_loss:5.7538 train_time:17863ms step_avg:270.66ms
step:77/500 train_loss:5.7265 train_time:18132ms step_avg:270.63ms
step:78/500 train_loss:5.8150 train_time:18404ms step_avg:270.65ms
step:79/500 train_loss:5.8306 train_time:18676ms step_avg:270.67ms
step:80/500 train_loss:5.6995 train_time:18947ms step_avg:270.67ms
step:81/500 train_loss:5.7890 train_time:19217ms step_avg:270.67ms
step:82/500 train_loss:5.5563 train_time:19486ms step_avg:270.64ms
step:83/500 train_loss:5.7203 train_time:19760ms step_avg:270.68ms
step:84/500 train_loss:5.7034 train_time:20031ms step_avg:270.69ms
step:85/500 train_loss:5.6518 train_time:20302ms step_avg:270.69ms
step:86/500 train_loss:5.5304 train_time:20573ms step_avg:270.70ms
step:87/500 train_loss:5.7293 train_time:20843ms step_avg:270.69ms
step:88/500 train_loss:5.6404 train_time:21118ms step_avg:270.75ms
step:89/500 train_loss:5.6933 train_time:21389ms step_avg:270.74ms
step:90/500 train_loss:5.6778 train_time:21661ms step_avg:270.77ms
step:91/500 train_loss:5.5914 train_time:21931ms step_avg:270.75ms
step:92/500 train_loss:5.5972 train_time:22204ms step_avg:270.78ms
step:93/500 train_loss:5.6941 train_time:22477ms step_avg:270.80ms
step:94/500 train_loss:5.5408 train_time:22745ms step_avg:270.78ms
step:95/500 train_loss:5.5381 train_time:23019ms step_avg:270.81ms
step:96/500 train_loss:5.5522 train_time:23290ms step_avg:270.82ms
step:97/500 train_loss:5.4795 train_time:23563ms step_avg:270.84ms
step:98/500 train_loss:5.5440 train_time:23832ms step_avg:270.82ms
step:99/500 train_loss:5.4723 train_time:24105ms step_avg:270.84ms
step:100/500 train_loss:5.5901 train_time:24380ms step_avg:270.89ms
step:101/500 train_loss:5.5534 train_time:24651ms step_avg:270.89ms w_mean:1.000 w_std:0.119 w_min:0.932 w_max:1.514
step:102/500 train_loss:5.4530 train_time:24922ms step_avg:270.89ms
step:103/500 train_loss:5.5515 train_time:25194ms step_avg:270.90ms
step:104/500 train_loss:5.5207 train_time:25464ms step_avg:270.90ms
step:105/500 train_loss:5.3517 train_time:25735ms step_avg:270.90ms
step:106/500 train_loss:5.4702 train_time:26007ms step_avg:270.90ms
step:107/500 train_loss:5.6576 train_time:26282ms step_avg:270.95ms
step:108/500 train_loss:5.4557 train_time:26555ms step_avg:270.97ms
step:109/500 train_loss:5.2111 train_time:26824ms step_avg:270.95ms
step:110/500 train_loss:5.4215 train_time:27096ms step_avg:270.96ms
step:111/500 train_loss:5.3851 train_time:27365ms step_avg:270.95ms
step:112/500 train_loss:5.3585 train_time:27637ms step_avg:270.95ms
step:113/500 train_loss:5.4595 train_time:27905ms step_avg:270.93ms
step:114/500 train_loss:5.3917 train_time:28181ms step_avg:270.97ms
step:115/500 train_loss:5.2461 train_time:28454ms step_avg:270.99ms
step:116/500 train_loss:5.4113 train_time:28724ms step_avg:270.98ms
step:117/500 train_loss:5.2754 train_time:28997ms step_avg:271.00ms
step:118/500 train_loss:5.2662 train_time:29270ms step_avg:271.02ms
step:119/500 train_loss:5.3932 train_time:29541ms step_avg:271.02ms
step:120/500 train_loss:5.3675 train_time:29813ms step_avg:271.03ms
step:121/500 train_loss:5.3156 train_time:30086ms step_avg:271.05ms
step:122/500 train_loss:5.1955 train_time:30360ms step_avg:271.07ms
step:123/500 train_loss:5.2971 train_time:30629ms step_avg:271.06ms
step:124/500 train_loss:5.1546 train_time:30902ms step_avg:271.07ms
step:125/500 train_loss:5.4573 train_time:31177ms step_avg:271.10ms
step:125/500 val_loss:5.2789 train_time:31178ms step_avg:271.11ms
step:126/500 train_loss:5.2972 train_time:31457ms step_avg:271.18ms
step:127/500 train_loss:5.2871 train_time:31735ms step_avg:271.24ms
step:128/500 train_loss:5.3454 train_time:32006ms step_avg:271.24ms
step:129/500 train_loss:5.2126 train_time:32275ms step_avg:271.22ms
step:130/500 train_loss:5.4729 train_time:32543ms step_avg:271.19ms
step:131/500 train_loss:5.2633 train_time:32818ms step_avg:271.23ms
step:132/500 train_loss:5.2600 train_time:33090ms step_avg:271.23ms
step:133/500 train_loss:5.2143 train_time:33363ms step_avg:271.24ms
step:134/500 train_loss:5.2448 train_time:33631ms step_avg:271.22ms
step:135/500 train_loss:5.1824 train_time:33904ms step_avg:271.23ms
step:136/500 train_loss:5.2439 train_time:34178ms step_avg:271.26ms
step:137/500 train_loss:5.0508 train_time:34452ms step_avg:271.27ms
step:138/500 train_loss:5.2061 train_time:34722ms step_avg:271.27ms
step:139/500 train_loss:5.1696 train_time:34994ms step_avg:271.27ms
step:140/500 train_loss:5.1754 train_time:35264ms step_avg:271.26ms
step:141/500 train_loss:5.2268 train_time:35537ms step_avg:271.27ms
step:142/500 train_loss:5.1309 train_time:35806ms step_avg:271.26ms
step:143/500 train_loss:5.2104 train_time:36079ms step_avg:271.27ms
step:144/500 train_loss:5.0220 train_time:36352ms step_avg:271.28ms
step:145/500 train_loss:5.1770 train_time:36623ms step_avg:271.28ms
step:146/500 train_loss:5.1200 train_time:36897ms step_avg:271.30ms
step:147/500 train_loss:5.0307 train_time:37168ms step_avg:271.30ms
step:148/500 train_loss:5.1493 train_time:37439ms step_avg:271.30ms
step:149/500 train_loss:5.1234 train_time:37710ms step_avg:271.30ms
step:150/500 train_loss:5.1831 train_time:37984ms step_avg:271.31ms
step:151/500 train_loss:5.1919 train_time:38258ms step_avg:271.34ms w_mean:1.000 w_std:0.130 w_min:0.925 w_max:1.504
step:152/500 train_loss:5.1196 train_time:38528ms step_avg:271.32ms
step:153/500 train_loss:5.0863 train_time:38801ms step_avg:271.33ms
step:154/500 train_loss:5.1709 train_time:39071ms step_avg:271.33ms
step:155/500 train_loss:5.1031 train_time:39343ms step_avg:271.33ms
step:156/500 train_loss:5.0917 train_time:39618ms step_avg:271.36ms
step:157/500 train_loss:5.1020 train_time:39891ms step_avg:271.36ms
step:158/500 train_loss:5.2298 train_time:40164ms step_avg:271.38ms
step:159/500 train_loss:5.0191 train_time:40432ms step_avg:271.36ms
step:160/500 train_loss:5.0789 train_time:40703ms step_avg:271.36ms
step:161/500 train_loss:4.9307 train_time:40978ms step_avg:271.38ms
step:162/500 train_loss:5.0849 train_time:41251ms step_avg:271.39ms
step:163/500 train_loss:5.1124 train_time:41522ms step_avg:271.38ms
step:164/500 train_loss:5.1052 train_time:41796ms step_avg:271.40ms
step:165/500 train_loss:4.9309 train_time:42067ms step_avg:271.40ms
step:166/500 train_loss:5.0488 train_time:42339ms step_avg:271.41ms
step:167/500 train_loss:5.1991 train_time:42609ms step_avg:271.40ms
step:168/500 train_loss:4.9794 train_time:42882ms step_avg:271.41ms
step:169/500 train_loss:5.0549 train_time:43159ms step_avg:271.44ms
step:170/500 train_loss:4.9282 train_time:43431ms step_avg:271.44ms
step:171/500 train_loss:4.8683 train_time:43702ms step_avg:271.44ms
step:172/500 train_loss:4.9791 train_time:43976ms step_avg:271.45ms
step:173/500 train_loss:4.9524 train_time:44248ms step_avg:271.46ms
step:174/500 train_loss:5.0095 train_time:44520ms step_avg:271.47ms
step:175/500 train_loss:5.1414 train_time:44791ms step_avg:271.46ms
step:176/500 train_loss:5.0365 train_time:45062ms step_avg:271.46ms
step:177/500 train_loss:4.8712 train_time:45334ms step_avg:271.46ms
step:178/500 train_loss:4.8556 train_time:45604ms step_avg:271.45ms
step:179/500 train_loss:4.8829 train_time:45878ms step_avg:271.46ms
step:180/500 train_loss:4.9324 train_time:46149ms step_avg:271.46ms
step:181/500 train_loss:4.9155 train_time:46421ms step_avg:271.47ms
step:182/500 train_loss:5.0316 train_time:46692ms step_avg:271.46ms
step:183/500 train_loss:4.9193 train_time:46964ms step_avg:271.47ms
step:184/500 train_loss:4.8525 train_time:47234ms step_avg:271.46ms
step:185/500 train_loss:4.8747 train_time:47504ms step_avg:271.45ms
step:186/500 train_loss:5.0059 train_time:47779ms step_avg:271.47ms
step:187/500 train_loss:4.8841 train_time:48052ms step_avg:271.48ms
step:188/500 train_loss:5.1260 train_time:48324ms step_avg:271.48ms
step:189/500 train_loss:4.9207 train_time:48861ms step_avg:272.97ms
step:190/500 train_loss:4.8396 train_time:49393ms step_avg:274.41ms
step:191/500 train_loss:4.9979 train_time:49660ms step_avg:274.37ms
step:192/500 train_loss:4.8417 train_time:49928ms step_avg:274.33ms
step:193/500 train_loss:4.7558 train_time:50198ms step_avg:274.31ms
step:194/500 train_loss:4.9607 train_time:50476ms step_avg:274.33ms
step:195/500 train_loss:4.8931 train_time:50745ms step_avg:274.30ms
step:196/500 train_loss:5.0793 train_time:51014ms step_avg:274.27ms
step:197/500 train_loss:4.9687 train_time:51282ms step_avg:274.24ms
step:198/500 train_loss:4.8179 train_time:51561ms step_avg:274.26ms
step:199/500 train_loss:4.8518 train_time:51831ms step_avg:274.24ms
step:200/500 train_loss:4.7570 train_time:52102ms step_avg:274.22ms
step:201/500 train_loss:4.8331 train_time:52374ms step_avg:274.21ms w_mean:1.000 w_std:0.139 w_min:0.921 w_max:1.496
step:202/500 train_loss:4.7604 train_time:52643ms step_avg:274.18ms
step:203/500 train_loss:4.9743 train_time:52917ms step_avg:274.18ms
step:204/500 train_loss:4.8863 train_time:53192ms step_avg:274.19ms
step:205/500 train_loss:4.8437 train_time:53464ms step_avg:274.17ms
step:206/500 train_loss:5.0007 train_time:53732ms step_avg:274.14ms
step:207/500 train_loss:4.6802 train_time:54002ms step_avg:274.12ms
step:208/500 train_loss:4.8302 train_time:54278ms step_avg:274.13ms
step:209/500 train_loss:4.7841 train_time:54552ms step_avg:274.13ms
step:210/500 train_loss:4.9479 train_time:54821ms step_avg:274.10ms
step:211/500 train_loss:4.8750 train_time:55093ms step_avg:274.09ms
step:212/500 train_loss:4.7604 train_time:55363ms step_avg:274.08ms
step:213/500 train_loss:4.8963 train_time:55634ms step_avg:274.06ms
step:214/500 train_loss:4.7294 train_time:55904ms step_avg:274.04ms
step:215/500 train_loss:4.8257 train_time:56179ms step_avg:274.04ms
step:216/500 train_loss:4.6818 train_time:56452ms step_avg:274.04ms
step:217/500 train_loss:4.8019 train_time:56727ms step_avg:274.05ms
step:218/500 train_loss:4.7795 train_time:57000ms step_avg:274.04ms
step:219/500 train_loss:4.7553 train_time:57271ms step_avg:274.03ms
step:220/500 train_loss:4.7662 train_time:57544ms step_avg:274.02ms
step:221/500 train_loss:4.7966 train_time:57819ms step_avg:274.02ms
step:222/500 train_loss:4.8244 train_time:58091ms step_avg:274.01ms
step:223/500 train_loss:4.7769 train_time:58364ms step_avg:274.01ms
step:224/500 train_loss:4.7701 train_time:58634ms step_avg:273.99ms
step:225/500 train_loss:4.8963 train_time:58903ms step_avg:273.97ms
step:226/500 train_loss:4.6454 train_time:59179ms step_avg:273.98ms
step:227/500 train_loss:4.6793 train_time:59452ms step_avg:273.97ms
step:228/500 train_loss:4.6590 train_time:59722ms step_avg:273.95ms
step:229/500 train_loss:4.8283 train_time:59994ms step_avg:273.95ms
step:230/500 train_loss:4.6529 train_time:60265ms step_avg:273.93ms
step:231/500 train_loss:4.8080 train_time:60537ms step_avg:273.92ms
step:232/500 train_loss:4.6742 train_time:60806ms step_avg:273.90ms
step:233/500 train_loss:4.6330 train_time:61079ms step_avg:273.90ms
step:234/500 train_loss:4.8343 train_time:61352ms step_avg:273.89ms
step:235/500 train_loss:4.6720 train_time:61624ms step_avg:273.88ms
step:236/500 train_loss:4.6141 train_time:61898ms step_avg:273.89ms
step:237/500 train_loss:4.8643 train_time:62169ms step_avg:273.87ms
step:238/500 train_loss:4.7476 train_time:62441ms step_avg:273.86ms
step:239/500 train_loss:4.6554 train_time:62712ms step_avg:273.85ms
step:240/500 train_loss:4.7977 train_time:62983ms step_avg:273.84ms
step:241/500 train_loss:4.7787 train_time:63259ms step_avg:273.85ms
step:242/500 train_loss:4.6898 train_time:63529ms step_avg:273.83ms
step:243/500 train_loss:4.8375 train_time:63801ms step_avg:273.83ms
step:244/500 train_loss:4.6797 train_time:64073ms step_avg:273.82ms
step:245/500 train_loss:4.6887 train_time:64344ms step_avg:273.80ms
step:246/500 train_loss:4.7589 train_time:64618ms step_avg:273.81ms
step:247/500 train_loss:4.7144 train_time:64890ms step_avg:273.80ms
step:248/500 train_loss:4.6717 train_time:65161ms step_avg:273.79ms
step:249/500 train_loss:4.8338 train_time:65435ms step_avg:273.78ms
step:250/500 train_loss:4.5777 train_time:65703ms step_avg:273.76ms
step:250/500 val_loss:4.6826 train_time:65705ms step_avg:273.77ms
step:251/500 train_loss:4.6212 train_time:65978ms step_avg:273.77ms w_mean:1.000 w_std:0.143 w_min:0.918 w_max:1.492
step:252/500 train_loss:4.7495 train_time:66257ms step_avg:273.79ms
step:253/500 train_loss:4.7357 train_time:66532ms step_avg:273.79ms
step:254/500 train_loss:4.6237 train_time:66802ms step_avg:273.78ms
step:255/500 train_loss:4.6459 train_time:67070ms step_avg:273.76ms
step:256/500 train_loss:4.7617 train_time:67349ms step_avg:273.78ms
step:257/500 train_loss:4.7130 train_time:67621ms step_avg:273.77ms
step:258/500 train_loss:4.6881 train_time:67892ms step_avg:273.76ms
step:259/500 train_loss:4.6198 train_time:68163ms step_avg:273.75ms
step:260/500 train_loss:4.6353 train_time:68434ms step_avg:273.74ms
step:261/500 train_loss:4.7031 train_time:68709ms step_avg:273.74ms
step:262/500 train_loss:4.7078 train_time:68981ms step_avg:273.73ms
step:263/500 train_loss:4.6136 train_time:69252ms step_avg:273.72ms
step:264/500 train_loss:4.5674 train_time:69524ms step_avg:273.72ms
step:265/500 train_loss:4.6178 train_time:69797ms step_avg:273.71ms
step:266/500 train_loss:4.4784 train_time:70072ms step_avg:273.72ms
step:267/500 train_loss:4.5333 train_time:70344ms step_avg:273.71ms
step:268/500 train_loss:4.5774 train_time:70613ms step_avg:273.69ms
step:269/500 train_loss:4.5330 train_time:70889ms step_avg:273.70ms
step:270/500 train_loss:4.5056 train_time:71162ms step_avg:273.70ms
step:271/500 train_loss:4.7176 train_time:71432ms step_avg:273.69ms
step:272/500 train_loss:4.6550 train_time:71703ms step_avg:273.68ms
step:273/500 train_loss:4.5150 train_time:71978ms step_avg:273.68ms
step:274/500 train_loss:4.5641 train_time:72252ms step_avg:273.68ms
step:275/500 train_loss:4.6817 train_time:72523ms step_avg:273.67ms
step:276/500 train_loss:4.6903 train_time:72792ms step_avg:273.65ms
step:277/500 train_loss:4.8909 train_time:73068ms step_avg:273.66ms
step:278/500 train_loss:4.6381 train_time:73342ms step_avg:273.66ms
step:279/500 train_loss:4.7706 train_time:73613ms step_avg:273.66ms
step:280/500 train_loss:4.6153 train_time:73887ms step_avg:273.65ms
step:281/500 train_loss:4.6836 train_time:74156ms step_avg:273.64ms
step:282/500 train_loss:4.5761 train_time:74428ms step_avg:273.63ms
step:283/500 train_loss:4.6986 train_time:74699ms step_avg:273.62ms
step:284/500 train_loss:4.5136 train_time:74973ms step_avg:273.63ms
step:285/500 train_loss:4.6779 train_time:75247ms step_avg:273.62ms
step:286/500 train_loss:4.6650 train_time:75518ms step_avg:273.62ms
step:287/500 train_loss:4.6956 train_time:75791ms step_avg:273.61ms
step:288/500 train_loss:4.5616 train_time:76064ms step_avg:273.61ms
step:289/500 train_loss:4.6251 train_time:76333ms step_avg:273.60ms
step:290/500 train_loss:4.4829 train_time:76607ms step_avg:273.60ms
step:291/500 train_loss:4.4865 train_time:76880ms step_avg:273.59ms
step:292/500 train_loss:4.6063 train_time:77153ms step_avg:273.59ms
step:293/500 train_loss:4.4980 train_time:77421ms step_avg:273.57ms
step:294/500 train_loss:4.5439 train_time:77694ms step_avg:273.57ms
step:295/500 train_loss:4.5714 train_time:77970ms step_avg:273.58ms
step:296/500 train_loss:4.4388 train_time:78243ms step_avg:273.58ms
step:297/500 train_loss:4.4298 train_time:78512ms step_avg:273.56ms
step:298/500 train_loss:4.4503 train_time:78786ms step_avg:273.56ms
step:299/500 train_loss:4.5633 train_time:79056ms step_avg:273.55ms
step:300/500 train_loss:4.4443 train_time:79329ms step_avg:273.55ms
step:301/500 train_loss:4.6219 train_time:79599ms step_avg:273.54ms w_mean:1.000 w_std:0.144 w_min:0.918 w_max:1.492
step:302/500 train_loss:4.5917 train_time:79873ms step_avg:273.54ms
step:303/500 train_loss:4.5166 train_time:80148ms step_avg:273.54ms
step:304/500 train_loss:4.5779 train_time:80418ms step_avg:273.53ms
step:305/500 train_loss:4.5648 train_time:80691ms step_avg:273.53ms
step:306/500 train_loss:5.0275 train_time:80964ms step_avg:273.53ms
step:307/500 train_loss:4.5255 train_time:81233ms step_avg:273.51ms
step:308/500 train_loss:4.4243 train_time:81507ms step_avg:273.51ms
step:309/500 train_loss:4.6122 train_time:81776ms step_avg:273.50ms
step:310/500 train_loss:4.4206 train_time:82051ms step_avg:273.50ms
step:311/500 train_loss:4.6485 train_time:82322ms step_avg:273.50ms
step:312/500 train_loss:4.5576 train_time:82594ms step_avg:273.49ms
step:313/500 train_loss:4.4741 train_time:82868ms step_avg:273.49ms
step:314/500 train_loss:4.5969 train_time:83140ms step_avg:273.49ms
step:315/500 train_loss:4.7277 train_time:83411ms step_avg:273.48ms
step:316/500 train_loss:4.5665 train_time:83682ms step_avg:273.47ms
step:317/500 train_loss:4.4565 train_time:83954ms step_avg:273.46ms
step:318/500 train_loss:4.4636 train_time:84224ms step_avg:273.45ms
step:319/500 train_loss:4.4861 train_time:84495ms step_avg:273.45ms
step:320/500 train_loss:4.4307 train_time:84767ms step_avg:273.44ms
step:321/500 train_loss:4.5306 train_time:85042ms step_avg:273.45ms
step:322/500 train_loss:4.5403 train_time:85312ms step_avg:273.44ms
step:323/500 train_loss:4.4993 train_time:85589ms step_avg:273.45ms
step:324/500 train_loss:4.5655 train_time:85860ms step_avg:273.44ms
step:325/500 train_loss:4.5685 train_time:86131ms step_avg:273.43ms
step:326/500 train_loss:4.6363 train_time:86403ms step_avg:273.43ms
step:327/500 train_loss:4.4884 train_time:86675ms step_avg:273.42ms
step:328/500 train_loss:4.9245 train_time:86949ms step_avg:273.42ms
step:329/500 train_loss:4.6383 train_time:87218ms step_avg:273.41ms
step:330/500 train_loss:4.4300 train_time:87490ms step_avg:273.41ms
step:331/500 train_loss:4.3984 train_time:87763ms step_avg:273.40ms
step:332/500 train_loss:4.5450 train_time:88034ms step_avg:273.40ms
step:333/500 train_loss:4.4641 train_time:88308ms step_avg:273.40ms
step:334/500 train_loss:4.4513 train_time:88581ms step_avg:273.40ms
step:335/500 train_loss:4.4083 train_time:88853ms step_avg:273.39ms
step:336/500 train_loss:4.6065 train_time:89122ms step_avg:273.38ms
step:337/500 train_loss:4.5422 train_time:89394ms step_avg:273.38ms
step:338/500 train_loss:5.0640 train_time:89668ms step_avg:273.38ms
step:339/500 train_loss:4.5144 train_time:89941ms step_avg:273.38ms
step:340/500 train_loss:4.4858 train_time:90213ms step_avg:273.37ms
step:341/500 train_loss:4.4619 train_time:90487ms step_avg:273.38ms
step:342/500 train_loss:4.4106 train_time:90758ms step_avg:273.37ms
step:343/500 train_loss:4.3868 train_time:91030ms step_avg:273.36ms
step:344/500 train_loss:4.4490 train_time:91301ms step_avg:273.36ms
step:345/500 train_loss:4.5365 train_time:91575ms step_avg:273.36ms
step:346/500 train_loss:4.4338 train_time:91849ms step_avg:273.36ms
step:347/500 train_loss:4.3826 train_time:92119ms step_avg:273.35ms
step:348/500 train_loss:4.4297 train_time:92391ms step_avg:273.35ms
step:349/500 train_loss:4.4277 train_time:92664ms step_avg:273.35ms
step:350/500 train_loss:4.3536 train_time:92935ms step_avg:273.34ms
step:351/500 train_loss:4.0481 train_time:93209ms step_avg:273.34ms w_mean:1.000 w_std:0.155 w_min:0.914 w_max:1.486
step:352/500 train_loss:4.3323 train_time:93483ms step_avg:273.34ms
step:353/500 train_loss:4.6707 train_time:93755ms step_avg:273.34ms
step:354/500 train_loss:4.2208 train_time:94025ms step_avg:273.33ms
step:355/500 train_loss:4.4650 train_time:94296ms step_avg:273.32ms
step:356/500 train_loss:4.3705 train_time:94571ms step_avg:273.33ms
step:357/500 train_loss:4.4670 train_time:94843ms step_avg:273.32ms
step:358/500 train_loss:4.4829 train_time:95113ms step_avg:273.31ms
step:359/500 train_loss:4.3744 train_time:95389ms step_avg:273.32ms
step:360/500 train_loss:4.6967 train_time:95661ms step_avg:273.32ms
step:361/500 train_loss:4.1175 train_time:95933ms step_avg:273.31ms
step:362/500 train_loss:4.5950 train_time:96205ms step_avg:273.31ms
step:363/500 train_loss:4.4891 train_time:96476ms step_avg:273.30ms
step:364/500 train_loss:4.3803 train_time:96749ms step_avg:273.30ms
step:365/500 train_loss:4.3152 train_time:97019ms step_avg:273.29ms
step:366/500 train_loss:4.4673 train_time:97293ms step_avg:273.30ms
step:367/500 train_loss:4.3972 train_time:97566ms step_avg:273.29ms
step:368/500 train_loss:4.3837 train_time:97835ms step_avg:273.28ms
step:369/500 train_loss:4.3917 train_time:98108ms step_avg:273.28ms
step:370/500 train_loss:4.2818 train_time:98379ms step_avg:273.27ms
step:371/500 train_loss:4.4310 train_time:98654ms step_avg:273.28ms
step:372/500 train_loss:4.3692 train_time:98926ms step_avg:273.28ms
step:373/500 train_loss:4.2433 train_time:99194ms step_avg:273.26ms
step:374/500 train_loss:4.4368 train_time:99468ms step_avg:273.26ms
step:375/500 train_loss:4.3705 train_time:99742ms step_avg:273.27ms
step:375/500 val_loss:4.3842 train_time:99743ms step_avg:273.27ms
step:376/500 train_loss:4.3652 train_time:100019ms step_avg:273.28ms
step:377/500 train_loss:4.4209 train_time:100296ms step_avg:273.29ms
step:378/500 train_loss:4.3202 train_time:100925ms step_avg:274.25ms
step:379/500 train_loss:4.3701 train_time:101195ms step_avg:274.24ms
step:380/500 train_loss:4.4481 train_time:101727ms step_avg:274.94ms
step:381/500 train_loss:4.4725 train_time:101993ms step_avg:274.91ms
step:382/500 train_loss:4.4235 train_time:102262ms step_avg:274.90ms
step:383/500 train_loss:4.4006 train_time:102530ms step_avg:274.88ms
step:384/500 train_loss:4.2993 train_time:102809ms step_avg:274.89ms
step:385/500 train_loss:4.4029 train_time:103081ms step_avg:274.88ms
step:386/500 train_loss:4.3163 train_time:103349ms step_avg:274.86ms
step:387/500 train_loss:4.4433 train_time:103617ms step_avg:274.85ms
step:388/500 train_loss:4.6440 train_time:103892ms step_avg:274.85ms
step:389/500 train_loss:4.3399 train_time:104167ms step_avg:274.85ms
step:390/500 train_loss:4.2986 train_time:104437ms step_avg:274.84ms
step:391/500 train_loss:4.4246 train_time:104706ms step_avg:274.82ms
step:392/500 train_loss:4.3525 train_time:104980ms step_avg:274.82ms
step:393/500 train_loss:4.4509 train_time:105252ms step_avg:274.81ms
step:394/500 train_loss:4.2801 train_time:105522ms step_avg:274.80ms
step:395/500 train_loss:4.4096 train_time:105790ms step_avg:274.78ms
step:396/500 train_loss:4.1992 train_time:106066ms step_avg:274.78ms
step:397/500 train_loss:4.3585 train_time:106338ms step_avg:274.78ms
step:398/500 train_loss:4.4512 train_time:106610ms step_avg:274.77ms
step:399/500 train_loss:4.4026 train_time:106882ms step_avg:274.76ms
step:400/500 train_loss:4.3214 train_time:107153ms step_avg:274.75ms
step:401/500 train_loss:4.3934 train_time:107425ms step_avg:274.74ms w_mean:1.000 w_std:0.150 w_min:0.915 w_max:1.486
step:402/500 train_loss:4.4226 train_time:107696ms step_avg:274.73ms
step:403/500 train_loss:4.3912 train_time:107966ms step_avg:274.72ms
step:404/500 train_loss:4.4862 train_time:108241ms step_avg:274.72ms
step:405/500 train_loss:4.2698 train_time:108510ms step_avg:274.71ms
step:406/500 train_loss:4.3169 train_time:108781ms step_avg:274.70ms
step:407/500 train_loss:4.5838 train_time:109050ms step_avg:274.68ms
step:408/500 train_loss:4.3551 train_time:109323ms step_avg:274.68ms
step:409/500 train_loss:4.3495 train_time:109595ms step_avg:274.67ms
step:410/500 train_loss:4.3969 train_time:109867ms step_avg:274.67ms
step:411/500 train_loss:4.2795 train_time:110140ms step_avg:274.66ms
step:412/500 train_loss:4.2965 train_time:110409ms step_avg:274.65ms
step:413/500 train_loss:4.7194 train_time:110685ms step_avg:274.65ms
step:414/500 train_loss:4.1718 train_time:110956ms step_avg:274.64ms
step:415/500 train_loss:4.5273 train_time:111227ms step_avg:274.64ms
step:416/500 train_loss:4.3066 train_time:111500ms step_avg:274.63ms
step:417/500 train_loss:4.2984 train_time:111771ms step_avg:274.62ms
step:418/500 train_loss:4.4792 train_time:112045ms step_avg:274.62ms
step:419/500 train_loss:4.2175 train_time:112315ms step_avg:274.61ms
step:420/500 train_loss:4.3137 train_time:112588ms step_avg:274.61ms
step:421/500 train_loss:4.2906 train_time:112862ms step_avg:274.60ms
step:422/500 train_loss:4.1731 train_time:113132ms step_avg:274.59ms
step:423/500 train_loss:4.2785 train_time:113406ms step_avg:274.59ms
step:424/500 train_loss:4.3921 train_time:113677ms step_avg:274.58ms
step:425/500 train_loss:4.2011 train_time:113950ms step_avg:274.58ms
step:426/500 train_loss:4.3566 train_time:114220ms step_avg:274.57ms
step:427/500 train_loss:4.2403 train_time:114490ms step_avg:274.56ms
step:428/500 train_loss:4.4096 train_time:114763ms step_avg:274.55ms
step:429/500 train_loss:4.3718 train_time:115037ms step_avg:274.55ms
step:430/500 train_loss:4.2830 train_time:115309ms step_avg:274.54ms
step:431/500 train_loss:4.2599 train_time:115579ms step_avg:274.54ms
step:432/500 train_loss:4.2159 train_time:115850ms step_avg:274.53ms
step:433/500 train_loss:4.2918 train_time:116124ms step_avg:274.52ms
step:434/500 train_loss:4.3647 train_time:116394ms step_avg:274.51ms
step:435/500 train_loss:4.2996 train_time:116667ms step_avg:274.51ms
step:436/500 train_loss:4.3440 train_time:116942ms step_avg:274.51ms
step:437/500 train_loss:4.3567 train_time:117210ms step_avg:274.50ms
step:438/500 train_loss:4.2439 train_time:117482ms step_avg:274.49ms
step:439/500 train_loss:4.2586 train_time:117755ms step_avg:274.49ms
step:440/500 train_loss:4.2246 train_time:118027ms step_avg:274.48ms
step:441/500 train_loss:4.4068 train_time:118299ms step_avg:274.48ms
step:442/500 train_loss:4.3132 train_time:118571ms step_avg:274.47ms
step:443/500 train_loss:4.2872 train_time:118845ms step_avg:274.47ms
step:444/500 train_loss:4.1800 train_time:119115ms step_avg:274.46ms
step:445/500 train_loss:4.4364 train_time:119386ms step_avg:274.45ms
step:446/500 train_loss:4.3584 train_time:119659ms step_avg:274.45ms
step:447/500 train_loss:4.3599 train_time:119929ms step_avg:274.44ms
step:448/500 train_loss:4.2782 train_time:120204ms step_avg:274.44ms
step:449/500 train_loss:4.3590 train_time:120477ms step_avg:274.43ms
step:450/500 train_loss:4.1975 train_time:120749ms step_avg:274.43ms
step:451/500 train_loss:4.2437 train_time:121018ms step_avg:274.42ms w_mean:1.000 w_std:0.152 w_min:0.913 w_max:1.484
step:452/500 train_loss:4.1278 train_time:121291ms step_avg:274.41ms
step:453/500 train_loss:4.2316 train_time:121562ms step_avg:274.41ms
step:454/500 train_loss:4.2058 train_time:121835ms step_avg:274.40ms
step:455/500 train_loss:4.1799 train_time:122105ms step_avg:274.39ms
step:456/500 train_loss:4.3911 train_time:122378ms step_avg:274.39ms
step:457/500 train_loss:4.2417 train_time:122650ms step_avg:274.39ms
step:458/500 train_loss:4.3337 train_time:122923ms step_avg:274.38ms
step:459/500 train_loss:4.3676 train_time:123191ms step_avg:274.37ms
step:460/500 train_loss:4.1693 train_time:123465ms step_avg:274.37ms
step:461/500 train_loss:4.3405 train_time:123738ms step_avg:274.36ms
step:462/500 train_loss:4.2485 train_time:124009ms step_avg:274.36ms
step:463/500 train_loss:4.2280 train_time:124284ms step_avg:274.36ms
step:464/500 train_loss:4.3132 train_time:124556ms step_avg:274.35ms
step:465/500 train_loss:4.2505 train_time:124828ms step_avg:274.35ms
step:466/500 train_loss:4.2500 train_time:125098ms step_avg:274.34ms
step:467/500 train_loss:4.3785 train_time:125370ms step_avg:274.33ms
step:468/500 train_loss:4.3878 train_time:125645ms step_avg:274.33ms
step:469/500 train_loss:4.3448 train_time:125917ms step_avg:274.33ms
step:470/500 train_loss:4.2546 train_time:126188ms step_avg:274.32ms
step:471/500 train_loss:4.3410 train_time:126461ms step_avg:274.32ms
step:472/500 train_loss:4.3854 train_time:126730ms step_avg:274.31ms
step:473/500 train_loss:4.2898 train_time:127006ms step_avg:274.31ms
step:474/500 train_loss:4.2690 train_time:127278ms step_avg:274.31ms
step:475/500 train_loss:4.1410 train_time:127551ms step_avg:274.30ms
step:476/500 train_loss:4.5656 train_time:127821ms step_avg:274.29ms
step:477/500 train_loss:4.3210 train_time:128092ms step_avg:274.29ms
step:478/500 train_loss:4.1309 train_time:128364ms step_avg:274.28ms
step:479/500 train_loss:4.3248 train_time:128637ms step_avg:274.28ms
step:480/500 train_loss:4.3093 train_time:128909ms step_avg:274.27ms
step:481/500 train_loss:4.4297 train_time:129182ms step_avg:274.27ms
step:482/500 train_loss:4.2650 train_time:129456ms step_avg:274.27ms
step:483/500 train_loss:4.0862 train_time:129727ms step_avg:274.26ms
step:484/500 train_loss:4.3554 train_time:129998ms step_avg:274.26ms
step:485/500 train_loss:4.2069 train_time:130270ms step_avg:274.25ms
step:486/500 train_loss:4.2312 train_time:130546ms step_avg:274.26ms
step:487/500 train_loss:4.1753 train_time:130816ms step_avg:274.25ms
step:488/500 train_loss:4.1993 train_time:131089ms step_avg:274.24ms
step:489/500 train_loss:4.4103 train_time:131363ms step_avg:274.24ms
step:490/500 train_loss:4.2669 train_time:131635ms step_avg:274.24ms
step:491/500 train_loss:4.1683 train_time:131906ms step_avg:274.23ms
step:492/500 train_loss:4.1729 train_time:132177ms step_avg:274.23ms
step:493/500 train_loss:4.2885 train_time:132451ms step_avg:274.23ms
step:494/500 train_loss:4.1337 train_time:132724ms step_avg:274.22ms
step:495/500 train_loss:4.2803 train_time:132996ms step_avg:274.22ms
step:496/500 train_loss:4.1957 train_time:133270ms step_avg:274.22ms
step:497/500 train_loss:4.1438 train_time:133545ms step_avg:274.22ms
step:498/500 train_loss:4.2890 train_time:133816ms step_avg:274.21ms
step:499/500 train_loss:4.3724 train_time:134088ms step_avg:274.21ms
step:500/500 train_loss:4.4201 train_time:134360ms step_avg:274.21ms
step:500/500 val_loss:4.2714 train_time:134362ms step_avg:274.21ms
