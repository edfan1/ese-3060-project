====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock and keep timing accurate, but skip checkpoint writes to save disk
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        if master_process:
            print("Checkpoint saving disabled; skipping state serialization.")
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 22:55:02 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   45C    P0             83W /  310W |    2363MiB /  81559MiB |     17%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     29%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   49C    P0             89W /  310W |    2363MiB /  81559MiB |     33%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             86W /  310W |    2363MiB /  81559MiB |     36%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   43C    P0             81W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0             85W /  310W |    2363MiB /  81559MiB |     31%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   53C    P0             88W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           81331      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           81332      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           81333      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           81334      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           81335      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           81336      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           81337      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           81338      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 42 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: focal
  clamp: [0.4, 2.0]
  schedule: constant
  focal_gamma: 2.0
====================================================================================================
step:0/500 val_loss:15.9787 train_time:286ms step_avg:nanms
step:1/500 train_loss:15.9775 train_time:51115ms step_avg:nanms w_mean:1.000 w_std:0.079 w_min:0.534 w_max:1.335
step:2/500 train_loss:9.3668 train_time:51383ms step_avg:nanms
step:3/500 train_loss:8.9493 train_time:51648ms step_avg:nanms
step:4/500 train_loss:8.5028 train_time:51915ms step_avg:nanms
step:5/500 train_loss:7.9804 train_time:52183ms step_avg:nanms
step:6/500 train_loss:7.8004 train_time:52455ms step_avg:nanms
step:7/500 train_loss:7.4504 train_time:52723ms step_avg:nanms
step:8/500 train_loss:7.5839 train_time:52989ms step_avg:nanms
step:9/500 train_loss:7.2844 train_time:53255ms step_avg:nanms
step:10/500 train_loss:7.0843 train_time:53521ms step_avg:nanms
step:11/500 train_loss:7.0417 train_time:268ms step_avg:nanms
step:12/500 train_loss:6.9859 train_time:536ms step_avg:nanms
step:13/500 train_loss:6.8699 train_time:803ms step_avg:267.55ms
step:14/500 train_loss:6.8325 train_time:1075ms step_avg:268.79ms
step:15/500 train_loss:6.7939 train_time:1342ms step_avg:268.33ms
step:16/500 train_loss:6.7214 train_time:1609ms step_avg:268.17ms
step:17/500 train_loss:6.7329 train_time:1877ms step_avg:268.14ms
step:18/500 train_loss:6.7590 train_time:2145ms step_avg:268.17ms
step:19/500 train_loss:6.5754 train_time:2415ms step_avg:268.30ms
step:20/500 train_loss:6.5889 train_time:2684ms step_avg:268.41ms
step:21/500 train_loss:6.2452 train_time:2952ms step_avg:268.40ms
step:22/500 train_loss:6.6320 train_time:3221ms step_avg:268.39ms
step:23/500 train_loss:6.8641 train_time:3490ms step_avg:268.48ms
step:24/500 train_loss:6.5077 train_time:3760ms step_avg:268.55ms
step:25/500 train_loss:6.6265 train_time:4029ms step_avg:268.57ms
step:26/500 train_loss:6.3445 train_time:4299ms step_avg:268.72ms
step:27/500 train_loss:6.2614 train_time:4569ms step_avg:268.78ms
step:28/500 train_loss:6.4262 train_time:4839ms step_avg:268.82ms
step:29/500 train_loss:6.0917 train_time:5109ms step_avg:268.89ms
step:30/500 train_loss:6.3730 train_time:5380ms step_avg:269.01ms
step:31/500 train_loss:6.2169 train_time:5648ms step_avg:268.94ms
step:32/500 train_loss:6.1836 train_time:5918ms step_avg:269.01ms
step:33/500 train_loss:6.0012 train_time:6187ms step_avg:268.98ms
step:34/500 train_loss:6.3367 train_time:6458ms step_avg:269.07ms
step:35/500 train_loss:6.2445 train_time:6730ms step_avg:269.21ms
step:36/500 train_loss:6.4093 train_time:6999ms step_avg:269.18ms
step:37/500 train_loss:6.3228 train_time:7268ms step_avg:269.19ms
step:38/500 train_loss:6.2194 train_time:7537ms step_avg:269.17ms
step:39/500 train_loss:6.1141 train_time:7805ms step_avg:269.13ms
step:40/500 train_loss:6.1673 train_time:8079ms step_avg:269.30ms
step:41/500 train_loss:6.0672 train_time:8349ms step_avg:269.32ms
step:42/500 train_loss:6.1076 train_time:8621ms step_avg:269.39ms
step:43/500 train_loss:5.9750 train_time:8890ms step_avg:269.39ms
step:44/500 train_loss:6.0608 train_time:9160ms step_avg:269.42ms
step:45/500 train_loss:6.0524 train_time:9431ms step_avg:269.45ms
step:46/500 train_loss:6.2347 train_time:9702ms step_avg:269.51ms
step:47/500 train_loss:6.0385 train_time:9975ms step_avg:269.58ms
step:48/500 train_loss:5.8782 train_time:10243ms step_avg:269.54ms
step:49/500 train_loss:6.1232 train_time:10514ms step_avg:269.58ms
step:50/500 train_loss:5.9918 train_time:10784ms step_avg:269.60ms
step:51/500 train_loss:6.1515 train_time:11054ms step_avg:269.60ms w_mean:1.000 w_std:0.093 w_min:0.978 w_max:2.444
step:52/500 train_loss:6.0090 train_time:11323ms step_avg:269.60ms
step:53/500 train_loss:5.8601 train_time:11596ms step_avg:269.66ms
step:54/500 train_loss:5.9772 train_time:11865ms step_avg:269.66ms
step:55/500 train_loss:5.9040 train_time:12138ms step_avg:269.73ms
step:56/500 train_loss:6.2101 train_time:12410ms step_avg:269.77ms
step:57/500 train_loss:5.8986 train_time:12683ms step_avg:269.85ms
step:58/500 train_loss:5.7759 train_time:12953ms step_avg:269.85ms
step:59/500 train_loss:5.9395 train_time:13225ms step_avg:269.89ms
step:60/500 train_loss:5.8712 train_time:13495ms step_avg:269.90ms
step:61/500 train_loss:5.9726 train_time:13767ms step_avg:269.95ms
step:62/500 train_loss:5.7653 train_time:14040ms step_avg:270.00ms
step:63/500 train_loss:5.8682 train_time:14310ms step_avg:270.00ms
step:64/500 train_loss:5.8280 train_time:14583ms step_avg:270.05ms
step:65/500 train_loss:5.7735 train_time:14852ms step_avg:270.04ms
step:66/500 train_loss:5.6630 train_time:15123ms step_avg:270.05ms
step:67/500 train_loss:5.8367 train_time:15399ms step_avg:270.15ms
step:68/500 train_loss:5.6949 train_time:15670ms step_avg:270.16ms
step:69/500 train_loss:5.9407 train_time:15939ms step_avg:270.16ms
step:70/500 train_loss:5.6105 train_time:16211ms step_avg:270.19ms
step:71/500 train_loss:5.6199 train_time:16483ms step_avg:270.22ms
step:72/500 train_loss:5.8325 train_time:16755ms step_avg:270.24ms
step:73/500 train_loss:5.7805 train_time:17022ms step_avg:270.20ms
step:74/500 train_loss:5.6677 train_time:17298ms step_avg:270.27ms
step:75/500 train_loss:5.7738 train_time:17570ms step_avg:270.32ms
step:76/500 train_loss:5.7459 train_time:17840ms step_avg:270.30ms
step:77/500 train_loss:5.7113 train_time:18112ms step_avg:270.32ms
step:78/500 train_loss:5.7890 train_time:18385ms step_avg:270.36ms
step:79/500 train_loss:5.8131 train_time:18659ms step_avg:270.42ms
step:80/500 train_loss:5.6783 train_time:18929ms step_avg:270.41ms
step:81/500 train_loss:5.7710 train_time:19202ms step_avg:270.46ms
step:82/500 train_loss:5.5292 train_time:19475ms step_avg:270.49ms
step:83/500 train_loss:5.7061 train_time:19744ms step_avg:270.47ms
step:84/500 train_loss:5.6705 train_time:20017ms step_avg:270.49ms
step:85/500 train_loss:5.6342 train_time:20286ms step_avg:270.49ms
step:86/500 train_loss:5.5022 train_time:20560ms step_avg:270.52ms
step:87/500 train_loss:5.7057 train_time:20831ms step_avg:270.53ms
step:88/500 train_loss:5.6129 train_time:21101ms step_avg:270.53ms
step:89/500 train_loss:5.6665 train_time:21375ms step_avg:270.57ms
step:90/500 train_loss:5.6540 train_time:21644ms step_avg:270.55ms
step:91/500 train_loss:5.5641 train_time:21919ms step_avg:270.61ms
step:92/500 train_loss:5.5744 train_time:22191ms step_avg:270.62ms
step:93/500 train_loss:5.6632 train_time:22462ms step_avg:270.62ms
step:94/500 train_loss:5.5174 train_time:22734ms step_avg:270.64ms
step:95/500 train_loss:5.5019 train_time:23006ms step_avg:270.65ms
step:96/500 train_loss:5.5254 train_time:23278ms step_avg:270.68ms
step:97/500 train_loss:5.4435 train_time:23548ms step_avg:270.67ms
step:98/500 train_loss:5.5172 train_time:23822ms step_avg:270.70ms
step:99/500 train_loss:5.4406 train_time:24095ms step_avg:270.73ms
step:100/500 train_loss:5.5682 train_time:24364ms step_avg:270.71ms
step:101/500 train_loss:5.5227 train_time:24637ms step_avg:270.73ms w_mean:1.000 w_std:0.062 w_min:0.989 w_max:2.474
step:102/500 train_loss:5.4335 train_time:24907ms step_avg:270.73ms
step:103/500 train_loss:5.5256 train_time:25181ms step_avg:270.76ms
step:104/500 train_loss:5.4912 train_time:25450ms step_avg:270.75ms
step:105/500 train_loss:5.3211 train_time:25723ms step_avg:270.77ms
step:106/500 train_loss:5.4352 train_time:25996ms step_avg:270.79ms
step:107/500 train_loss:5.6458 train_time:26270ms step_avg:270.82ms
step:108/500 train_loss:5.4212 train_time:26538ms step_avg:270.80ms
step:109/500 train_loss:5.1713 train_time:26807ms step_avg:270.78ms
step:110/500 train_loss:5.3815 train_time:27080ms step_avg:270.80ms
step:111/500 train_loss:5.3542 train_time:27352ms step_avg:270.81ms
step:112/500 train_loss:5.3263 train_time:27623ms step_avg:270.82ms
step:113/500 train_loss:5.4280 train_time:27899ms step_avg:270.87ms
step:114/500 train_loss:5.3589 train_time:28172ms step_avg:270.89ms
step:115/500 train_loss:5.2077 train_time:28442ms step_avg:270.87ms
step:116/500 train_loss:5.3826 train_time:28714ms step_avg:270.89ms
step:117/500 train_loss:5.2411 train_time:28987ms step_avg:270.90ms
step:118/500 train_loss:5.2275 train_time:29258ms step_avg:270.91ms
step:119/500 train_loss:5.3531 train_time:29526ms step_avg:270.88ms
step:120/500 train_loss:5.3379 train_time:29800ms step_avg:270.91ms
step:121/500 train_loss:5.2659 train_time:30074ms step_avg:270.94ms
step:122/500 train_loss:5.1556 train_time:30344ms step_avg:270.93ms
step:123/500 train_loss:5.2571 train_time:30615ms step_avg:270.93ms
step:124/500 train_loss:5.1222 train_time:30886ms step_avg:270.93ms
step:125/500 train_loss:5.4267 train_time:31159ms step_avg:270.94ms
step:125/500 val_loss:5.2494 train_time:31161ms step_avg:270.96ms
step:126/500 train_loss:5.2739 train_time:31432ms step_avg:270.97ms
step:127/500 train_loss:5.2499 train_time:31708ms step_avg:271.01ms
step:128/500 train_loss:5.3176 train_time:31986ms step_avg:271.07ms
step:129/500 train_loss:5.1752 train_time:32254ms step_avg:271.05ms
step:130/500 train_loss:5.4406 train_time:32525ms step_avg:271.04ms
step:131/500 train_loss:5.2338 train_time:32798ms step_avg:271.05ms
step:132/500 train_loss:5.2254 train_time:33069ms step_avg:271.06ms
step:133/500 train_loss:5.1730 train_time:33342ms step_avg:271.07ms
step:134/500 train_loss:5.2158 train_time:33611ms step_avg:271.06ms
step:135/500 train_loss:5.1403 train_time:33885ms step_avg:271.08ms
step:136/500 train_loss:5.2170 train_time:34157ms step_avg:271.09ms
step:137/500 train_loss:5.0175 train_time:34428ms step_avg:271.09ms
step:138/500 train_loss:5.1808 train_time:34703ms step_avg:271.11ms
step:139/500 train_loss:5.1364 train_time:34975ms step_avg:271.13ms
step:140/500 train_loss:5.1450 train_time:35246ms step_avg:271.12ms
step:141/500 train_loss:5.1928 train_time:35518ms step_avg:271.13ms
step:142/500 train_loss:5.0993 train_time:35790ms step_avg:271.14ms
step:143/500 train_loss:5.1741 train_time:36061ms step_avg:271.14ms
step:144/500 train_loss:4.9940 train_time:36331ms step_avg:271.13ms
step:145/500 train_loss:5.1459 train_time:36604ms step_avg:271.14ms
step:146/500 train_loss:5.0858 train_time:36878ms step_avg:271.16ms
step:147/500 train_loss:4.9948 train_time:37147ms step_avg:271.15ms
step:148/500 train_loss:5.1216 train_time:37418ms step_avg:271.15ms
step:149/500 train_loss:5.0930 train_time:37691ms step_avg:271.16ms
step:150/500 train_loss:5.1542 train_time:37963ms step_avg:271.16ms
step:151/500 train_loss:5.1680 train_time:38233ms step_avg:271.15ms w_mean:1.000 w_std:0.063 w_min:0.990 w_max:2.475
step:152/500 train_loss:5.0881 train_time:38506ms step_avg:271.17ms
step:153/500 train_loss:5.0634 train_time:38782ms step_avg:271.20ms
step:154/500 train_loss:5.1388 train_time:39054ms step_avg:271.21ms
step:155/500 train_loss:5.0765 train_time:39327ms step_avg:271.22ms
step:156/500 train_loss:5.0495 train_time:39600ms step_avg:271.23ms
step:157/500 train_loss:5.0664 train_time:39869ms step_avg:271.22ms
step:158/500 train_loss:5.1975 train_time:40144ms step_avg:271.25ms
step:159/500 train_loss:4.9831 train_time:40416ms step_avg:271.25ms
step:160/500 train_loss:5.0424 train_time:40688ms step_avg:271.25ms
step:161/500 train_loss:4.8996 train_time:40959ms step_avg:271.25ms
step:162/500 train_loss:5.0511 train_time:41228ms step_avg:271.24ms
step:163/500 train_loss:5.0842 train_time:41504ms step_avg:271.27ms
step:164/500 train_loss:5.0707 train_time:41777ms step_avg:271.28ms
step:165/500 train_loss:4.9001 train_time:42049ms step_avg:271.29ms
step:166/500 train_loss:5.0112 train_time:42323ms step_avg:271.30ms
step:167/500 train_loss:5.1639 train_time:42595ms step_avg:271.30ms
step:168/500 train_loss:4.9469 train_time:42865ms step_avg:271.30ms
step:169/500 train_loss:5.0315 train_time:43138ms step_avg:271.31ms
step:170/500 train_loss:4.9003 train_time:43411ms step_avg:271.32ms
step:171/500 train_loss:4.8391 train_time:43684ms step_avg:271.33ms
step:172/500 train_loss:4.9495 train_time:43957ms step_avg:271.34ms
step:173/500 train_loss:4.9214 train_time:44228ms step_avg:271.33ms
step:174/500 train_loss:4.9799 train_time:44504ms step_avg:271.36ms
step:175/500 train_loss:5.1130 train_time:44776ms step_avg:271.37ms
step:176/500 train_loss:5.0043 train_time:45047ms step_avg:271.37ms
step:177/500 train_loss:4.8415 train_time:45321ms step_avg:271.38ms
step:178/500 train_loss:4.8165 train_time:45592ms step_avg:271.38ms
step:179/500 train_loss:4.8633 train_time:45864ms step_avg:271.38ms
step:180/500 train_loss:4.9066 train_time:46135ms step_avg:271.39ms
step:181/500 train_loss:4.8901 train_time:46408ms step_avg:271.39ms
step:182/500 train_loss:5.0038 train_time:46684ms step_avg:271.42ms
step:183/500 train_loss:4.8915 train_time:46955ms step_avg:271.42ms
step:184/500 train_loss:4.8185 train_time:47228ms step_avg:271.42ms
step:185/500 train_loss:4.8523 train_time:47502ms step_avg:271.44ms
step:186/500 train_loss:4.9759 train_time:47775ms step_avg:271.45ms
step:187/500 train_loss:4.8557 train_time:48046ms step_avg:271.45ms
step:188/500 train_loss:5.0948 train_time:48318ms step_avg:271.45ms
step:189/500 train_loss:4.8958 train_time:48848ms step_avg:272.89ms
step:190/500 train_loss:4.8105 train_time:49386ms step_avg:274.37ms
step:191/500 train_loss:4.9648 train_time:49653ms step_avg:274.32ms
step:192/500 train_loss:4.8128 train_time:49923ms step_avg:274.30ms
step:193/500 train_loss:4.7315 train_time:50190ms step_avg:274.26ms
step:194/500 train_loss:4.9329 train_time:50465ms step_avg:274.27ms
step:195/500 train_loss:4.8746 train_time:50738ms step_avg:274.26ms
step:196/500 train_loss:5.0514 train_time:51006ms step_avg:274.23ms
step:197/500 train_loss:4.9507 train_time:51276ms step_avg:274.20ms
step:198/500 train_loss:4.7878 train_time:51547ms step_avg:274.19ms
step:199/500 train_loss:4.8330 train_time:51824ms step_avg:274.20ms
step:200/500 train_loss:4.7242 train_time:52095ms step_avg:274.19ms
step:201/500 train_loss:4.8068 train_time:52365ms step_avg:274.16ms w_mean:1.000 w_std:0.053 w_min:0.993 w_max:2.482
step:202/500 train_loss:4.7229 train_time:52636ms step_avg:274.15ms
step:203/500 train_loss:4.9535 train_time:52907ms step_avg:274.13ms
step:204/500 train_loss:4.8622 train_time:53182ms step_avg:274.13ms
step:205/500 train_loss:4.8322 train_time:53449ms step_avg:274.10ms
step:206/500 train_loss:4.9817 train_time:53723ms step_avg:274.10ms
step:207/500 train_loss:4.6569 train_time:53993ms step_avg:274.08ms
step:208/500 train_loss:4.8067 train_time:54265ms step_avg:274.06ms
step:209/500 train_loss:4.7630 train_time:54536ms step_avg:274.05ms
step:210/500 train_loss:4.9238 train_time:54809ms step_avg:274.04ms
step:211/500 train_loss:4.8479 train_time:55086ms step_avg:274.06ms
step:212/500 train_loss:4.7364 train_time:55354ms step_avg:274.03ms
step:213/500 train_loss:4.8604 train_time:55626ms step_avg:274.02ms
step:214/500 train_loss:4.7035 train_time:55898ms step_avg:274.01ms
step:215/500 train_loss:4.7945 train_time:56169ms step_avg:273.99ms
step:216/500 train_loss:4.6471 train_time:56443ms step_avg:274.00ms
step:217/500 train_loss:4.7720 train_time:56714ms step_avg:273.98ms
step:218/500 train_loss:4.7554 train_time:56987ms step_avg:273.98ms
step:219/500 train_loss:4.7282 train_time:57259ms step_avg:273.96ms
step:220/500 train_loss:4.7415 train_time:57529ms step_avg:273.95ms
step:221/500 train_loss:4.7668 train_time:57804ms step_avg:273.95ms
step:222/500 train_loss:4.8055 train_time:58078ms step_avg:273.95ms
step:223/500 train_loss:4.7424 train_time:58351ms step_avg:273.95ms
step:224/500 train_loss:4.7443 train_time:58620ms step_avg:273.93ms
step:225/500 train_loss:4.8717 train_time:58890ms step_avg:273.91ms
step:226/500 train_loss:4.6108 train_time:59162ms step_avg:273.90ms
step:227/500 train_loss:4.6490 train_time:59433ms step_avg:273.89ms
step:228/500 train_loss:4.6340 train_time:59706ms step_avg:273.88ms
step:229/500 train_loss:4.7981 train_time:59979ms step_avg:273.88ms
step:230/500 train_loss:4.6303 train_time:60248ms step_avg:273.86ms
step:231/500 train_loss:4.7802 train_time:60523ms step_avg:273.86ms
step:232/500 train_loss:4.6443 train_time:60798ms step_avg:273.86ms
step:233/500 train_loss:4.6122 train_time:61068ms step_avg:273.85ms
step:234/500 train_loss:4.8142 train_time:61343ms step_avg:273.85ms
step:235/500 train_loss:4.6479 train_time:61616ms step_avg:273.85ms
step:236/500 train_loss:4.5948 train_time:61889ms step_avg:273.84ms
step:237/500 train_loss:4.8346 train_time:62162ms step_avg:273.84ms
step:238/500 train_loss:4.7203 train_time:62430ms step_avg:273.81ms
step:239/500 train_loss:4.6373 train_time:62707ms step_avg:273.83ms
step:240/500 train_loss:4.7769 train_time:62981ms step_avg:273.83ms
step:241/500 train_loss:4.7592 train_time:63250ms step_avg:273.81ms
step:242/500 train_loss:4.6637 train_time:63522ms step_avg:273.80ms
step:243/500 train_loss:4.8191 train_time:63793ms step_avg:273.79ms
step:244/500 train_loss:4.6538 train_time:64065ms step_avg:273.78ms
step:245/500 train_loss:4.6672 train_time:64338ms step_avg:273.78ms
step:246/500 train_loss:4.7359 train_time:64609ms step_avg:273.77ms
step:247/500 train_loss:4.6934 train_time:64884ms step_avg:273.77ms
step:248/500 train_loss:4.6522 train_time:65156ms step_avg:273.76ms
step:249/500 train_loss:4.8203 train_time:65428ms step_avg:273.76ms
step:250/500 train_loss:4.5576 train_time:65700ms step_avg:273.75ms
step:250/500 val_loss:4.6598 train_time:65702ms step_avg:273.76ms
step:251/500 train_loss:4.5969 train_time:65974ms step_avg:273.75ms w_mean:1.000 w_std:0.050 w_min:0.994 w_max:2.484
step:252/500 train_loss:4.7239 train_time:66255ms step_avg:273.78ms
step:253/500 train_loss:4.7190 train_time:66532ms step_avg:273.79ms
step:254/500 train_loss:4.5939 train_time:66803ms step_avg:273.78ms
step:255/500 train_loss:4.6103 train_time:67071ms step_avg:273.76ms
step:256/500 train_loss:4.7466 train_time:67344ms step_avg:273.75ms
step:257/500 train_loss:4.6908 train_time:67616ms step_avg:273.75ms
step:258/500 train_loss:4.6676 train_time:67889ms step_avg:273.75ms
step:259/500 train_loss:4.5993 train_time:68156ms step_avg:273.72ms
step:260/500 train_loss:4.6128 train_time:68432ms step_avg:273.73ms
step:261/500 train_loss:4.6844 train_time:68706ms step_avg:273.73ms
step:262/500 train_loss:4.6875 train_time:68975ms step_avg:273.71ms
step:263/500 train_loss:4.5927 train_time:69247ms step_avg:273.70ms
step:264/500 train_loss:4.5434 train_time:69517ms step_avg:273.69ms
step:265/500 train_loss:4.5953 train_time:69792ms step_avg:273.69ms
step:266/500 train_loss:4.4584 train_time:70063ms step_avg:273.68ms
step:267/500 train_loss:4.5115 train_time:70335ms step_avg:273.68ms
step:268/500 train_loss:4.5489 train_time:70607ms step_avg:273.67ms
step:269/500 train_loss:4.5137 train_time:70877ms step_avg:273.66ms
step:270/500 train_loss:4.4793 train_time:71150ms step_avg:273.65ms
step:271/500 train_loss:4.6941 train_time:71420ms step_avg:273.64ms
step:272/500 train_loss:4.6301 train_time:71695ms step_avg:273.65ms
step:273/500 train_loss:4.4929 train_time:71965ms step_avg:273.63ms
step:274/500 train_loss:4.5417 train_time:72236ms step_avg:273.62ms
step:275/500 train_loss:4.6524 train_time:72510ms step_avg:273.62ms
step:276/500 train_loss:4.6717 train_time:72783ms step_avg:273.62ms
step:277/500 train_loss:4.8687 train_time:73054ms step_avg:273.61ms
step:278/500 train_loss:4.6167 train_time:73328ms step_avg:273.61ms
step:279/500 train_loss:4.7450 train_time:73600ms step_avg:273.61ms
step:280/500 train_loss:4.5871 train_time:73871ms step_avg:273.60ms
step:281/500 train_loss:4.6642 train_time:74142ms step_avg:273.59ms
step:282/500 train_loss:4.5487 train_time:74417ms step_avg:273.59ms
step:283/500 train_loss:4.6629 train_time:74692ms step_avg:273.60ms
step:284/500 train_loss:4.4881 train_time:74963ms step_avg:273.59ms
step:285/500 train_loss:4.6559 train_time:75235ms step_avg:273.58ms
step:286/500 train_loss:4.6408 train_time:75508ms step_avg:273.58ms
step:287/500 train_loss:4.6738 train_time:75778ms step_avg:273.57ms
step:288/500 train_loss:4.5396 train_time:76050ms step_avg:273.56ms
step:289/500 train_loss:4.6052 train_time:76323ms step_avg:273.56ms
step:290/500 train_loss:4.4588 train_time:76596ms step_avg:273.56ms
step:291/500 train_loss:4.4578 train_time:76869ms step_avg:273.56ms
step:292/500 train_loss:4.5816 train_time:77139ms step_avg:273.54ms
step:293/500 train_loss:4.4713 train_time:77413ms step_avg:273.54ms
step:294/500 train_loss:4.5240 train_time:77686ms step_avg:273.54ms
step:295/500 train_loss:4.5432 train_time:77955ms step_avg:273.53ms
step:296/500 train_loss:4.4098 train_time:78228ms step_avg:273.52ms
step:297/500 train_loss:4.4025 train_time:78500ms step_avg:273.52ms
step:298/500 train_loss:4.4301 train_time:78770ms step_avg:273.51ms
step:299/500 train_loss:4.5335 train_time:79042ms step_avg:273.50ms
step:300/500 train_loss:4.4143 train_time:79316ms step_avg:273.50ms
step:301/500 train_loss:4.5942 train_time:79592ms step_avg:273.51ms w_mean:1.000 w_std:0.055 w_min:0.992 w_max:2.481
step:302/500 train_loss:4.5672 train_time:79861ms step_avg:273.50ms
step:303/500 train_loss:4.4882 train_time:80133ms step_avg:273.49ms
step:304/500 train_loss:4.5593 train_time:80406ms step_avg:273.49ms
step:305/500 train_loss:4.5422 train_time:80676ms step_avg:273.48ms
step:306/500 train_loss:5.0125 train_time:80951ms step_avg:273.48ms
step:307/500 train_loss:4.4958 train_time:81222ms step_avg:273.47ms
step:308/500 train_loss:4.4054 train_time:81495ms step_avg:273.47ms
step:309/500 train_loss:4.5888 train_time:81766ms step_avg:273.47ms
step:310/500 train_loss:4.3926 train_time:82037ms step_avg:273.46ms
step:311/500 train_loss:4.6242 train_time:82312ms step_avg:273.46ms
step:312/500 train_loss:4.5303 train_time:82589ms step_avg:273.47ms
step:313/500 train_loss:4.4464 train_time:82857ms step_avg:273.45ms
step:314/500 train_loss:4.5713 train_time:83132ms step_avg:273.46ms
step:315/500 train_loss:4.6916 train_time:83405ms step_avg:273.46ms
step:316/500 train_loss:4.5353 train_time:83676ms step_avg:273.45ms
step:317/500 train_loss:4.4183 train_time:83950ms step_avg:273.45ms
step:318/500 train_loss:4.4439 train_time:84220ms step_avg:273.44ms
step:319/500 train_loss:4.4573 train_time:84493ms step_avg:273.44ms
step:320/500 train_loss:4.4080 train_time:84763ms step_avg:273.43ms
step:321/500 train_loss:4.4980 train_time:85036ms step_avg:273.43ms
step:322/500 train_loss:4.5069 train_time:85311ms step_avg:273.43ms
step:323/500 train_loss:4.4689 train_time:85582ms step_avg:273.42ms
step:324/500 train_loss:4.5448 train_time:85852ms step_avg:273.41ms
step:325/500 train_loss:4.5440 train_time:86126ms step_avg:273.42ms
step:326/500 train_loss:4.6002 train_time:86401ms step_avg:273.42ms
step:327/500 train_loss:4.4541 train_time:86672ms step_avg:273.41ms
step:328/500 train_loss:4.8970 train_time:86941ms step_avg:273.40ms
step:329/500 train_loss:4.6120 train_time:87215ms step_avg:273.40ms
step:330/500 train_loss:4.4010 train_time:87491ms step_avg:273.41ms
step:331/500 train_loss:4.3609 train_time:87762ms step_avg:273.40ms
step:332/500 train_loss:4.5137 train_time:88035ms step_avg:273.40ms
step:333/500 train_loss:4.4369 train_time:88308ms step_avg:273.40ms
step:334/500 train_loss:4.4232 train_time:88577ms step_avg:273.39ms
step:335/500 train_loss:4.3794 train_time:88852ms step_avg:273.39ms
step:336/500 train_loss:4.5706 train_time:89125ms step_avg:273.39ms
step:337/500 train_loss:4.5055 train_time:89397ms step_avg:273.39ms
step:338/500 train_loss:5.0434 train_time:89670ms step_avg:273.38ms
step:339/500 train_loss:4.4851 train_time:89940ms step_avg:273.37ms
step:340/500 train_loss:4.4522 train_time:90214ms step_avg:273.38ms
step:341/500 train_loss:4.4406 train_time:90487ms step_avg:273.38ms
step:342/500 train_loss:4.3783 train_time:90756ms step_avg:273.36ms
step:343/500 train_loss:4.3507 train_time:91032ms step_avg:273.37ms
step:344/500 train_loss:4.4169 train_time:91305ms step_avg:273.37ms
step:345/500 train_loss:4.5084 train_time:91576ms step_avg:273.36ms
step:346/500 train_loss:4.3996 train_time:91848ms step_avg:273.36ms
step:347/500 train_loss:4.3385 train_time:92120ms step_avg:273.35ms
step:348/500 train_loss:4.3876 train_time:92394ms step_avg:273.35ms
step:349/500 train_loss:4.3983 train_time:92665ms step_avg:273.35ms
step:350/500 train_loss:4.3269 train_time:92936ms step_avg:273.34ms
step:351/500 train_loss:4.0139 train_time:93212ms step_avg:273.35ms w_mean:1.000 w_std:0.043 w_min:0.996 w_max:2.489
step:352/500 train_loss:4.3067 train_time:93486ms step_avg:273.35ms
step:353/500 train_loss:4.6456 train_time:93756ms step_avg:273.34ms
step:354/500 train_loss:4.1828 train_time:94029ms step_avg:273.34ms
step:355/500 train_loss:4.4350 train_time:94300ms step_avg:273.33ms
step:356/500 train_loss:4.3357 train_time:94572ms step_avg:273.33ms
step:357/500 train_loss:4.4376 train_time:94845ms step_avg:273.33ms
step:358/500 train_loss:4.4326 train_time:95118ms step_avg:273.33ms
step:359/500 train_loss:4.3490 train_time:95392ms step_avg:273.33ms
step:360/500 train_loss:4.6342 train_time:95663ms step_avg:273.32ms
step:361/500 train_loss:4.0828 train_time:95935ms step_avg:273.32ms
step:362/500 train_loss:4.5609 train_time:96211ms step_avg:273.33ms
step:363/500 train_loss:4.4543 train_time:96484ms step_avg:273.33ms
step:364/500 train_loss:4.3437 train_time:96755ms step_avg:273.32ms
step:365/500 train_loss:4.2804 train_time:97030ms step_avg:273.32ms
step:366/500 train_loss:4.4350 train_time:97305ms step_avg:273.33ms
step:367/500 train_loss:4.3745 train_time:97576ms step_avg:273.32ms
step:368/500 train_loss:4.3569 train_time:97850ms step_avg:273.32ms
step:369/500 train_loss:4.3593 train_time:98123ms step_avg:273.32ms
step:370/500 train_loss:4.2536 train_time:98396ms step_avg:273.32ms
step:371/500 train_loss:4.4049 train_time:98666ms step_avg:273.31ms
step:372/500 train_loss:4.3305 train_time:98936ms step_avg:273.30ms
step:373/500 train_loss:4.2104 train_time:99212ms step_avg:273.31ms
step:374/500 train_loss:4.4088 train_time:99485ms step_avg:273.31ms
step:375/500 train_loss:4.3352 train_time:99756ms step_avg:273.30ms
step:375/500 val_loss:4.3518 train_time:99758ms step_avg:273.31ms
step:376/500 train_loss:4.3295 train_time:100031ms step_avg:273.31ms
step:377/500 train_loss:4.3895 train_time:100306ms step_avg:273.31ms
step:378/500 train_loss:4.2846 train_time:100834ms step_avg:274.01ms
step:379/500 train_loss:4.3373 train_time:101102ms step_avg:273.99ms
step:380/500 train_loss:4.4111 train_time:101643ms step_avg:274.71ms
step:381/500 train_loss:4.4429 train_time:101910ms step_avg:274.69ms
step:382/500 train_loss:4.3839 train_time:102179ms step_avg:274.67ms
step:383/500 train_loss:4.3638 train_time:102447ms step_avg:274.66ms
step:384/500 train_loss:4.2729 train_time:102725ms step_avg:274.67ms
step:385/500 train_loss:4.3779 train_time:102996ms step_avg:274.66ms
step:386/500 train_loss:4.2864 train_time:103267ms step_avg:274.65ms
step:387/500 train_loss:4.4089 train_time:103534ms step_avg:274.63ms
step:388/500 train_loss:4.6137 train_time:103809ms step_avg:274.63ms
step:389/500 train_loss:4.3083 train_time:104080ms step_avg:274.62ms
step:390/500 train_loss:4.2695 train_time:104350ms step_avg:274.61ms
step:391/500 train_loss:4.3954 train_time:104621ms step_avg:274.60ms
step:392/500 train_loss:4.3161 train_time:104895ms step_avg:274.59ms
step:393/500 train_loss:4.4222 train_time:105164ms step_avg:274.58ms
step:394/500 train_loss:4.2409 train_time:105435ms step_avg:274.57ms
step:395/500 train_loss:4.3835 train_time:105707ms step_avg:274.56ms
step:396/500 train_loss:4.1608 train_time:105976ms step_avg:274.55ms
step:397/500 train_loss:4.3261 train_time:106250ms step_avg:274.55ms
step:398/500 train_loss:4.4194 train_time:106517ms step_avg:274.53ms
step:399/500 train_loss:4.3618 train_time:106793ms step_avg:274.53ms
step:400/500 train_loss:4.2966 train_time:107064ms step_avg:274.52ms
step:401/500 train_loss:4.3488 train_time:107335ms step_avg:274.51ms w_mean:1.000 w_std:0.035 w_min:0.997 w_max:2.492
step:402/500 train_loss:4.3968 train_time:107608ms step_avg:274.51ms
step:403/500 train_loss:4.3591 train_time:107882ms step_avg:274.51ms
step:404/500 train_loss:4.4576 train_time:108152ms step_avg:274.50ms
step:405/500 train_loss:4.2303 train_time:108426ms step_avg:274.50ms
step:406/500 train_loss:4.2806 train_time:108697ms step_avg:274.49ms
step:407/500 train_loss:4.5561 train_time:108968ms step_avg:274.48ms
step:408/500 train_loss:4.3287 train_time:109239ms step_avg:274.47ms
step:409/500 train_loss:4.3151 train_time:109511ms step_avg:274.46ms
step:410/500 train_loss:4.3638 train_time:109784ms step_avg:274.46ms
step:411/500 train_loss:4.2540 train_time:110055ms step_avg:274.45ms
step:412/500 train_loss:4.2692 train_time:110329ms step_avg:274.45ms
step:413/500 train_loss:4.6823 train_time:110600ms step_avg:274.44ms
step:414/500 train_loss:4.1324 train_time:110872ms step_avg:274.43ms
step:415/500 train_loss:4.4958 train_time:111145ms step_avg:274.43ms
step:416/500 train_loss:4.2663 train_time:111416ms step_avg:274.42ms
step:417/500 train_loss:4.2691 train_time:111691ms step_avg:274.42ms
step:418/500 train_loss:4.4485 train_time:111962ms step_avg:274.42ms
step:419/500 train_loss:4.1855 train_time:112232ms step_avg:274.41ms
step:420/500 train_loss:4.2891 train_time:112505ms step_avg:274.40ms
step:421/500 train_loss:4.2559 train_time:112776ms step_avg:274.39ms
step:422/500 train_loss:4.1482 train_time:113050ms step_avg:274.39ms
step:423/500 train_loss:4.2572 train_time:113321ms step_avg:274.38ms
step:424/500 train_loss:4.3685 train_time:113594ms step_avg:274.38ms
step:425/500 train_loss:4.1598 train_time:113866ms step_avg:274.38ms
step:426/500 train_loss:4.3246 train_time:114137ms step_avg:274.37ms
step:427/500 train_loss:4.2062 train_time:114412ms step_avg:274.37ms
step:428/500 train_loss:4.3870 train_time:114685ms step_avg:274.36ms
step:429/500 train_loss:4.3386 train_time:114953ms step_avg:274.35ms
step:430/500 train_loss:4.2503 train_time:115226ms step_avg:274.35ms
step:431/500 train_loss:4.2277 train_time:115497ms step_avg:274.34ms
step:432/500 train_loss:4.1664 train_time:115770ms step_avg:274.34ms
step:433/500 train_loss:4.2617 train_time:116038ms step_avg:274.32ms
step:434/500 train_loss:4.3402 train_time:116312ms step_avg:274.32ms
step:435/500 train_loss:4.2611 train_time:116586ms step_avg:274.32ms
step:436/500 train_loss:4.3147 train_time:116855ms step_avg:274.31ms
step:437/500 train_loss:4.3236 train_time:117128ms step_avg:274.31ms
step:438/500 train_loss:4.2069 train_time:117399ms step_avg:274.30ms
step:439/500 train_loss:4.2289 train_time:117671ms step_avg:274.29ms
step:440/500 train_loss:4.1948 train_time:117944ms step_avg:274.29ms
step:441/500 train_loss:4.3779 train_time:118216ms step_avg:274.28ms
step:442/500 train_loss:4.2776 train_time:118491ms step_avg:274.29ms
step:443/500 train_loss:4.2609 train_time:118762ms step_avg:274.28ms
step:444/500 train_loss:4.1558 train_time:119035ms step_avg:274.27ms
step:445/500 train_loss:4.4053 train_time:119310ms step_avg:274.28ms
step:446/500 train_loss:4.3319 train_time:119583ms step_avg:274.27ms
step:447/500 train_loss:4.3333 train_time:119853ms step_avg:274.26ms
step:448/500 train_loss:4.2451 train_time:120127ms step_avg:274.26ms
step:449/500 train_loss:4.3354 train_time:120396ms step_avg:274.25ms
step:450/500 train_loss:4.1704 train_time:120668ms step_avg:274.25ms
step:451/500 train_loss:4.2118 train_time:120938ms step_avg:274.24ms w_mean:1.000 w_std:0.050 w_min:0.994 w_max:2.485
step:452/500 train_loss:4.1018 train_time:121212ms step_avg:274.24ms
step:453/500 train_loss:4.1983 train_time:121486ms step_avg:274.23ms
step:454/500 train_loss:4.1789 train_time:121758ms step_avg:274.23ms
step:455/500 train_loss:4.1547 train_time:122031ms step_avg:274.23ms
step:456/500 train_loss:4.3637 train_time:122305ms step_avg:274.23ms
step:457/500 train_loss:4.2175 train_time:122575ms step_avg:274.22ms
step:458/500 train_loss:4.3039 train_time:122846ms step_avg:274.21ms
step:459/500 train_loss:4.3353 train_time:123116ms step_avg:274.20ms
step:460/500 train_loss:4.1388 train_time:123393ms step_avg:274.21ms
step:461/500 train_loss:4.3110 train_time:123665ms step_avg:274.20ms
step:462/500 train_loss:4.2087 train_time:123935ms step_avg:274.19ms
step:463/500 train_loss:4.1976 train_time:124209ms step_avg:274.19ms
step:464/500 train_loss:4.2866 train_time:124484ms step_avg:274.19ms
step:465/500 train_loss:4.2238 train_time:124754ms step_avg:274.18ms
step:466/500 train_loss:4.2203 train_time:125029ms step_avg:274.19ms
step:467/500 train_loss:4.3439 train_time:125302ms step_avg:274.18ms
step:468/500 train_loss:4.3448 train_time:125573ms step_avg:274.18ms
step:469/500 train_loss:4.3146 train_time:125845ms step_avg:274.17ms
step:470/500 train_loss:4.2239 train_time:126117ms step_avg:274.17ms
step:471/500 train_loss:4.3050 train_time:126391ms step_avg:274.17ms
step:472/500 train_loss:4.3532 train_time:126660ms step_avg:274.16ms
step:473/500 train_loss:4.2654 train_time:126934ms step_avg:274.16ms
step:474/500 train_loss:4.2352 train_time:127207ms step_avg:274.15ms
step:475/500 train_loss:4.1104 train_time:127476ms step_avg:274.14ms
step:476/500 train_loss:4.5386 train_time:127750ms step_avg:274.14ms
step:477/500 train_loss:4.2930 train_time:128023ms step_avg:274.14ms
step:478/500 train_loss:4.0923 train_time:128297ms step_avg:274.14ms
step:479/500 train_loss:4.3045 train_time:128566ms step_avg:274.13ms
step:480/500 train_loss:4.2777 train_time:128835ms step_avg:274.12ms
step:481/500 train_loss:4.4058 train_time:129110ms step_avg:274.12ms
step:482/500 train_loss:4.2364 train_time:129384ms step_avg:274.12ms
step:483/500 train_loss:4.0532 train_time:129655ms step_avg:274.11ms
step:484/500 train_loss:4.3285 train_time:129927ms step_avg:274.11ms
step:485/500 train_loss:4.1730 train_time:130197ms step_avg:274.10ms
step:486/500 train_loss:4.2037 train_time:130471ms step_avg:274.10ms
step:487/500 train_loss:4.1438 train_time:130743ms step_avg:274.09ms
step:488/500 train_loss:4.1746 train_time:131015ms step_avg:274.09ms
step:489/500 train_loss:4.3830 train_time:131291ms step_avg:274.09ms
step:490/500 train_loss:4.2339 train_time:131562ms step_avg:274.09ms
step:491/500 train_loss:4.1328 train_time:131834ms step_avg:274.08ms
step:492/500 train_loss:4.1504 train_time:132106ms step_avg:274.08ms
step:493/500 train_loss:4.2562 train_time:132376ms step_avg:274.07ms
step:494/500 train_loss:4.1037 train_time:132651ms step_avg:274.07ms
step:495/500 train_loss:4.2475 train_time:132925ms step_avg:274.07ms
step:496/500 train_loss:4.1713 train_time:133196ms step_avg:274.07ms
step:497/500 train_loss:4.1076 train_time:133469ms step_avg:274.06ms
step:498/500 train_loss:4.2564 train_time:133738ms step_avg:274.05ms
step:499/500 train_loss:4.3437 train_time:134012ms step_avg:274.05ms
step:500/500 train_loss:4.3908 train_time:134286ms step_avg:274.05ms
step:500/500 val_loss:4.2414 train_time:134288ms step_avg:274.06ms
