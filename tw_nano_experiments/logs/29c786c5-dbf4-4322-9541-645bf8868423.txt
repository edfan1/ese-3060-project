====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # also save the seed for reproducibility
        if args.seed is not None:
            log['seed'] = args.seed
        # save token weighting config
        if tw_config.enabled:
            log['tw_config'] = {
                'function': tw_config.function,
                'clamp_min': tw_config.clamp_min,
                'clamp_max': tw_config.clamp_max,
                'schedule': tw_config.schedule,
            }
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 21:31:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |     35%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     35%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   49C    P0             89W /  310W |    2363MiB /  81559MiB |     31%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             86W /  310W |    2363MiB /  81559MiB |     32%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   43C    P0             82W /  310W |    2363MiB /  81559MiB |     33%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0             86W /  310W |    2363MiB /  81559MiB |     31%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   53C    P0             88W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   44C    P0             83W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           65696      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           65697      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           65698      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           65699      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           65700      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           65701      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           65702      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           65703      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 42 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: log
  clamp: [0.8, 1.3]
  schedule: constant
====================================================================================================
step:0/500 val_loss:15.9787 train_time:264ms step_avg:nanms
step:1/500 train_loss:15.9775 train_time:62112ms step_avg:nanms w_mean:1.000 w_std:0.000 w_min:1.000 w_max:1.000
step:2/500 train_loss:9.3675 train_time:63052ms step_avg:nanms
step:3/500 train_loss:8.9261 train_time:63316ms step_avg:nanms
step:4/500 train_loss:8.7460 train_time:63581ms step_avg:nanms
step:5/500 train_loss:8.2629 train_time:63848ms step_avg:nanms
step:6/500 train_loss:7.7564 train_time:64121ms step_avg:nanms
step:7/500 train_loss:7.3455 train_time:64391ms step_avg:nanms
step:8/500 train_loss:7.5402 train_time:64657ms step_avg:nanms
step:9/500 train_loss:7.2727 train_time:64924ms step_avg:nanms
step:10/500 train_loss:7.0942 train_time:65195ms step_avg:nanms
step:11/500 train_loss:7.0471 train_time:265ms step_avg:nanms
step:12/500 train_loss:7.0133 train_time:538ms step_avg:nanms
step:13/500 train_loss:6.8210 train_time:807ms step_avg:268.98ms
step:14/500 train_loss:6.8289 train_time:1077ms step_avg:269.27ms
step:15/500 train_loss:6.7927 train_time:1343ms step_avg:268.67ms
step:16/500 train_loss:6.7182 train_time:1616ms step_avg:269.36ms
step:17/500 train_loss:6.7280 train_time:1883ms step_avg:268.97ms
step:18/500 train_loss:6.7608 train_time:2151ms step_avg:268.91ms
step:19/500 train_loss:6.5843 train_time:2422ms step_avg:269.06ms
step:20/500 train_loss:6.5944 train_time:2693ms step_avg:269.31ms
step:21/500 train_loss:6.2538 train_time:2963ms step_avg:269.36ms
step:22/500 train_loss:6.6466 train_time:3233ms step_avg:269.41ms
step:23/500 train_loss:6.8761 train_time:3499ms step_avg:269.15ms
step:24/500 train_loss:6.5179 train_time:3767ms step_avg:269.09ms
step:25/500 train_loss:6.6313 train_time:4041ms step_avg:269.39ms
step:26/500 train_loss:6.3616 train_time:4313ms step_avg:269.55ms
step:27/500 train_loss:6.2698 train_time:4579ms step_avg:269.37ms
step:28/500 train_loss:6.4501 train_time:4848ms step_avg:269.31ms
step:29/500 train_loss:6.1011 train_time:5119ms step_avg:269.42ms
step:30/500 train_loss:6.3851 train_time:5389ms step_avg:269.44ms
step:31/500 train_loss:6.2250 train_time:5659ms step_avg:269.46ms
step:32/500 train_loss:6.1983 train_time:5928ms step_avg:269.44ms
step:33/500 train_loss:6.0113 train_time:6196ms step_avg:269.40ms
step:34/500 train_loss:6.3487 train_time:6466ms step_avg:269.42ms
step:35/500 train_loss:6.2629 train_time:6737ms step_avg:269.49ms
step:36/500 train_loss:6.4229 train_time:7009ms step_avg:269.58ms
step:37/500 train_loss:6.3389 train_time:7279ms step_avg:269.59ms
step:38/500 train_loss:6.2245 train_time:7550ms step_avg:269.63ms
step:39/500 train_loss:6.1231 train_time:7821ms step_avg:269.70ms
step:40/500 train_loss:6.1763 train_time:8091ms step_avg:269.69ms
step:41/500 train_loss:6.0789 train_time:8360ms step_avg:269.69ms
step:42/500 train_loss:6.1148 train_time:8631ms step_avg:269.71ms
step:43/500 train_loss:5.9886 train_time:8899ms step_avg:269.68ms
step:44/500 train_loss:6.0638 train_time:9168ms step_avg:269.66ms
step:45/500 train_loss:6.0683 train_time:9440ms step_avg:269.71ms
step:46/500 train_loss:6.2374 train_time:9713ms step_avg:269.81ms
step:47/500 train_loss:6.0493 train_time:9983ms step_avg:269.80ms
step:48/500 train_loss:5.8859 train_time:10254ms step_avg:269.85ms
step:49/500 train_loss:6.1402 train_time:10525ms step_avg:269.87ms
step:50/500 train_loss:5.9997 train_time:10797ms step_avg:269.91ms
step:51/500 train_loss:6.1648 train_time:11065ms step_avg:269.88ms w_mean:1.000 w_std:0.104 w_min:0.940 w_max:1.527
step:52/500 train_loss:6.0185 train_time:11337ms step_avg:269.94ms
step:53/500 train_loss:5.8797 train_time:11609ms step_avg:269.99ms
step:54/500 train_loss:5.9864 train_time:11880ms step_avg:269.99ms
step:55/500 train_loss:5.9205 train_time:12152ms step_avg:270.05ms
step:56/500 train_loss:6.2179 train_time:12423ms step_avg:270.07ms
step:57/500 train_loss:5.9087 train_time:12695ms step_avg:270.11ms
step:58/500 train_loss:5.7849 train_time:12966ms step_avg:270.12ms
step:59/500 train_loss:5.9505 train_time:13239ms step_avg:270.19ms
step:60/500 train_loss:5.8862 train_time:13512ms step_avg:270.25ms
step:61/500 train_loss:5.9826 train_time:13782ms step_avg:270.24ms
step:62/500 train_loss:5.7853 train_time:14054ms step_avg:270.27ms
step:63/500 train_loss:5.8769 train_time:14325ms step_avg:270.28ms
step:64/500 train_loss:5.8560 train_time:14595ms step_avg:270.29ms
step:65/500 train_loss:5.7729 train_time:14866ms step_avg:270.29ms
step:66/500 train_loss:5.6838 train_time:15139ms step_avg:270.34ms
step:67/500 train_loss:5.8419 train_time:15414ms step_avg:270.42ms
step:68/500 train_loss:5.7151 train_time:15684ms step_avg:270.41ms
step:69/500 train_loss:5.9550 train_time:15956ms step_avg:270.45ms
step:70/500 train_loss:5.6264 train_time:16229ms step_avg:270.49ms
step:71/500 train_loss:5.6381 train_time:16500ms step_avg:270.50ms
step:72/500 train_loss:5.8471 train_time:16774ms step_avg:270.55ms
step:73/500 train_loss:5.7904 train_time:17047ms step_avg:270.59ms
step:74/500 train_loss:5.6773 train_time:17320ms step_avg:270.63ms
step:75/500 train_loss:5.7982 train_time:17591ms step_avg:270.63ms
step:76/500 train_loss:5.7506 train_time:17861ms step_avg:270.62ms
step:77/500 train_loss:5.7286 train_time:18136ms step_avg:270.68ms
step:78/500 train_loss:5.8022 train_time:18408ms step_avg:270.70ms
step:79/500 train_loss:5.8373 train_time:18679ms step_avg:270.71ms
step:80/500 train_loss:5.6858 train_time:18951ms step_avg:270.72ms
step:81/500 train_loss:5.7872 train_time:19222ms step_avg:270.73ms
step:82/500 train_loss:5.5463 train_time:19493ms step_avg:270.74ms
step:83/500 train_loss:5.7228 train_time:19765ms step_avg:270.75ms
step:84/500 train_loss:5.6804 train_time:20039ms step_avg:270.79ms
step:85/500 train_loss:5.6465 train_time:20312ms step_avg:270.82ms
step:86/500 train_loss:5.5059 train_time:20582ms step_avg:270.81ms
step:87/500 train_loss:5.7217 train_time:20856ms step_avg:270.85ms
step:88/500 train_loss:5.6213 train_time:21127ms step_avg:270.86ms
step:89/500 train_loss:5.6795 train_time:21398ms step_avg:270.86ms
step:90/500 train_loss:5.6658 train_time:21671ms step_avg:270.88ms
step:91/500 train_loss:5.5777 train_time:21941ms step_avg:270.88ms
step:92/500 train_loss:5.5841 train_time:22216ms step_avg:270.92ms
step:93/500 train_loss:5.6772 train_time:22486ms step_avg:270.92ms
step:94/500 train_loss:5.5325 train_time:22758ms step_avg:270.93ms
step:95/500 train_loss:5.5130 train_time:23031ms step_avg:270.96ms
step:96/500 train_loss:5.5387 train_time:23301ms step_avg:270.94ms
step:97/500 train_loss:5.4562 train_time:23575ms step_avg:270.98ms
step:98/500 train_loss:5.5293 train_time:23847ms step_avg:270.99ms
step:99/500 train_loss:5.4539 train_time:24120ms step_avg:271.01ms
step:100/500 train_loss:5.5720 train_time:24391ms step_avg:271.01ms
step:101/500 train_loss:5.5386 train_time:24662ms step_avg:271.01ms w_mean:1.000 w_std:0.118 w_min:0.933 w_max:1.516
step:102/500 train_loss:5.4318 train_time:24936ms step_avg:271.04ms
step:103/500 train_loss:5.5445 train_time:25210ms step_avg:271.07ms
step:104/500 train_loss:5.5018 train_time:25480ms step_avg:271.06ms
step:105/500 train_loss:5.3380 train_time:25754ms step_avg:271.09ms
step:106/500 train_loss:5.4404 train_time:26026ms step_avg:271.10ms
step:107/500 train_loss:5.6389 train_time:26298ms step_avg:271.12ms
step:108/500 train_loss:5.4317 train_time:26572ms step_avg:271.14ms
step:109/500 train_loss:5.1951 train_time:26842ms step_avg:271.13ms
step:110/500 train_loss:5.3905 train_time:27116ms step_avg:271.16ms
step:111/500 train_loss:5.3717 train_time:27386ms step_avg:271.15ms
step:112/500 train_loss:5.3344 train_time:27659ms step_avg:271.16ms
step:113/500 train_loss:5.4446 train_time:27931ms step_avg:271.18ms
step:114/500 train_loss:5.3703 train_time:28203ms step_avg:271.18ms
step:115/500 train_loss:5.2352 train_time:28475ms step_avg:271.19ms
step:116/500 train_loss:5.3929 train_time:28746ms step_avg:271.19ms
step:117/500 train_loss:5.2589 train_time:29021ms step_avg:271.22ms
step:118/500 train_loss:5.2388 train_time:29292ms step_avg:271.22ms
step:119/500 train_loss:5.3670 train_time:29563ms step_avg:271.22ms
step:120/500 train_loss:5.3467 train_time:29835ms step_avg:271.23ms
step:121/500 train_loss:5.2860 train_time:30111ms step_avg:271.27ms
step:122/500 train_loss:5.1717 train_time:30381ms step_avg:271.26ms
step:123/500 train_loss:5.2771 train_time:30656ms step_avg:271.29ms
step:124/500 train_loss:5.1384 train_time:30928ms step_avg:271.30ms
step:125/500 train_loss:5.4413 train_time:31199ms step_avg:271.30ms
step:125/500 val_loss:5.2636 train_time:31201ms step_avg:271.31ms
step:126/500 train_loss:5.2846 train_time:31475ms step_avg:271.34ms
step:127/500 train_loss:5.2662 train_time:31752ms step_avg:271.39ms
step:128/500 train_loss:5.3286 train_time:32028ms step_avg:271.42ms
step:129/500 train_loss:5.1911 train_time:32298ms step_avg:271.41ms
step:130/500 train_loss:5.4540 train_time:32569ms step_avg:271.41ms
step:131/500 train_loss:5.2498 train_time:32846ms step_avg:271.46ms
step:132/500 train_loss:5.2349 train_time:33120ms step_avg:271.48ms
step:133/500 train_loss:5.1877 train_time:33390ms step_avg:271.47ms
step:134/500 train_loss:5.2286 train_time:33663ms step_avg:271.48ms
step:135/500 train_loss:5.1576 train_time:33933ms step_avg:271.47ms
step:136/500 train_loss:5.2245 train_time:34210ms step_avg:271.51ms
step:137/500 train_loss:5.0410 train_time:34483ms step_avg:271.52ms
step:138/500 train_loss:5.1877 train_time:34754ms step_avg:271.51ms
step:139/500 train_loss:5.1516 train_time:35027ms step_avg:271.52ms
step:140/500 train_loss:5.1568 train_time:35302ms step_avg:271.55ms
step:141/500 train_loss:5.2093 train_time:35572ms step_avg:271.55ms
step:142/500 train_loss:5.1121 train_time:35847ms step_avg:271.57ms
step:143/500 train_loss:5.1991 train_time:36118ms step_avg:271.57ms
step:144/500 train_loss:4.9990 train_time:36391ms step_avg:271.57ms
step:145/500 train_loss:5.1570 train_time:36666ms step_avg:271.60ms
step:146/500 train_loss:5.0981 train_time:36939ms step_avg:271.61ms
step:147/500 train_loss:5.0129 train_time:37211ms step_avg:271.61ms
step:148/500 train_loss:5.1295 train_time:37483ms step_avg:271.62ms
step:149/500 train_loss:5.1073 train_time:37754ms step_avg:271.61ms
step:150/500 train_loss:5.1604 train_time:38028ms step_avg:271.63ms
step:151/500 train_loss:5.1782 train_time:38302ms step_avg:271.64ms w_mean:1.000 w_std:0.128 w_min:0.927 w_max:1.506
step:152/500 train_loss:5.0982 train_time:38572ms step_avg:271.64ms
step:153/500 train_loss:5.0769 train_time:38847ms step_avg:271.66ms
step:154/500 train_loss:5.1480 train_time:39120ms step_avg:271.66ms
step:155/500 train_loss:5.0916 train_time:39392ms step_avg:271.67ms
step:156/500 train_loss:5.0664 train_time:39666ms step_avg:271.69ms
step:157/500 train_loss:5.0808 train_time:39942ms step_avg:271.71ms
step:158/500 train_loss:5.2063 train_time:40213ms step_avg:271.71ms
step:159/500 train_loss:4.9984 train_time:40485ms step_avg:271.71ms
step:160/500 train_loss:5.0610 train_time:40759ms step_avg:271.73ms
step:161/500 train_loss:4.9111 train_time:41034ms step_avg:271.75ms
step:162/500 train_loss:5.0671 train_time:41307ms step_avg:271.76ms
step:163/500 train_loss:5.0982 train_time:41576ms step_avg:271.74ms
step:164/500 train_loss:5.0869 train_time:41849ms step_avg:271.75ms
step:165/500 train_loss:4.9144 train_time:42125ms step_avg:271.78ms
step:166/500 train_loss:5.0303 train_time:42400ms step_avg:271.80ms
step:167/500 train_loss:5.1810 train_time:42670ms step_avg:271.78ms
step:168/500 train_loss:4.9637 train_time:42944ms step_avg:271.80ms
step:169/500 train_loss:5.0422 train_time:43221ms step_avg:271.83ms
step:170/500 train_loss:4.9166 train_time:43492ms step_avg:271.82ms
step:171/500 train_loss:4.8581 train_time:43769ms step_avg:271.86ms
step:172/500 train_loss:4.9642 train_time:44039ms step_avg:271.84ms
step:173/500 train_loss:4.9384 train_time:44312ms step_avg:271.85ms
step:174/500 train_loss:4.9933 train_time:44583ms step_avg:271.85ms
step:175/500 train_loss:5.1262 train_time:44854ms step_avg:271.84ms
step:176/500 train_loss:5.0194 train_time:45128ms step_avg:271.85ms
step:177/500 train_loss:4.8525 train_time:45404ms step_avg:271.88ms
step:178/500 train_loss:4.8348 train_time:45674ms step_avg:271.87ms
step:179/500 train_loss:4.8813 train_time:45948ms step_avg:271.88ms
step:180/500 train_loss:4.9165 train_time:46220ms step_avg:271.88ms
step:181/500 train_loss:4.9076 train_time:46492ms step_avg:271.88ms
step:182/500 train_loss:5.0182 train_time:46767ms step_avg:271.90ms
step:183/500 train_loss:4.9091 train_time:47041ms step_avg:271.92ms
step:184/500 train_loss:4.8367 train_time:47312ms step_avg:271.91ms
step:185/500 train_loss:4.8730 train_time:47585ms step_avg:271.91ms
step:186/500 train_loss:4.9863 train_time:47858ms step_avg:271.92ms
step:187/500 train_loss:4.8777 train_time:48132ms step_avg:271.93ms
step:188/500 train_loss:5.1149 train_time:48407ms step_avg:271.95ms
step:189/500 train_loss:4.9093 train_time:48939ms step_avg:273.40ms
step:190/500 train_loss:4.8223 train_time:49569ms step_avg:275.38ms
step:191/500 train_loss:4.9833 train_time:49836ms step_avg:275.34ms
step:192/500 train_loss:4.8237 train_time:50107ms step_avg:275.31ms
step:193/500 train_loss:4.7475 train_time:50374ms step_avg:275.27ms
step:194/500 train_loss:4.9399 train_time:50650ms step_avg:275.27ms
step:195/500 train_loss:4.8898 train_time:50922ms step_avg:275.26ms
step:196/500 train_loss:5.0628 train_time:51190ms step_avg:275.22ms
step:197/500 train_loss:4.9644 train_time:51463ms step_avg:275.20ms
step:198/500 train_loss:4.8004 train_time:51734ms step_avg:275.18ms
step:199/500 train_loss:4.8490 train_time:52008ms step_avg:275.17ms
step:200/500 train_loss:4.7345 train_time:52276ms step_avg:275.14ms
step:201/500 train_loss:4.8270 train_time:52551ms step_avg:275.13ms w_mean:1.000 w_std:0.137 w_min:0.922 w_max:1.499
step:202/500 train_loss:4.7388 train_time:52825ms step_avg:275.13ms
step:203/500 train_loss:4.9710 train_time:53095ms step_avg:275.10ms
step:204/500 train_loss:4.8784 train_time:53366ms step_avg:275.08ms
step:205/500 train_loss:4.8476 train_time:53641ms step_avg:275.08ms
step:206/500 train_loss:5.0021 train_time:53912ms step_avg:275.06ms
step:207/500 train_loss:4.6747 train_time:54184ms step_avg:275.05ms
step:208/500 train_loss:4.8261 train_time:54455ms step_avg:275.03ms
step:209/500 train_loss:4.7733 train_time:54729ms step_avg:275.02ms
step:210/500 train_loss:4.9412 train_time:55002ms step_avg:275.01ms
step:211/500 train_loss:4.8654 train_time:55272ms step_avg:274.98ms
step:212/500 train_loss:4.7545 train_time:55547ms step_avg:274.98ms
step:213/500 train_loss:4.8821 train_time:55821ms step_avg:274.98ms
step:214/500 train_loss:4.7264 train_time:56091ms step_avg:274.96ms
step:215/500 train_loss:4.8090 train_time:56365ms step_avg:274.95ms
step:216/500 train_loss:4.6735 train_time:56640ms step_avg:274.95ms
step:217/500 train_loss:4.7853 train_time:56912ms step_avg:274.94ms
step:218/500 train_loss:4.7783 train_time:57183ms step_avg:274.92ms
step:219/500 train_loss:4.7439 train_time:57455ms step_avg:274.90ms
step:220/500 train_loss:4.7615 train_time:57729ms step_avg:274.90ms
step:221/500 train_loss:4.7850 train_time:58003ms step_avg:274.90ms
step:222/500 train_loss:4.8247 train_time:58272ms step_avg:274.87ms
step:223/500 train_loss:4.7619 train_time:58547ms step_avg:274.87ms
step:224/500 train_loss:4.7601 train_time:58820ms step_avg:274.86ms
step:225/500 train_loss:4.8893 train_time:59092ms step_avg:274.85ms
step:226/500 train_loss:4.6309 train_time:59365ms step_avg:274.84ms
step:227/500 train_loss:4.6710 train_time:59638ms step_avg:274.83ms
step:228/500 train_loss:4.6537 train_time:59910ms step_avg:274.82ms
step:229/500 train_loss:4.8176 train_time:60184ms step_avg:274.81ms
step:230/500 train_loss:4.6483 train_time:60455ms step_avg:274.80ms
step:231/500 train_loss:4.7956 train_time:60730ms step_avg:274.80ms
step:232/500 train_loss:4.6617 train_time:61005ms step_avg:274.80ms
step:233/500 train_loss:4.6272 train_time:61276ms step_avg:274.78ms
step:234/500 train_loss:4.8341 train_time:61549ms step_avg:274.77ms
step:235/500 train_loss:4.6629 train_time:61823ms step_avg:274.77ms
step:236/500 train_loss:4.6188 train_time:62094ms step_avg:274.75ms
step:237/500 train_loss:4.8527 train_time:62368ms step_avg:274.75ms
step:238/500 train_loss:4.7372 train_time:62641ms step_avg:274.74ms
step:239/500 train_loss:4.6548 train_time:62911ms step_avg:274.72ms
step:240/500 train_loss:4.7934 train_time:63186ms step_avg:274.72ms
step:241/500 train_loss:4.7744 train_time:63460ms step_avg:274.72ms
step:242/500 train_loss:4.6842 train_time:63732ms step_avg:274.71ms
step:243/500 train_loss:4.8293 train_time:64007ms step_avg:274.71ms
step:244/500 train_loss:4.6730 train_time:64279ms step_avg:274.70ms
step:245/500 train_loss:4.6864 train_time:64551ms step_avg:274.69ms
step:246/500 train_loss:4.7504 train_time:64826ms step_avg:274.69ms
step:247/500 train_loss:4.7119 train_time:65102ms step_avg:274.69ms
step:248/500 train_loss:4.6684 train_time:65372ms step_avg:274.67ms
step:249/500 train_loss:4.8462 train_time:65647ms step_avg:274.67ms
step:250/500 train_loss:4.5728 train_time:65919ms step_avg:274.66ms
step:250/500 val_loss:4.6800 train_time:65920ms step_avg:274.67ms
step:251/500 train_loss:4.6159 train_time:66194ms step_avg:274.66ms w_mean:1.000 w_std:0.143 w_min:0.918 w_max:1.492
step:252/500 train_loss:4.7461 train_time:66471ms step_avg:274.67ms
step:253/500 train_loss:4.7347 train_time:66748ms step_avg:274.68ms
step:254/500 train_loss:4.6150 train_time:67018ms step_avg:274.67ms
step:255/500 train_loss:4.6274 train_time:67287ms step_avg:274.64ms
step:256/500 train_loss:4.7693 train_time:67562ms step_avg:274.64ms
step:257/500 train_loss:4.7102 train_time:67839ms step_avg:274.65ms
step:258/500 train_loss:4.6907 train_time:68109ms step_avg:274.63ms
step:259/500 train_loss:4.6169 train_time:68381ms step_avg:274.62ms
step:260/500 train_loss:4.6324 train_time:68655ms step_avg:274.62ms
step:261/500 train_loss:4.7028 train_time:68929ms step_avg:274.62ms
step:262/500 train_loss:4.7076 train_time:69203ms step_avg:274.61ms
step:263/500 train_loss:4.6142 train_time:69473ms step_avg:274.60ms
step:264/500 train_loss:4.5642 train_time:69746ms step_avg:274.59ms
step:265/500 train_loss:4.6121 train_time:70023ms step_avg:274.60ms
step:266/500 train_loss:4.4769 train_time:70297ms step_avg:274.60ms
step:267/500 train_loss:4.5279 train_time:70568ms step_avg:274.58ms
step:268/500 train_loss:4.5744 train_time:70844ms step_avg:274.59ms
step:269/500 train_loss:4.5310 train_time:71116ms step_avg:274.58ms
step:270/500 train_loss:4.5057 train_time:71388ms step_avg:274.57ms
step:271/500 train_loss:4.7156 train_time:71662ms step_avg:274.57ms
step:272/500 train_loss:4.6556 train_time:71935ms step_avg:274.56ms
step:273/500 train_loss:4.5129 train_time:72208ms step_avg:274.56ms
step:274/500 train_loss:4.5626 train_time:72480ms step_avg:274.55ms
step:275/500 train_loss:4.6765 train_time:72752ms step_avg:274.53ms
step:276/500 train_loss:4.6910 train_time:73026ms step_avg:274.53ms
step:277/500 train_loss:4.8943 train_time:73301ms step_avg:274.53ms
step:278/500 train_loss:4.6371 train_time:73583ms step_avg:274.57ms
step:279/500 train_loss:4.7682 train_time:73847ms step_avg:274.53ms
step:280/500 train_loss:4.6078 train_time:74121ms step_avg:274.52ms
step:281/500 train_loss:4.6883 train_time:74397ms step_avg:274.53ms
step:282/500 train_loss:4.5702 train_time:74668ms step_avg:274.51ms
step:283/500 train_loss:4.6955 train_time:74942ms step_avg:274.51ms
step:284/500 train_loss:4.5105 train_time:75215ms step_avg:274.51ms
step:285/500 train_loss:4.6754 train_time:75487ms step_avg:274.50ms
step:286/500 train_loss:4.6685 train_time:75762ms step_avg:274.50ms
step:287/500 train_loss:4.6965 train_time:76036ms step_avg:274.50ms
step:288/500 train_loss:4.5640 train_time:76309ms step_avg:274.49ms
step:289/500 train_loss:4.6268 train_time:76581ms step_avg:274.48ms
step:290/500 train_loss:4.4845 train_time:76852ms step_avg:274.47ms
step:291/500 train_loss:4.4807 train_time:77128ms step_avg:274.48ms
step:292/500 train_loss:4.6063 train_time:77403ms step_avg:274.48ms
step:293/500 train_loss:4.4984 train_time:77674ms step_avg:274.47ms
step:294/500 train_loss:4.5425 train_time:77946ms step_avg:274.46ms
step:295/500 train_loss:4.5687 train_time:78221ms step_avg:274.46ms
step:296/500 train_loss:4.4347 train_time:78497ms step_avg:274.46ms
step:297/500 train_loss:4.4293 train_time:78767ms step_avg:274.45ms
step:298/500 train_loss:4.4554 train_time:79042ms step_avg:274.45ms
step:299/500 train_loss:4.5611 train_time:79316ms step_avg:274.45ms
step:300/500 train_loss:4.4435 train_time:79587ms step_avg:274.44ms
step:301/500 train_loss:4.6261 train_time:79863ms step_avg:274.44ms w_mean:1.000 w_std:0.143 w_min:0.919 w_max:1.493
step:302/500 train_loss:4.5920 train_time:80137ms step_avg:274.44ms
step:303/500 train_loss:4.5206 train_time:80408ms step_avg:274.43ms
step:304/500 train_loss:4.5799 train_time:80680ms step_avg:274.42ms
step:305/500 train_loss:4.5672 train_time:80951ms step_avg:274.41ms
step:306/500 train_loss:5.0396 train_time:81225ms step_avg:274.41ms
step:307/500 train_loss:4.5243 train_time:81501ms step_avg:274.41ms
step:308/500 train_loss:4.4300 train_time:81772ms step_avg:274.40ms
step:309/500 train_loss:4.6116 train_time:82045ms step_avg:274.40ms
step:310/500 train_loss:4.4192 train_time:82319ms step_avg:274.40ms
step:311/500 train_loss:4.6455 train_time:82590ms step_avg:274.38ms
step:312/500 train_loss:4.5623 train_time:82862ms step_avg:274.38ms
step:313/500 train_loss:4.4737 train_time:83136ms step_avg:274.38ms
step:314/500 train_loss:4.5989 train_time:83408ms step_avg:274.37ms
step:315/500 train_loss:4.7338 train_time:83681ms step_avg:274.36ms
step:316/500 train_loss:4.5643 train_time:83952ms step_avg:274.35ms
step:317/500 train_loss:4.4545 train_time:84227ms step_avg:274.36ms
step:318/500 train_loss:4.4685 train_time:84504ms step_avg:274.36ms
step:319/500 train_loss:4.4852 train_time:84776ms step_avg:274.35ms
step:320/500 train_loss:4.4380 train_time:85046ms step_avg:274.34ms
step:321/500 train_loss:4.5230 train_time:85322ms step_avg:274.35ms
step:322/500 train_loss:4.5375 train_time:85598ms step_avg:274.35ms
step:323/500 train_loss:4.5032 train_time:85868ms step_avg:274.34ms
step:324/500 train_loss:4.5720 train_time:86144ms step_avg:274.34ms
step:325/500 train_loss:4.5728 train_time:86418ms step_avg:274.34ms
step:326/500 train_loss:4.6277 train_time:86689ms step_avg:274.33ms
step:327/500 train_loss:4.4836 train_time:86963ms step_avg:274.33ms
step:328/500 train_loss:4.9263 train_time:87237ms step_avg:274.33ms
step:329/500 train_loss:4.6408 train_time:87509ms step_avg:274.32ms
step:330/500 train_loss:4.4354 train_time:87782ms step_avg:274.32ms
step:331/500 train_loss:4.3968 train_time:88055ms step_avg:274.31ms
step:332/500 train_loss:4.5476 train_time:88331ms step_avg:274.32ms
step:333/500 train_loss:4.4664 train_time:88604ms step_avg:274.32ms
step:334/500 train_loss:4.4540 train_time:88875ms step_avg:274.30ms
step:335/500 train_loss:4.4164 train_time:89146ms step_avg:274.30ms
step:336/500 train_loss:4.6074 train_time:89425ms step_avg:274.31ms
step:337/500 train_loss:4.5441 train_time:89699ms step_avg:274.31ms
step:338/500 train_loss:5.0809 train_time:89968ms step_avg:274.29ms
step:339/500 train_loss:4.5142 train_time:90242ms step_avg:274.29ms
step:340/500 train_loss:4.4821 train_time:90519ms step_avg:274.30ms
step:341/500 train_loss:4.4719 train_time:90790ms step_avg:274.29ms
step:342/500 train_loss:4.4103 train_time:91063ms step_avg:274.29ms
step:343/500 train_loss:4.3861 train_time:91336ms step_avg:274.28ms
step:344/500 train_loss:4.4507 train_time:91609ms step_avg:274.28ms
step:345/500 train_loss:4.5385 train_time:91880ms step_avg:274.27ms
step:346/500 train_loss:4.4340 train_time:92151ms step_avg:274.26ms
step:347/500 train_loss:4.3789 train_time:92425ms step_avg:274.26ms
step:348/500 train_loss:4.4398 train_time:92699ms step_avg:274.26ms
step:349/500 train_loss:4.4329 train_time:92970ms step_avg:274.25ms
step:350/500 train_loss:4.3593 train_time:93243ms step_avg:274.24ms
step:351/500 train_loss:4.0451 train_time:93516ms step_avg:274.24ms w_mean:1.000 w_std:0.156 w_min:0.914 w_max:1.485
step:352/500 train_loss:4.3387 train_time:93788ms step_avg:274.23ms
step:353/500 train_loss:4.6758 train_time:94063ms step_avg:274.24ms
step:354/500 train_loss:4.2218 train_time:94337ms step_avg:274.24ms
step:355/500 train_loss:4.4649 train_time:94609ms step_avg:274.23ms
step:356/500 train_loss:4.3791 train_time:94880ms step_avg:274.22ms
step:357/500 train_loss:4.4654 train_time:95151ms step_avg:274.21ms
step:358/500 train_loss:4.4892 train_time:95425ms step_avg:274.21ms
step:359/500 train_loss:4.3841 train_time:95702ms step_avg:274.22ms
step:360/500 train_loss:4.7032 train_time:95973ms step_avg:274.21ms
step:361/500 train_loss:4.1242 train_time:96246ms step_avg:274.21ms
step:362/500 train_loss:4.5986 train_time:96520ms step_avg:274.20ms
step:363/500 train_loss:4.4928 train_time:96794ms step_avg:274.20ms
step:364/500 train_loss:4.3795 train_time:97065ms step_avg:274.19ms
step:365/500 train_loss:4.3129 train_time:97341ms step_avg:274.20ms
step:366/500 train_loss:4.4702 train_time:97615ms step_avg:274.20ms
step:367/500 train_loss:4.3986 train_time:97886ms step_avg:274.19ms
step:368/500 train_loss:4.3888 train_time:98160ms step_avg:274.19ms
step:369/500 train_loss:4.3945 train_time:98431ms step_avg:274.18ms
step:370/500 train_loss:4.2885 train_time:98705ms step_avg:274.18ms
step:371/500 train_loss:4.4286 train_time:98976ms step_avg:274.17ms
step:372/500 train_loss:4.3742 train_time:99248ms step_avg:274.17ms
step:373/500 train_loss:4.2518 train_time:99524ms step_avg:274.17ms
step:374/500 train_loss:4.4382 train_time:99798ms step_avg:274.17ms
step:375/500 train_loss:4.3692 train_time:100068ms step_avg:274.16ms
step:375/500 val_loss:4.3865 train_time:100070ms step_avg:274.16ms
step:376/500 train_loss:4.3711 train_time:100345ms step_avg:274.17ms
step:377/500 train_loss:4.4292 train_time:100623ms step_avg:274.18ms
step:378/500 train_loss:4.3181 train_time:101151ms step_avg:274.87ms
step:379/500 train_loss:4.3703 train_time:101420ms step_avg:274.85ms
step:380/500 train_loss:4.4444 train_time:101956ms step_avg:275.56ms
step:381/500 train_loss:4.4748 train_time:102224ms step_avg:275.54ms
step:382/500 train_loss:4.4226 train_time:102492ms step_avg:275.52ms
step:383/500 train_loss:4.4001 train_time:102761ms step_avg:275.50ms
step:384/500 train_loss:4.3068 train_time:103042ms step_avg:275.51ms
step:385/500 train_loss:4.4073 train_time:103315ms step_avg:275.51ms
step:386/500 train_loss:4.3182 train_time:103585ms step_avg:275.49ms
step:387/500 train_loss:4.4472 train_time:103855ms step_avg:275.48ms
step:388/500 train_loss:4.6491 train_time:104133ms step_avg:275.48ms
step:389/500 train_loss:4.3425 train_time:104406ms step_avg:275.48ms
step:390/500 train_loss:4.2965 train_time:104676ms step_avg:275.46ms
step:391/500 train_loss:4.4258 train_time:104947ms step_avg:275.45ms
step:392/500 train_loss:4.3487 train_time:105220ms step_avg:275.44ms
step:393/500 train_loss:4.4544 train_time:105494ms step_avg:275.44ms
step:394/500 train_loss:4.2769 train_time:105764ms step_avg:275.43ms
step:395/500 train_loss:4.4075 train_time:106036ms step_avg:275.42ms
step:396/500 train_loss:4.2067 train_time:106310ms step_avg:275.42ms
step:397/500 train_loss:4.3586 train_time:106582ms step_avg:275.40ms
step:398/500 train_loss:4.4536 train_time:106853ms step_avg:275.39ms
step:399/500 train_loss:4.4043 train_time:107124ms step_avg:275.38ms
step:400/500 train_loss:4.3263 train_time:107397ms step_avg:275.38ms
step:401/500 train_loss:4.3906 train_time:107670ms step_avg:275.37ms w_mean:1.000 w_std:0.150 w_min:0.914 w_max:1.486
step:402/500 train_loss:4.4231 train_time:107939ms step_avg:275.36ms
step:403/500 train_loss:4.3960 train_time:108214ms step_avg:275.35ms
step:404/500 train_loss:4.4855 train_time:108488ms step_avg:275.35ms
step:405/500 train_loss:4.2703 train_time:108758ms step_avg:275.34ms
step:406/500 train_loss:4.3140 train_time:109033ms step_avg:275.34ms
step:407/500 train_loss:4.5881 train_time:109306ms step_avg:275.33ms
step:408/500 train_loss:4.3564 train_time:109578ms step_avg:275.32ms
step:409/500 train_loss:4.3449 train_time:109852ms step_avg:275.32ms
step:410/500 train_loss:4.3958 train_time:110126ms step_avg:275.32ms
step:411/500 train_loss:4.2859 train_time:110398ms step_avg:275.31ms
step:412/500 train_loss:4.2989 train_time:110669ms step_avg:275.30ms
step:413/500 train_loss:4.7188 train_time:110939ms step_avg:275.28ms
step:414/500 train_loss:4.1714 train_time:111215ms step_avg:275.28ms
step:415/500 train_loss:4.5266 train_time:111491ms step_avg:275.29ms
step:416/500 train_loss:4.3057 train_time:111763ms step_avg:275.28ms
step:417/500 train_loss:4.2972 train_time:112037ms step_avg:275.28ms
step:418/500 train_loss:4.4806 train_time:112310ms step_avg:275.27ms
step:419/500 train_loss:4.2158 train_time:112580ms step_avg:275.26ms
step:420/500 train_loss:4.3163 train_time:112854ms step_avg:275.25ms
step:421/500 train_loss:4.2911 train_time:113127ms step_avg:275.25ms
step:422/500 train_loss:4.1828 train_time:113399ms step_avg:275.24ms
step:423/500 train_loss:4.2858 train_time:113671ms step_avg:275.23ms
step:424/500 train_loss:4.3951 train_time:113941ms step_avg:275.22ms
step:425/500 train_loss:4.2023 train_time:114216ms step_avg:275.22ms
step:426/500 train_loss:4.3681 train_time:114491ms step_avg:275.22ms
step:427/500 train_loss:4.2461 train_time:114762ms step_avg:275.21ms
step:428/500 train_loss:4.4202 train_time:115037ms step_avg:275.21ms
step:429/500 train_loss:4.3721 train_time:115310ms step_avg:275.20ms
step:430/500 train_loss:4.2805 train_time:115579ms step_avg:275.19ms
step:431/500 train_loss:4.2611 train_time:115853ms step_avg:275.19ms
step:432/500 train_loss:4.2113 train_time:116127ms step_avg:275.18ms
step:433/500 train_loss:4.2961 train_time:116402ms step_avg:275.18ms
step:434/500 train_loss:4.3699 train_time:116675ms step_avg:275.18ms
step:435/500 train_loss:4.3027 train_time:116947ms step_avg:275.17ms
step:436/500 train_loss:4.3452 train_time:117220ms step_avg:275.16ms
step:437/500 train_loss:4.3570 train_time:117495ms step_avg:275.16ms
step:438/500 train_loss:4.2470 train_time:117767ms step_avg:275.16ms
step:439/500 train_loss:4.2604 train_time:118039ms step_avg:275.15ms
step:440/500 train_loss:4.2195 train_time:118314ms step_avg:275.15ms
step:441/500 train_loss:4.4129 train_time:118588ms step_avg:275.15ms
step:442/500 train_loss:4.3176 train_time:118859ms step_avg:275.14ms
step:443/500 train_loss:4.2932 train_time:119133ms step_avg:275.13ms
step:444/500 train_loss:4.1885 train_time:119407ms step_avg:275.13ms
step:445/500 train_loss:4.4312 train_time:119678ms step_avg:275.12ms
step:446/500 train_loss:4.3625 train_time:119953ms step_avg:275.12ms
step:447/500 train_loss:4.3582 train_time:120228ms step_avg:275.12ms
step:448/500 train_loss:4.2901 train_time:120500ms step_avg:275.11ms
step:449/500 train_loss:4.3635 train_time:120772ms step_avg:275.11ms
step:450/500 train_loss:4.2001 train_time:121043ms step_avg:275.10ms
step:451/500 train_loss:4.2476 train_time:121320ms step_avg:275.10ms w_mean:1.000 w_std:0.152 w_min:0.913 w_max:1.484
step:452/500 train_loss:4.1326 train_time:121595ms step_avg:275.10ms
step:453/500 train_loss:4.2284 train_time:121866ms step_avg:275.09ms
step:454/500 train_loss:4.2098 train_time:122136ms step_avg:275.08ms
step:455/500 train_loss:4.1880 train_time:122414ms step_avg:275.09ms
step:456/500 train_loss:4.3884 train_time:122688ms step_avg:275.09ms
step:457/500 train_loss:4.2435 train_time:122958ms step_avg:275.07ms
step:458/500 train_loss:4.3352 train_time:123232ms step_avg:275.07ms
step:459/500 train_loss:4.3702 train_time:123506ms step_avg:275.07ms
step:460/500 train_loss:4.1702 train_time:123778ms step_avg:275.06ms
step:461/500 train_loss:4.3411 train_time:124054ms step_avg:275.06ms
step:462/500 train_loss:4.2446 train_time:124326ms step_avg:275.06ms
step:463/500 train_loss:4.2257 train_time:124601ms step_avg:275.06ms
step:464/500 train_loss:4.3182 train_time:124873ms step_avg:275.05ms
step:465/500 train_loss:4.2488 train_time:125147ms step_avg:275.05ms
step:466/500 train_loss:4.2531 train_time:125421ms step_avg:275.05ms
step:467/500 train_loss:4.3767 train_time:125695ms step_avg:275.04ms
step:468/500 train_loss:4.3798 train_time:125966ms step_avg:275.03ms
step:469/500 train_loss:4.3435 train_time:126236ms step_avg:275.02ms
step:470/500 train_loss:4.2535 train_time:126513ms step_avg:275.03ms
step:471/500 train_loss:4.3448 train_time:126787ms step_avg:275.03ms
step:472/500 train_loss:4.3906 train_time:127058ms step_avg:275.02ms
step:473/500 train_loss:4.2907 train_time:127332ms step_avg:275.01ms
step:474/500 train_loss:4.2652 train_time:127607ms step_avg:275.01ms
step:475/500 train_loss:4.1422 train_time:127878ms step_avg:275.01ms
step:476/500 train_loss:4.5683 train_time:128153ms step_avg:275.01ms
step:477/500 train_loss:4.3187 train_time:128431ms step_avg:275.01ms
step:478/500 train_loss:4.1326 train_time:128702ms step_avg:275.00ms
step:479/500 train_loss:4.3298 train_time:128975ms step_avg:275.00ms
step:480/500 train_loss:4.3105 train_time:129249ms step_avg:275.00ms
step:481/500 train_loss:4.4330 train_time:129521ms step_avg:274.99ms
step:482/500 train_loss:4.2670 train_time:129795ms step_avg:274.99ms
step:483/500 train_loss:4.0833 train_time:130066ms step_avg:274.98ms
step:484/500 train_loss:4.3584 train_time:130337ms step_avg:274.97ms
step:485/500 train_loss:4.2089 train_time:130613ms step_avg:274.98ms
step:486/500 train_loss:4.2333 train_time:130889ms step_avg:274.98ms
step:487/500 train_loss:4.1847 train_time:131159ms step_avg:274.97ms
step:488/500 train_loss:4.2014 train_time:131434ms step_avg:274.97ms
step:489/500 train_loss:4.4151 train_time:131707ms step_avg:274.96ms
step:490/500 train_loss:4.2713 train_time:131978ms step_avg:274.95ms
step:491/500 train_loss:4.1709 train_time:132254ms step_avg:274.96ms
step:492/500 train_loss:4.1757 train_time:132528ms step_avg:274.95ms
step:493/500 train_loss:4.2888 train_time:132800ms step_avg:274.95ms
step:494/500 train_loss:4.1326 train_time:133071ms step_avg:274.94ms
step:495/500 train_loss:4.2836 train_time:133342ms step_avg:274.93ms
step:496/500 train_loss:4.2013 train_time:133617ms step_avg:274.93ms
step:497/500 train_loss:4.1496 train_time:133891ms step_avg:274.93ms
step:498/500 train_loss:4.2806 train_time:134163ms step_avg:274.92ms
step:499/500 train_loss:4.3751 train_time:134436ms step_avg:274.92ms
step:500/500 train_loss:4.4219 train_time:134711ms step_avg:274.92ms
step:500/500 val_loss:4.2740 train_time:134712ms step_avg:274.92ms
