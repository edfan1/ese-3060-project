====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # also save the seed for reproducibility
        if args.seed is not None:
            log['seed'] = args.seed
        # save token weighting config
        if tw_config.enabled:
            log['tw_config'] = {
                'function': tw_config.function,
                'clamp_min': tw_config.clamp_min,
                'clamp_max': tw_config.clamp_max,
                'schedule': tw_config.schedule,
            }
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 21:49:51 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   46C    P0             83W /  310W |    2363MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     33%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   50C    P0             89W /  310W |    2363MiB /  81559MiB |      5%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   46C    P0             86W /  310W |    2363MiB /  81559MiB |     19%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   44C    P0             82W /  310W |    2363MiB /  81559MiB |     32%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   49C    P0             86W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   52C    P0             88W /  310W |    2363MiB /  81559MiB |     14%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           71438      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           71439      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           71440      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           71441      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           71442      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           71443      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           71444      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           71445      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 123 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: focal
  clamp: [0.5, 2.0]
  schedule: constant
  focal_gamma: 2.0
====================================================================================================
step:0/500 val_loss:16.0073 train_time:271ms step_avg:nanms
step:1/500 train_loss:15.9981 train_time:64594ms step_avg:nanms w_mean:1.000 w_std:0.081 w_min:0.666 w_max:1.332
step:2/500 train_loss:9.3713 train_time:65859ms step_avg:nanms
step:3/500 train_loss:8.9614 train_time:66143ms step_avg:nanms
step:4/500 train_loss:8.6600 train_time:66426ms step_avg:nanms
step:5/500 train_loss:8.1069 train_time:66708ms step_avg:nanms
step:6/500 train_loss:7.7466 train_time:66990ms step_avg:nanms
step:7/500 train_loss:7.3102 train_time:67273ms step_avg:nanms
step:8/500 train_loss:7.4760 train_time:67560ms step_avg:nanms
step:9/500 train_loss:7.2626 train_time:67844ms step_avg:nanms
step:10/500 train_loss:7.0970 train_time:68127ms step_avg:nanms
step:11/500 train_loss:7.0636 train_time:282ms step_avg:nanms
step:12/500 train_loss:6.9985 train_time:565ms step_avg:nanms
step:13/500 train_loss:6.8154 train_time:848ms step_avg:282.52ms
step:14/500 train_loss:6.8292 train_time:1131ms step_avg:282.78ms
step:15/500 train_loss:6.7801 train_time:1416ms step_avg:283.26ms
step:16/500 train_loss:6.7045 train_time:1701ms step_avg:283.53ms
step:17/500 train_loss:6.7119 train_time:1984ms step_avg:283.36ms
step:18/500 train_loss:6.7390 train_time:2267ms step_avg:283.40ms
step:19/500 train_loss:6.5628 train_time:2552ms step_avg:283.58ms
step:20/500 train_loss:6.5735 train_time:2837ms step_avg:283.71ms
step:21/500 train_loss:6.2442 train_time:3119ms step_avg:283.56ms
step:22/500 train_loss:6.6305 train_time:3403ms step_avg:283.57ms
step:23/500 train_loss:6.8736 train_time:3686ms step_avg:283.54ms
step:24/500 train_loss:6.5075 train_time:3973ms step_avg:283.81ms
step:25/500 train_loss:6.6256 train_time:4256ms step_avg:283.76ms
step:26/500 train_loss:6.3502 train_time:4541ms step_avg:283.79ms
step:27/500 train_loss:6.2639 train_time:4824ms step_avg:283.74ms
step:28/500 train_loss:6.4352 train_time:5106ms step_avg:283.67ms
step:29/500 train_loss:6.0972 train_time:5389ms step_avg:283.65ms
step:30/500 train_loss:6.3824 train_time:5675ms step_avg:283.74ms
step:31/500 train_loss:6.2318 train_time:5961ms step_avg:283.84ms
step:32/500 train_loss:6.1879 train_time:6244ms step_avg:283.82ms
step:33/500 train_loss:6.0043 train_time:6528ms step_avg:283.82ms
step:34/500 train_loss:6.3406 train_time:6813ms step_avg:283.86ms
step:35/500 train_loss:6.2511 train_time:7100ms step_avg:283.99ms
step:36/500 train_loss:6.4075 train_time:7383ms step_avg:283.95ms
step:37/500 train_loss:6.3300 train_time:7667ms step_avg:283.95ms
step:38/500 train_loss:6.2193 train_time:7951ms step_avg:283.97ms
step:39/500 train_loss:6.1130 train_time:8237ms step_avg:284.02ms
step:40/500 train_loss:6.1675 train_time:8521ms step_avg:284.04ms
step:41/500 train_loss:6.0718 train_time:8806ms step_avg:284.08ms
step:42/500 train_loss:6.1132 train_time:9090ms step_avg:284.05ms
step:43/500 train_loss:5.9804 train_time:9375ms step_avg:284.09ms
step:44/500 train_loss:6.0646 train_time:9660ms step_avg:284.11ms
step:45/500 train_loss:6.0509 train_time:9943ms step_avg:284.09ms
step:46/500 train_loss:6.2365 train_time:10229ms step_avg:284.13ms
step:47/500 train_loss:6.0365 train_time:10514ms step_avg:284.17ms
step:48/500 train_loss:5.8762 train_time:10799ms step_avg:284.20ms
step:49/500 train_loss:6.1268 train_time:11083ms step_avg:284.18ms
step:50/500 train_loss:5.9935 train_time:11367ms step_avg:284.18ms
step:51/500 train_loss:6.1505 train_time:11652ms step_avg:284.19ms w_mean:1.000 w_std:0.026 w_min:0.997 w_max:1.995
step:52/500 train_loss:6.0105 train_time:11938ms step_avg:284.24ms
step:53/500 train_loss:5.8582 train_time:12222ms step_avg:284.23ms
step:54/500 train_loss:5.9913 train_time:12506ms step_avg:284.24ms
step:55/500 train_loss:5.9067 train_time:12791ms step_avg:284.24ms
step:56/500 train_loss:6.2132 train_time:13076ms step_avg:284.26ms
step:57/500 train_loss:5.8981 train_time:13362ms step_avg:284.31ms
step:58/500 train_loss:5.7798 train_time:13646ms step_avg:284.29ms
step:59/500 train_loss:5.9357 train_time:13930ms step_avg:284.29ms
step:60/500 train_loss:5.8791 train_time:14216ms step_avg:284.32ms
step:61/500 train_loss:5.9642 train_time:14501ms step_avg:284.34ms
step:62/500 train_loss:5.7685 train_time:14786ms step_avg:284.35ms
step:63/500 train_loss:5.8674 train_time:15072ms step_avg:284.38ms
step:64/500 train_loss:5.8333 train_time:15358ms step_avg:284.41ms
step:65/500 train_loss:5.7427 train_time:15643ms step_avg:284.41ms
step:66/500 train_loss:5.6635 train_time:15927ms step_avg:284.41ms
step:67/500 train_loss:5.8354 train_time:16212ms step_avg:284.43ms
step:68/500 train_loss:5.6999 train_time:16501ms step_avg:284.50ms
step:69/500 train_loss:5.9423 train_time:16785ms step_avg:284.49ms
step:70/500 train_loss:5.6115 train_time:17070ms step_avg:284.51ms
step:71/500 train_loss:5.6389 train_time:17357ms step_avg:284.55ms
step:72/500 train_loss:5.8471 train_time:17641ms step_avg:284.54ms
step:73/500 train_loss:5.7836 train_time:17925ms step_avg:284.53ms
step:74/500 train_loss:5.6722 train_time:18210ms step_avg:284.54ms
step:75/500 train_loss:5.7853 train_time:18497ms step_avg:284.58ms
step:76/500 train_loss:5.7476 train_time:18783ms step_avg:284.59ms
step:77/500 train_loss:5.7171 train_time:19068ms step_avg:284.59ms
step:78/500 train_loss:5.8003 train_time:19353ms step_avg:284.61ms
step:79/500 train_loss:5.8197 train_time:19640ms step_avg:284.64ms
step:80/500 train_loss:5.6930 train_time:19925ms step_avg:284.64ms
step:81/500 train_loss:5.7788 train_time:20209ms step_avg:284.64ms
step:82/500 train_loss:5.5395 train_time:20495ms step_avg:284.65ms
step:83/500 train_loss:5.7142 train_time:20780ms step_avg:284.65ms
step:84/500 train_loss:5.6799 train_time:21064ms step_avg:284.65ms
step:85/500 train_loss:5.6392 train_time:21349ms step_avg:284.65ms
step:86/500 train_loss:5.5111 train_time:21635ms step_avg:284.67ms
step:87/500 train_loss:5.7186 train_time:21920ms step_avg:284.67ms
step:88/500 train_loss:5.6242 train_time:22204ms step_avg:284.67ms
step:89/500 train_loss:5.6829 train_time:22489ms step_avg:284.67ms
step:90/500 train_loss:5.6646 train_time:22776ms step_avg:284.69ms
step:91/500 train_loss:5.5742 train_time:23063ms step_avg:284.73ms
step:92/500 train_loss:5.5867 train_time:23347ms step_avg:284.72ms
step:93/500 train_loss:5.6808 train_time:23634ms step_avg:284.74ms
step:94/500 train_loss:5.5317 train_time:23921ms step_avg:284.78ms
step:95/500 train_loss:5.5175 train_time:24206ms step_avg:284.78ms
step:96/500 train_loss:5.5373 train_time:24492ms step_avg:284.79ms
step:97/500 train_loss:5.4562 train_time:24779ms step_avg:284.81ms
step:98/500 train_loss:5.5300 train_time:25064ms step_avg:284.82ms
step:99/500 train_loss:5.4528 train_time:25349ms step_avg:284.82ms
step:100/500 train_loss:5.5800 train_time:25637ms step_avg:284.86ms
step:101/500 train_loss:5.5425 train_time:25922ms step_avg:284.86ms w_mean:1.000 w_std:0.032 w_min:0.996 w_max:1.993
step:102/500 train_loss:5.4327 train_time:26207ms step_avg:284.86ms
step:103/500 train_loss:5.5389 train_time:26494ms step_avg:284.88ms
step:104/500 train_loss:5.5019 train_time:26780ms step_avg:284.89ms
step:105/500 train_loss:5.3377 train_time:27066ms step_avg:284.90ms
step:106/500 train_loss:5.4494 train_time:27351ms step_avg:284.90ms
step:107/500 train_loss:5.6470 train_time:27636ms step_avg:284.91ms
step:108/500 train_loss:5.4346 train_time:27922ms step_avg:284.92ms
step:109/500 train_loss:5.1940 train_time:28207ms step_avg:284.92ms
step:110/500 train_loss:5.3987 train_time:28495ms step_avg:284.95ms
step:111/500 train_loss:5.3689 train_time:28780ms step_avg:284.95ms
step:112/500 train_loss:5.3409 train_time:29065ms step_avg:284.95ms
step:113/500 train_loss:5.4512 train_time:29351ms step_avg:284.96ms
step:114/500 train_loss:5.3744 train_time:29637ms step_avg:284.98ms
step:115/500 train_loss:5.2299 train_time:29924ms step_avg:284.99ms
step:116/500 train_loss:5.3961 train_time:30208ms step_avg:284.98ms
step:117/500 train_loss:5.2583 train_time:30494ms step_avg:284.99ms
step:118/500 train_loss:5.2538 train_time:30780ms step_avg:285.00ms
step:119/500 train_loss:5.3719 train_time:31065ms step_avg:285.00ms
step:120/500 train_loss:5.3574 train_time:31351ms step_avg:285.01ms
step:121/500 train_loss:5.2852 train_time:31640ms step_avg:285.04ms
step:122/500 train_loss:5.1816 train_time:31924ms step_avg:285.03ms
step:123/500 train_loss:5.2765 train_time:32209ms step_avg:285.03ms
step:124/500 train_loss:5.1472 train_time:32496ms step_avg:285.06ms
step:125/500 train_loss:5.4382 train_time:32782ms step_avg:285.06ms
step:125/500 val_loss:5.2690 train_time:32783ms step_avg:285.07ms
step:126/500 train_loss:5.2913 train_time:33054ms step_avg:284.95ms
step:127/500 train_loss:5.2659 train_time:33344ms step_avg:284.99ms
step:128/500 train_loss:5.3317 train_time:33629ms step_avg:284.99ms
step:129/500 train_loss:5.1890 train_time:33913ms step_avg:284.99ms
step:130/500 train_loss:5.4602 train_time:34201ms step_avg:285.01ms
step:131/500 train_loss:5.2452 train_time:34487ms step_avg:285.02ms
step:132/500 train_loss:5.2475 train_time:34773ms step_avg:285.02ms
step:133/500 train_loss:5.1915 train_time:35058ms step_avg:285.02ms
step:134/500 train_loss:5.2402 train_time:35346ms step_avg:285.05ms
step:135/500 train_loss:5.1617 train_time:35630ms step_avg:285.04ms
step:136/500 train_loss:5.2354 train_time:35916ms step_avg:285.05ms
step:137/500 train_loss:5.0329 train_time:36203ms step_avg:285.06ms
step:138/500 train_loss:5.1933 train_time:36488ms step_avg:285.06ms
step:139/500 train_loss:5.1534 train_time:36773ms step_avg:285.06ms
step:140/500 train_loss:5.1625 train_time:37060ms step_avg:285.07ms
step:141/500 train_loss:5.2149 train_time:37347ms step_avg:285.09ms
step:142/500 train_loss:5.1173 train_time:37632ms step_avg:285.09ms
step:143/500 train_loss:5.1925 train_time:37918ms step_avg:285.10ms
step:144/500 train_loss:5.0076 train_time:38206ms step_avg:285.12ms
step:145/500 train_loss:5.1661 train_time:38491ms step_avg:285.12ms
step:146/500 train_loss:5.1045 train_time:38777ms step_avg:285.12ms
step:147/500 train_loss:5.0137 train_time:39063ms step_avg:285.13ms
step:148/500 train_loss:5.1327 train_time:39349ms step_avg:285.14ms
step:149/500 train_loss:5.1065 train_time:39632ms step_avg:285.12ms
step:150/500 train_loss:5.1676 train_time:39918ms step_avg:285.13ms
step:151/500 train_loss:5.1783 train_time:40204ms step_avg:285.13ms w_mean:1.000 w_std:0.027 w_min:0.998 w_max:1.995
step:152/500 train_loss:5.0976 train_time:40489ms step_avg:285.14ms
step:153/500 train_loss:5.0775 train_time:40775ms step_avg:285.14ms
step:154/500 train_loss:5.1538 train_time:41061ms step_avg:285.15ms
step:155/500 train_loss:5.0920 train_time:41348ms step_avg:285.16ms
step:156/500 train_loss:5.0732 train_time:41632ms step_avg:285.15ms
step:157/500 train_loss:5.0869 train_time:41918ms step_avg:285.15ms
step:158/500 train_loss:5.2106 train_time:42205ms step_avg:285.17ms
step:159/500 train_loss:4.9990 train_time:42490ms step_avg:285.17ms
step:160/500 train_loss:5.0601 train_time:42773ms step_avg:285.16ms
step:161/500 train_loss:4.9194 train_time:43060ms step_avg:285.16ms
step:162/500 train_loss:5.0691 train_time:43347ms step_avg:285.18ms
step:163/500 train_loss:5.1021 train_time:43631ms step_avg:285.17ms
step:164/500 train_loss:5.0898 train_time:43917ms step_avg:285.17ms
step:165/500 train_loss:4.9178 train_time:44204ms step_avg:285.19ms
step:166/500 train_loss:5.0337 train_time:44490ms step_avg:285.19ms
step:167/500 train_loss:5.1828 train_time:44774ms step_avg:285.18ms
step:168/500 train_loss:4.9636 train_time:45059ms step_avg:285.19ms
step:169/500 train_loss:5.0435 train_time:45346ms step_avg:285.20ms
step:170/500 train_loss:4.9114 train_time:45629ms step_avg:285.18ms
step:171/500 train_loss:4.8545 train_time:45914ms step_avg:285.18ms
step:172/500 train_loss:4.9598 train_time:46200ms step_avg:285.19ms
step:173/500 train_loss:4.9386 train_time:46487ms step_avg:285.19ms
step:174/500 train_loss:4.9921 train_time:46771ms step_avg:285.19ms
step:175/500 train_loss:5.1339 train_time:47056ms step_avg:285.19ms
step:176/500 train_loss:5.0171 train_time:47345ms step_avg:285.21ms
step:177/500 train_loss:4.8558 train_time:47629ms step_avg:285.20ms
step:178/500 train_loss:4.8353 train_time:47913ms step_avg:285.20ms
step:179/500 train_loss:4.8738 train_time:48199ms step_avg:285.20ms
step:180/500 train_loss:4.9142 train_time:48487ms step_avg:285.22ms
step:181/500 train_loss:4.9035 train_time:48771ms step_avg:285.21ms
step:182/500 train_loss:5.0162 train_time:49054ms step_avg:285.20ms
step:183/500 train_loss:4.9074 train_time:49341ms step_avg:285.21ms
step:184/500 train_loss:4.8312 train_time:49627ms step_avg:285.21ms
step:185/500 train_loss:4.8635 train_time:49912ms step_avg:285.21ms
step:186/500 train_loss:4.9852 train_time:50197ms step_avg:285.21ms
step:187/500 train_loss:4.8713 train_time:50484ms step_avg:285.22ms
step:188/500 train_loss:5.1107 train_time:50769ms step_avg:285.22ms
step:189/500 train_loss:4.9073 train_time:51366ms step_avg:286.96ms
step:190/500 train_loss:4.8219 train_time:51957ms step_avg:288.65ms
step:191/500 train_loss:4.9843 train_time:52244ms step_avg:288.64ms
step:192/500 train_loss:4.8183 train_time:52527ms step_avg:288.61ms
step:193/500 train_loss:4.7454 train_time:52811ms step_avg:288.58ms
step:194/500 train_loss:4.9445 train_time:53095ms step_avg:288.56ms
step:195/500 train_loss:4.8823 train_time:53381ms step_avg:288.55ms
step:196/500 train_loss:5.0653 train_time:53666ms step_avg:288.53ms
step:197/500 train_loss:4.9617 train_time:53950ms step_avg:288.50ms
step:198/500 train_loss:4.7983 train_time:54234ms step_avg:288.48ms
step:199/500 train_loss:4.8423 train_time:54520ms step_avg:288.47ms
step:200/500 train_loss:4.7339 train_time:54807ms step_avg:288.46ms
step:201/500 train_loss:4.8193 train_time:55092ms step_avg:288.44ms w_mean:1.000 w_std:0.021 w_min:0.998 w_max:1.997
step:202/500 train_loss:4.7445 train_time:55377ms step_avg:288.42ms
step:203/500 train_loss:4.9698 train_time:55664ms step_avg:288.41ms
step:204/500 train_loss:4.8689 train_time:55949ms step_avg:288.40ms
step:205/500 train_loss:4.8353 train_time:56232ms step_avg:288.37ms
step:206/500 train_loss:4.9925 train_time:56517ms step_avg:288.35ms
step:207/500 train_loss:4.6691 train_time:56805ms step_avg:288.35ms
step:208/500 train_loss:4.8160 train_time:57089ms step_avg:288.33ms
step:209/500 train_loss:4.7685 train_time:57374ms step_avg:288.31ms
step:210/500 train_loss:4.9352 train_time:57659ms step_avg:288.29ms
step:211/500 train_loss:4.8587 train_time:57946ms step_avg:288.29ms
step:212/500 train_loss:4.7490 train_time:58229ms step_avg:288.26ms
step:213/500 train_loss:4.8796 train_time:58515ms step_avg:288.25ms
step:214/500 train_loss:4.7178 train_time:58802ms step_avg:288.25ms
step:215/500 train_loss:4.8098 train_time:59088ms step_avg:288.23ms
step:216/500 train_loss:4.6651 train_time:59373ms step_avg:288.22ms
step:217/500 train_loss:4.7803 train_time:59658ms step_avg:288.20ms
step:218/500 train_loss:4.7631 train_time:59946ms step_avg:288.20ms
step:219/500 train_loss:4.7410 train_time:60231ms step_avg:288.19ms
step:220/500 train_loss:4.7548 train_time:60516ms step_avg:288.17ms
step:221/500 train_loss:4.7787 train_time:60802ms step_avg:288.16ms
step:222/500 train_loss:4.8149 train_time:61089ms step_avg:288.15ms
step:223/500 train_loss:4.7551 train_time:61372ms step_avg:288.13ms
step:224/500 train_loss:4.7628 train_time:61657ms step_avg:288.12ms
step:225/500 train_loss:4.8809 train_time:61944ms step_avg:288.11ms
step:226/500 train_loss:4.6334 train_time:62228ms step_avg:288.09ms
step:227/500 train_loss:4.6585 train_time:62512ms step_avg:288.07ms
step:228/500 train_loss:4.6505 train_time:62797ms step_avg:288.06ms
step:229/500 train_loss:4.8145 train_time:63085ms step_avg:288.06ms
step:230/500 train_loss:4.6365 train_time:63370ms step_avg:288.04ms
step:231/500 train_loss:4.7947 train_time:63653ms step_avg:288.02ms
step:232/500 train_loss:4.6557 train_time:63940ms step_avg:288.02ms
step:233/500 train_loss:4.6173 train_time:64225ms step_avg:288.00ms
step:234/500 train_loss:4.8217 train_time:64511ms step_avg:287.99ms
step:235/500 train_loss:4.6603 train_time:64797ms step_avg:287.99ms
step:236/500 train_loss:4.5975 train_time:65084ms step_avg:287.98ms
step:237/500 train_loss:4.8535 train_time:65370ms step_avg:287.97ms
step:238/500 train_loss:4.7283 train_time:65654ms step_avg:287.96ms
step:239/500 train_loss:4.6417 train_time:65941ms step_avg:287.95ms
step:240/500 train_loss:4.7830 train_time:66227ms step_avg:287.94ms
step:241/500 train_loss:4.7680 train_time:66512ms step_avg:287.93ms
step:242/500 train_loss:4.6753 train_time:66797ms step_avg:287.92ms
step:243/500 train_loss:4.8288 train_time:67084ms step_avg:287.91ms
step:244/500 train_loss:4.6680 train_time:67369ms step_avg:287.90ms
step:245/500 train_loss:4.6774 train_time:67653ms step_avg:287.88ms
step:246/500 train_loss:4.7485 train_time:67939ms step_avg:287.88ms
step:247/500 train_loss:4.7029 train_time:68226ms step_avg:287.87ms
step:248/500 train_loss:4.6610 train_time:68511ms step_avg:287.86ms
step:249/500 train_loss:4.8221 train_time:68794ms step_avg:287.84ms
step:250/500 train_loss:4.5647 train_time:69082ms step_avg:287.84ms
step:250/500 val_loss:4.6701 train_time:69082ms step_avg:287.84ms
step:251/500 train_loss:4.6045 train_time:69354ms step_avg:287.78ms w_mean:1.000 w_std:0.027 w_min:0.998 w_max:1.995
step:252/500 train_loss:4.7365 train_time:69645ms step_avg:287.79ms
step:253/500 train_loss:4.7282 train_time:69933ms step_avg:287.79ms
step:254/500 train_loss:4.6056 train_time:70217ms step_avg:287.78ms
step:255/500 train_loss:4.6280 train_time:70502ms step_avg:287.76ms
step:256/500 train_loss:4.7600 train_time:70790ms step_avg:287.76ms
step:257/500 train_loss:4.7012 train_time:71073ms step_avg:287.75ms
step:258/500 train_loss:4.6753 train_time:71358ms step_avg:287.73ms
step:259/500 train_loss:4.6065 train_time:71646ms step_avg:287.73ms
step:260/500 train_loss:4.6207 train_time:71931ms step_avg:287.72ms
step:261/500 train_loss:4.6894 train_time:72215ms step_avg:287.71ms
step:262/500 train_loss:4.6997 train_time:72499ms step_avg:287.70ms
step:263/500 train_loss:4.6062 train_time:72788ms step_avg:287.70ms
step:264/500 train_loss:4.5530 train_time:73073ms step_avg:287.69ms
step:265/500 train_loss:4.6086 train_time:73357ms step_avg:287.68ms
step:266/500 train_loss:4.4605 train_time:73644ms step_avg:287.67ms
step:267/500 train_loss:4.5195 train_time:73930ms step_avg:287.66ms
step:268/500 train_loss:4.5641 train_time:74214ms step_avg:287.65ms
step:269/500 train_loss:4.5197 train_time:74500ms step_avg:287.64ms
step:270/500 train_loss:4.4898 train_time:74787ms step_avg:287.64ms
step:271/500 train_loss:4.7090 train_time:75071ms step_avg:287.63ms
step:272/500 train_loss:4.6449 train_time:75355ms step_avg:287.61ms
step:273/500 train_loss:4.5033 train_time:75641ms step_avg:287.61ms
step:274/500 train_loss:4.5488 train_time:75928ms step_avg:287.61ms
step:275/500 train_loss:4.6726 train_time:76213ms step_avg:287.60ms
step:276/500 train_loss:4.6790 train_time:76496ms step_avg:287.58ms
step:277/500 train_loss:4.8829 train_time:76782ms step_avg:287.57ms
step:278/500 train_loss:4.6251 train_time:77069ms step_avg:287.57ms
step:279/500 train_loss:4.7607 train_time:77354ms step_avg:287.56ms
step:280/500 train_loss:4.6118 train_time:77637ms step_avg:287.55ms
step:281/500 train_loss:4.6597 train_time:77924ms step_avg:287.54ms
step:282/500 train_loss:4.5684 train_time:78211ms step_avg:287.54ms
step:283/500 train_loss:4.6849 train_time:78495ms step_avg:287.53ms
step:284/500 train_loss:4.5027 train_time:78781ms step_avg:287.52ms
step:285/500 train_loss:4.6716 train_time:79067ms step_avg:287.51ms
step:286/500 train_loss:4.6568 train_time:79352ms step_avg:287.51ms
step:287/500 train_loss:4.6866 train_time:79636ms step_avg:287.50ms
step:288/500 train_loss:4.5521 train_time:79922ms step_avg:287.49ms
step:289/500 train_loss:4.6150 train_time:80209ms step_avg:287.49ms
step:290/500 train_loss:4.4792 train_time:80493ms step_avg:287.48ms
step:291/500 train_loss:4.4747 train_time:80777ms step_avg:287.46ms
step:292/500 train_loss:4.6003 train_time:81062ms step_avg:287.46ms
step:293/500 train_loss:4.4861 train_time:81351ms step_avg:287.46ms
step:294/500 train_loss:4.5375 train_time:81635ms step_avg:287.45ms
step:295/500 train_loss:4.5563 train_time:81921ms step_avg:287.44ms
step:296/500 train_loss:4.4251 train_time:82206ms step_avg:287.43ms
step:297/500 train_loss:4.4127 train_time:82492ms step_avg:287.43ms
step:298/500 train_loss:4.4419 train_time:82776ms step_avg:287.42ms
step:299/500 train_loss:4.5492 train_time:83061ms step_avg:287.41ms
step:300/500 train_loss:4.4325 train_time:83349ms step_avg:287.41ms
step:301/500 train_loss:4.6142 train_time:83633ms step_avg:287.40ms w_mean:1.000 w_std:0.018 w_min:0.999 w_max:1.997
step:302/500 train_loss:4.5836 train_time:83919ms step_avg:287.39ms
step:303/500 train_loss:4.5084 train_time:84206ms step_avg:287.39ms
step:304/500 train_loss:4.5706 train_time:84493ms step_avg:287.39ms
step:305/500 train_loss:4.5579 train_time:84776ms step_avg:287.38ms
step:306/500 train_loss:5.0288 train_time:85061ms step_avg:287.37ms
step:307/500 train_loss:4.5164 train_time:85348ms step_avg:287.37ms
step:308/500 train_loss:4.4147 train_time:85634ms step_avg:287.36ms
step:309/500 train_loss:4.6073 train_time:85920ms step_avg:287.36ms
step:310/500 train_loss:4.4041 train_time:86206ms step_avg:287.35ms
step:311/500 train_loss:4.6400 train_time:86492ms step_avg:287.35ms
step:312/500 train_loss:4.5517 train_time:86776ms step_avg:287.34ms
step:313/500 train_loss:4.4659 train_time:87062ms step_avg:287.33ms
step:314/500 train_loss:4.5933 train_time:87350ms step_avg:287.34ms
step:315/500 train_loss:4.7272 train_time:87635ms step_avg:287.33ms
step:316/500 train_loss:4.5583 train_time:87921ms step_avg:287.32ms
step:317/500 train_loss:4.4514 train_time:88207ms step_avg:287.32ms
step:318/500 train_loss:4.4614 train_time:88495ms step_avg:287.32ms
step:319/500 train_loss:4.4773 train_time:88781ms step_avg:287.32ms
step:320/500 train_loss:4.4234 train_time:89067ms step_avg:287.31ms
step:321/500 train_loss:4.5187 train_time:89352ms step_avg:287.31ms
step:322/500 train_loss:4.5347 train_time:89636ms step_avg:287.29ms
step:323/500 train_loss:4.4962 train_time:89924ms step_avg:287.30ms
step:324/500 train_loss:4.5639 train_time:90210ms step_avg:287.29ms
step:325/500 train_loss:4.5560 train_time:90495ms step_avg:287.28ms
step:326/500 train_loss:4.6376 train_time:90780ms step_avg:287.28ms
step:327/500 train_loss:4.4848 train_time:91066ms step_avg:287.27ms
step:328/500 train_loss:4.9183 train_time:91352ms step_avg:287.27ms
step:329/500 train_loss:4.6320 train_time:91636ms step_avg:287.26ms
step:330/500 train_loss:4.4227 train_time:91922ms step_avg:287.26ms
step:331/500 train_loss:4.4029 train_time:92209ms step_avg:287.26ms
step:332/500 train_loss:4.5389 train_time:92494ms step_avg:287.25ms
step:333/500 train_loss:4.4579 train_time:92778ms step_avg:287.24ms
step:334/500 train_loss:4.4500 train_time:93065ms step_avg:287.24ms
step:335/500 train_loss:4.4017 train_time:93351ms step_avg:287.23ms
step:336/500 train_loss:4.6041 train_time:93635ms step_avg:287.22ms
step:337/500 train_loss:4.5358 train_time:93921ms step_avg:287.22ms
step:338/500 train_loss:5.0820 train_time:94207ms step_avg:287.22ms
step:339/500 train_loss:4.5118 train_time:94492ms step_avg:287.21ms
step:340/500 train_loss:4.4838 train_time:94777ms step_avg:287.20ms
step:341/500 train_loss:4.4579 train_time:95064ms step_avg:287.20ms
step:342/500 train_loss:4.4046 train_time:95351ms step_avg:287.20ms
step:343/500 train_loss:4.3751 train_time:95636ms step_avg:287.19ms
step:344/500 train_loss:4.4430 train_time:95921ms step_avg:287.19ms
step:345/500 train_loss:4.5317 train_time:96208ms step_avg:287.19ms
step:346/500 train_loss:4.4272 train_time:96493ms step_avg:287.18ms
step:347/500 train_loss:4.3775 train_time:96778ms step_avg:287.17ms
step:348/500 train_loss:4.4346 train_time:97065ms step_avg:287.17ms
step:349/500 train_loss:4.4242 train_time:97351ms step_avg:287.17ms
step:350/500 train_loss:4.3488 train_time:97641ms step_avg:287.18ms
step:351/500 train_loss:4.0341 train_time:97927ms step_avg:287.18ms w_mean:1.000 w_std:0.022 w_min:0.999 w_max:1.997
step:352/500 train_loss:4.3261 train_time:98213ms step_avg:287.17ms
step:353/500 train_loss:4.6738 train_time:98497ms step_avg:287.16ms
step:354/500 train_loss:4.2127 train_time:98783ms step_avg:287.16ms
step:355/500 train_loss:4.4629 train_time:99070ms step_avg:287.16ms
step:356/500 train_loss:4.3751 train_time:99356ms step_avg:287.16ms
step:357/500 train_loss:4.4648 train_time:99641ms step_avg:287.15ms
step:358/500 train_loss:4.4845 train_time:99929ms step_avg:287.15ms
step:359/500 train_loss:4.3767 train_time:100213ms step_avg:287.14ms
step:360/500 train_loss:4.7232 train_time:100497ms step_avg:287.14ms
step:361/500 train_loss:4.1089 train_time:100785ms step_avg:287.14ms
step:362/500 train_loss:4.5942 train_time:101070ms step_avg:287.13ms
step:363/500 train_loss:4.4948 train_time:101355ms step_avg:287.12ms
step:364/500 train_loss:4.3765 train_time:101640ms step_avg:287.12ms
step:365/500 train_loss:4.3104 train_time:101928ms step_avg:287.12ms
step:366/500 train_loss:4.4685 train_time:102212ms step_avg:287.11ms
step:367/500 train_loss:4.3932 train_time:102496ms step_avg:287.10ms
step:368/500 train_loss:4.3805 train_time:102781ms step_avg:287.10ms
step:369/500 train_loss:4.3911 train_time:103066ms step_avg:287.09ms
step:370/500 train_loss:4.2776 train_time:103351ms step_avg:287.09ms
step:371/500 train_loss:4.4261 train_time:103636ms step_avg:287.08ms
step:372/500 train_loss:4.3683 train_time:103922ms step_avg:287.08ms
step:373/500 train_loss:4.2420 train_time:104210ms step_avg:287.08ms
step:374/500 train_loss:4.4318 train_time:104494ms step_avg:287.07ms
step:375/500 train_loss:4.3647 train_time:104779ms step_avg:287.07ms
step:375/500 val_loss:4.3822 train_time:104780ms step_avg:287.07ms
step:376/500 train_loss:4.3660 train_time:105054ms step_avg:287.03ms
step:377/500 train_loss:4.4167 train_time:105342ms step_avg:287.04ms
step:378/500 train_loss:4.3148 train_time:105940ms step_avg:287.88ms
step:379/500 train_loss:4.3654 train_time:106225ms step_avg:287.87ms
step:380/500 train_loss:4.4422 train_time:106823ms step_avg:288.71ms
step:381/500 train_loss:4.4766 train_time:107109ms step_avg:288.70ms
step:382/500 train_loss:4.4239 train_time:107393ms step_avg:288.69ms
step:383/500 train_loss:4.4006 train_time:107677ms step_avg:288.68ms
step:384/500 train_loss:4.2950 train_time:107963ms step_avg:288.67ms
step:385/500 train_loss:4.3979 train_time:108248ms step_avg:288.66ms
step:386/500 train_loss:4.3113 train_time:108530ms step_avg:288.64ms
step:387/500 train_loss:4.4452 train_time:108815ms step_avg:288.63ms
step:388/500 train_loss:4.6415 train_time:109099ms step_avg:288.62ms
step:389/500 train_loss:4.3347 train_time:109387ms step_avg:288.62ms
step:390/500 train_loss:4.2901 train_time:109671ms step_avg:288.61ms
step:391/500 train_loss:4.4285 train_time:109955ms step_avg:288.60ms
step:392/500 train_loss:4.3460 train_time:110238ms step_avg:288.58ms
step:393/500 train_loss:4.4503 train_time:110523ms step_avg:288.57ms
step:394/500 train_loss:4.2747 train_time:110808ms step_avg:288.56ms
step:395/500 train_loss:4.4040 train_time:111093ms step_avg:288.55ms
step:396/500 train_loss:4.1992 train_time:111377ms step_avg:288.54ms
step:397/500 train_loss:4.3531 train_time:111662ms step_avg:288.53ms
step:398/500 train_loss:4.4533 train_time:111948ms step_avg:288.53ms
step:399/500 train_loss:4.4074 train_time:112235ms step_avg:288.52ms
step:400/500 train_loss:4.3136 train_time:112518ms step_avg:288.51ms
step:401/500 train_loss:4.3889 train_time:112803ms step_avg:288.50ms w_mean:1.000 w_std:0.009 w_min:1.000 w_max:2.000
step:402/500 train_loss:4.4200 train_time:113090ms step_avg:288.50ms
step:403/500 train_loss:4.3867 train_time:113375ms step_avg:288.49ms
step:404/500 train_loss:4.4814 train_time:113659ms step_avg:288.48ms
step:405/500 train_loss:4.2724 train_time:113944ms step_avg:288.47ms
step:406/500 train_loss:4.3116 train_time:114233ms step_avg:288.47ms
step:407/500 train_loss:4.5911 train_time:114517ms step_avg:288.46ms
step:408/500 train_loss:4.3503 train_time:114801ms step_avg:288.44ms
step:409/500 train_loss:4.3427 train_time:115088ms step_avg:288.44ms
step:410/500 train_loss:4.3937 train_time:115373ms step_avg:288.43ms
step:411/500 train_loss:4.2811 train_time:115659ms step_avg:288.43ms
step:412/500 train_loss:4.2949 train_time:115944ms step_avg:288.42ms
step:413/500 train_loss:4.7238 train_time:116232ms step_avg:288.42ms
step:414/500 train_loss:4.1626 train_time:116515ms step_avg:288.40ms
step:415/500 train_loss:4.5223 train_time:116800ms step_avg:288.39ms
step:416/500 train_loss:4.3003 train_time:117086ms step_avg:288.39ms
step:417/500 train_loss:4.2960 train_time:117373ms step_avg:288.39ms
step:418/500 train_loss:4.4775 train_time:117656ms step_avg:288.37ms
step:419/500 train_loss:4.2133 train_time:117939ms step_avg:288.36ms
step:420/500 train_loss:4.3103 train_time:118226ms step_avg:288.36ms
step:421/500 train_loss:4.2870 train_time:118512ms step_avg:288.35ms
step:422/500 train_loss:4.1662 train_time:118798ms step_avg:288.34ms
step:423/500 train_loss:4.2739 train_time:119082ms step_avg:288.33ms
step:424/500 train_loss:4.3886 train_time:119369ms step_avg:288.33ms
step:425/500 train_loss:4.2036 train_time:119654ms step_avg:288.32ms
step:426/500 train_loss:4.3518 train_time:119938ms step_avg:288.31ms
step:427/500 train_loss:4.2411 train_time:120226ms step_avg:288.31ms
step:428/500 train_loss:4.4161 train_time:120512ms step_avg:288.31ms
step:429/500 train_loss:4.3658 train_time:120796ms step_avg:288.30ms
step:430/500 train_loss:4.2780 train_time:121081ms step_avg:288.29ms
step:431/500 train_loss:4.2570 train_time:121368ms step_avg:288.28ms
step:432/500 train_loss:4.2214 train_time:121636ms step_avg:288.24ms
step:433/500 train_loss:4.2787 train_time:121905ms step_avg:288.19ms
step:434/500 train_loss:4.3607 train_time:122180ms step_avg:288.16ms
step:435/500 train_loss:4.3007 train_time:122453ms step_avg:288.12ms
step:436/500 train_loss:4.3355 train_time:122719ms step_avg:288.07ms
step:437/500 train_loss:4.3543 train_time:122990ms step_avg:288.03ms
step:438/500 train_loss:4.2376 train_time:123262ms step_avg:287.99ms
step:439/500 train_loss:4.2526 train_time:123536ms step_avg:287.96ms
step:440/500 train_loss:4.2193 train_time:123805ms step_avg:287.92ms
step:441/500 train_loss:4.4046 train_time:124077ms step_avg:287.88ms
step:442/500 train_loss:4.3146 train_time:124349ms step_avg:287.85ms
step:443/500 train_loss:4.2840 train_time:124618ms step_avg:287.80ms
step:444/500 train_loss:4.1710 train_time:124891ms step_avg:287.77ms
step:445/500 train_loss:4.4388 train_time:125160ms step_avg:287.72ms
step:446/500 train_loss:4.3527 train_time:125434ms step_avg:287.69ms
step:447/500 train_loss:4.3500 train_time:125704ms step_avg:287.65ms
step:448/500 train_loss:4.2715 train_time:125976ms step_avg:287.62ms
step:449/500 train_loss:4.3546 train_time:126248ms step_avg:287.58ms
step:450/500 train_loss:4.1975 train_time:126517ms step_avg:287.54ms
step:451/500 train_loss:4.2323 train_time:126789ms step_avg:287.50ms w_mean:1.000 w_std:0.011 w_min:1.000 w_max:1.999
step:452/500 train_loss:4.1254 train_time:127058ms step_avg:287.46ms
step:453/500 train_loss:4.2244 train_time:127333ms step_avg:287.43ms
step:454/500 train_loss:4.1975 train_time:127602ms step_avg:287.39ms
step:455/500 train_loss:4.1734 train_time:127875ms step_avg:287.36ms
step:456/500 train_loss:4.3862 train_time:128147ms step_avg:287.33ms
step:457/500 train_loss:4.2391 train_time:128416ms step_avg:287.28ms
step:458/500 train_loss:4.3318 train_time:128688ms step_avg:287.25ms
step:459/500 train_loss:4.3596 train_time:128958ms step_avg:287.21ms
step:460/500 train_loss:4.1596 train_time:129232ms step_avg:287.18ms
step:461/500 train_loss:4.3343 train_time:129500ms step_avg:287.14ms
step:462/500 train_loss:4.2393 train_time:129772ms step_avg:287.11ms
step:463/500 train_loss:4.2192 train_time:130044ms step_avg:287.07ms
step:464/500 train_loss:4.3063 train_time:130317ms step_avg:287.04ms
step:465/500 train_loss:4.2447 train_time:130587ms step_avg:287.00ms
step:466/500 train_loss:4.2461 train_time:130858ms step_avg:286.97ms
step:467/500 train_loss:4.3737 train_time:131133ms step_avg:286.94ms
step:468/500 train_loss:4.3910 train_time:131403ms step_avg:286.91ms
step:469/500 train_loss:4.3389 train_time:131675ms step_avg:286.87ms
step:470/500 train_loss:4.2541 train_time:131946ms step_avg:286.84ms
step:471/500 train_loss:4.3365 train_time:132216ms step_avg:286.80ms
step:472/500 train_loss:4.3825 train_time:132489ms step_avg:286.77ms
step:473/500 train_loss:4.2818 train_time:132759ms step_avg:286.74ms
step:474/500 train_loss:4.2648 train_time:133030ms step_avg:286.70ms
step:475/500 train_loss:4.1328 train_time:133298ms step_avg:286.66ms
step:476/500 train_loss:4.5744 train_time:133571ms step_avg:286.63ms
step:477/500 train_loss:4.3139 train_time:133838ms step_avg:286.59ms
step:478/500 train_loss:4.1255 train_time:134110ms step_avg:286.56ms
step:479/500 train_loss:4.3179 train_time:134379ms step_avg:286.52ms
step:480/500 train_loss:4.3041 train_time:134653ms step_avg:286.50ms
step:481/500 train_loss:4.4277 train_time:134923ms step_avg:286.46ms
step:482/500 train_loss:4.2598 train_time:135194ms step_avg:286.43ms
step:483/500 train_loss:4.0709 train_time:135464ms step_avg:286.39ms
step:484/500 train_loss:4.3476 train_time:135736ms step_avg:286.36ms
step:485/500 train_loss:4.1950 train_time:136007ms step_avg:286.33ms
step:486/500 train_loss:4.2286 train_time:136277ms step_avg:286.30ms
step:487/500 train_loss:4.1752 train_time:136549ms step_avg:286.27ms
step:488/500 train_loss:4.1923 train_time:136818ms step_avg:286.23ms
step:489/500 train_loss:4.4085 train_time:137092ms step_avg:286.21ms
step:490/500 train_loss:4.2669 train_time:137363ms step_avg:286.17ms
step:491/500 train_loss:4.1680 train_time:137637ms step_avg:286.15ms
step:492/500 train_loss:4.1653 train_time:137905ms step_avg:286.11ms
step:493/500 train_loss:4.2790 train_time:138178ms step_avg:286.08ms
step:494/500 train_loss:4.1228 train_time:138450ms step_avg:286.05ms
step:495/500 train_loss:4.2734 train_time:138719ms step_avg:286.02ms
step:496/500 train_loss:4.1897 train_time:138991ms step_avg:285.99ms
step:497/500 train_loss:4.1425 train_time:139259ms step_avg:285.95ms
step:498/500 train_loss:4.2853 train_time:139535ms step_avg:285.93ms
step:499/500 train_loss:4.3728 train_time:139806ms step_avg:285.90ms
step:500/500 train_loss:4.4237 train_time:140078ms step_avg:285.87ms
step:500/500 val_loss:4.2661 train_time:140079ms step_avg:285.88ms
