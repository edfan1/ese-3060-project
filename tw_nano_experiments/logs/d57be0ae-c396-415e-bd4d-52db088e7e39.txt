====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # also save the seed for reproducibility
        if args.seed is not None:
            log['seed'] = args.seed
        # save token weighting config
        if tw_config.enabled:
            log['tw_config'] = {
                'function': tw_config.function,
                'clamp_min': tw_config.clamp_min,
                'clamp_max': tw_config.clamp_max,
                'schedule': tw_config.schedule,
            }
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 21:04:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   35C    P0             78W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   35C    P0             80W /  310W |    2363MiB /  81559MiB |     29%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   41C    P0             83W /  310W |    2363MiB /  81559MiB |     30%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   37C    P0             80W /  310W |    2363MiB /  81559MiB |     35%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   35C    P0             78W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   40C    P0             81W /  310W |    2363MiB /  81559MiB |     32%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   39C    P0             79W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   34C    P0             79W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           52116      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           52117      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           52118      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           52119      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           52120      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           52121      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           52122      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           52123      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 42 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: linear
  clamp: [0.5, 2.0]
  schedule: constant
====================================================================================================
step:0/500 val_loss:15.9787 train_time:279ms step_avg:nanms
step:1/500 train_loss:15.9775 train_time:73303ms step_avg:nanms w_mean:1.000 w_std:0.041 w_min:0.500 w_max:1.156
step:2/500 train_loss:9.3666 train_time:73842ms step_avg:nanms
step:3/500 train_loss:8.9780 train_time:74107ms step_avg:nanms
step:4/500 train_loss:8.6141 train_time:74371ms step_avg:nanms
step:5/500 train_loss:8.2273 train_time:74642ms step_avg:nanms
step:6/500 train_loss:7.8353 train_time:74912ms step_avg:nanms
step:7/500 train_loss:7.4194 train_time:75179ms step_avg:nanms
step:8/500 train_loss:7.6431 train_time:75449ms step_avg:nanms
step:9/500 train_loss:7.3898 train_time:75717ms step_avg:nanms
step:10/500 train_loss:7.1294 train_time:75986ms step_avg:nanms
step:11/500 train_loss:7.1305 train_time:272ms step_avg:nanms
step:12/500 train_loss:7.0528 train_time:537ms step_avg:nanms
step:13/500 train_loss:6.8973 train_time:804ms step_avg:268.05ms
step:14/500 train_loss:6.8892 train_time:1073ms step_avg:268.29ms
step:15/500 train_loss:6.8736 train_time:1340ms step_avg:268.03ms
step:16/500 train_loss:6.7745 train_time:1606ms step_avg:267.63ms
step:17/500 train_loss:6.8326 train_time:1875ms step_avg:267.83ms
step:18/500 train_loss:6.7939 train_time:2143ms step_avg:267.86ms
step:19/500 train_loss:6.6824 train_time:2413ms step_avg:268.08ms
step:20/500 train_loss:6.6398 train_time:2682ms step_avg:268.20ms
step:21/500 train_loss:6.3354 train_time:2948ms step_avg:268.04ms
step:22/500 train_loss:6.6962 train_time:3221ms step_avg:268.38ms
step:23/500 train_loss:6.9238 train_time:3486ms step_avg:268.14ms
step:24/500 train_loss:6.5827 train_time:3755ms step_avg:268.19ms
step:25/500 train_loss:6.6777 train_time:4028ms step_avg:268.55ms
step:26/500 train_loss:6.4213 train_time:4298ms step_avg:268.63ms
step:27/500 train_loss:6.3167 train_time:4565ms step_avg:268.52ms
step:28/500 train_loss:6.5052 train_time:4831ms step_avg:268.37ms
step:29/500 train_loss:6.1545 train_time:5103ms step_avg:268.59ms
step:30/500 train_loss:6.4320 train_time:5375ms step_avg:268.76ms
step:31/500 train_loss:6.2886 train_time:5645ms step_avg:268.79ms
step:32/500 train_loss:6.2472 train_time:5913ms step_avg:268.79ms
step:33/500 train_loss:6.0845 train_time:6184ms step_avg:268.89ms
step:34/500 train_loss:6.4038 train_time:6458ms step_avg:269.09ms
step:35/500 train_loss:6.3157 train_time:6727ms step_avg:269.06ms
step:36/500 train_loss:6.4381 train_time:6999ms step_avg:269.18ms
step:37/500 train_loss:6.4045 train_time:7266ms step_avg:269.10ms
step:38/500 train_loss:6.2601 train_time:7541ms step_avg:269.31ms
step:39/500 train_loss:6.1817 train_time:7810ms step_avg:269.33ms
step:40/500 train_loss:6.2254 train_time:8081ms step_avg:269.36ms
step:41/500 train_loss:6.1299 train_time:8347ms step_avg:269.27ms
step:42/500 train_loss:6.1617 train_time:8617ms step_avg:269.29ms
step:43/500 train_loss:6.0317 train_time:8885ms step_avg:269.23ms
step:44/500 train_loss:6.1233 train_time:9155ms step_avg:269.26ms
step:45/500 train_loss:6.1136 train_time:9424ms step_avg:269.26ms
step:46/500 train_loss:6.2805 train_time:9691ms step_avg:269.20ms
step:47/500 train_loss:6.0986 train_time:9959ms step_avg:269.17ms
step:48/500 train_loss:5.9466 train_time:10230ms step_avg:269.20ms
step:49/500 train_loss:6.1732 train_time:10500ms step_avg:269.23ms
step:50/500 train_loss:6.0465 train_time:10768ms step_avg:269.20ms
step:51/500 train_loss:6.1951 train_time:11037ms step_avg:269.20ms w_mean:1.000 w_std:0.425 w_min:0.488 w_max:1.950
step:52/500 train_loss:6.0637 train_time:11309ms step_avg:269.27ms
step:53/500 train_loss:5.9333 train_time:11575ms step_avg:269.18ms
step:54/500 train_loss:6.0202 train_time:11847ms step_avg:269.25ms
step:55/500 train_loss:5.9815 train_time:12114ms step_avg:269.19ms
step:56/500 train_loss:6.2330 train_time:12384ms step_avg:269.22ms
step:57/500 train_loss:5.9852 train_time:12652ms step_avg:269.20ms
step:58/500 train_loss:5.8209 train_time:12922ms step_avg:269.21ms
step:59/500 train_loss:6.0217 train_time:13191ms step_avg:269.20ms
step:60/500 train_loss:5.9175 train_time:13462ms step_avg:269.24ms
step:61/500 train_loss:6.0444 train_time:13732ms step_avg:269.25ms
step:62/500 train_loss:5.8277 train_time:14004ms step_avg:269.30ms
step:63/500 train_loss:5.9382 train_time:14272ms step_avg:269.29ms
step:64/500 train_loss:5.8926 train_time:14543ms step_avg:269.32ms
step:65/500 train_loss:5.8240 train_time:14815ms step_avg:269.37ms
step:66/500 train_loss:5.7340 train_time:15085ms step_avg:269.38ms
step:67/500 train_loss:5.9078 train_time:15353ms step_avg:269.35ms
step:68/500 train_loss:5.7569 train_time:15625ms step_avg:269.39ms
step:69/500 train_loss:6.0069 train_time:15895ms step_avg:269.41ms
step:70/500 train_loss:5.6771 train_time:16166ms step_avg:269.43ms
step:71/500 train_loss:5.7007 train_time:16436ms step_avg:269.43ms
step:72/500 train_loss:5.8936 train_time:16704ms step_avg:269.41ms
step:73/500 train_loss:5.8407 train_time:16974ms step_avg:269.43ms
step:74/500 train_loss:5.7412 train_time:17243ms step_avg:269.42ms
step:75/500 train_loss:5.8340 train_time:17516ms step_avg:269.48ms
step:76/500 train_loss:5.8111 train_time:17783ms step_avg:269.44ms
step:77/500 train_loss:5.7625 train_time:18054ms step_avg:269.46ms
step:78/500 train_loss:5.8668 train_time:18324ms step_avg:269.47ms
step:79/500 train_loss:5.8574 train_time:18596ms step_avg:269.50ms
step:80/500 train_loss:5.7515 train_time:18864ms step_avg:269.49ms
step:81/500 train_loss:5.8118 train_time:19134ms step_avg:269.49ms
step:82/500 train_loss:5.6105 train_time:19407ms step_avg:269.54ms
step:83/500 train_loss:5.7583 train_time:19679ms step_avg:269.58ms
step:84/500 train_loss:5.7353 train_time:19949ms step_avg:269.58ms
step:85/500 train_loss:5.7135 train_time:20220ms step_avg:269.61ms
step:86/500 train_loss:5.5351 train_time:20490ms step_avg:269.60ms
step:87/500 train_loss:5.7945 train_time:20760ms step_avg:269.61ms
step:88/500 train_loss:5.6368 train_time:21032ms step_avg:269.64ms
step:89/500 train_loss:5.7750 train_time:21302ms step_avg:269.64ms
step:90/500 train_loss:5.6801 train_time:21570ms step_avg:269.63ms
step:91/500 train_loss:5.6597 train_time:21841ms step_avg:269.65ms
step:92/500 train_loss:5.6170 train_time:22113ms step_avg:269.67ms
step:93/500 train_loss:5.7403 train_time:22383ms step_avg:269.68ms
step:94/500 train_loss:5.5692 train_time:22657ms step_avg:269.72ms
step:95/500 train_loss:5.5719 train_time:22929ms step_avg:269.75ms
step:96/500 train_loss:5.5857 train_time:23198ms step_avg:269.75ms
step:97/500 train_loss:5.5182 train_time:23469ms step_avg:269.76ms
step:98/500 train_loss:5.5764 train_time:23741ms step_avg:269.78ms
step:99/500 train_loss:5.5124 train_time:24013ms step_avg:269.80ms
step:100/500 train_loss:5.6105 train_time:24283ms step_avg:269.81ms
step:101/500 train_loss:5.6018 train_time:24554ms step_avg:269.82ms w_mean:1.000 w_std:0.441 w_min:0.487 w_max:1.947
step:102/500 train_loss:5.4792 train_time:24825ms step_avg:269.84ms
step:103/500 train_loss:5.6103 train_time:25096ms step_avg:269.85ms
step:104/500 train_loss:5.5426 train_time:25370ms step_avg:269.89ms
step:105/500 train_loss:5.4073 train_time:25645ms step_avg:269.95ms
step:106/500 train_loss:5.5036 train_time:25917ms step_avg:269.97ms
step:107/500 train_loss:5.6814 train_time:26186ms step_avg:269.96ms
step:108/500 train_loss:5.5185 train_time:26455ms step_avg:269.95ms
step:109/500 train_loss:5.2373 train_time:26730ms step_avg:270.00ms
step:110/500 train_loss:5.4734 train_time:27001ms step_avg:270.01ms
step:111/500 train_loss:5.4126 train_time:27271ms step_avg:270.01ms
step:112/500 train_loss:5.4228 train_time:27542ms step_avg:270.02ms
step:113/500 train_loss:5.4886 train_time:27816ms step_avg:270.05ms
step:114/500 train_loss:5.4404 train_time:28087ms step_avg:270.06ms
step:115/500 train_loss:5.2839 train_time:28353ms step_avg:270.03ms
step:116/500 train_loss:5.4615 train_time:28626ms step_avg:270.05ms
step:117/500 train_loss:5.3113 train_time:28896ms step_avg:270.05ms
step:118/500 train_loss:5.3109 train_time:29168ms step_avg:270.08ms
step:119/500 train_loss:5.4296 train_time:29439ms step_avg:270.08ms
step:120/500 train_loss:5.4069 train_time:29712ms step_avg:270.11ms
step:121/500 train_loss:5.3657 train_time:29981ms step_avg:270.10ms
step:122/500 train_loss:5.2163 train_time:30253ms step_avg:270.11ms
step:123/500 train_loss:5.3497 train_time:30524ms step_avg:270.13ms
step:124/500 train_loss:5.1916 train_time:30794ms step_avg:270.12ms
step:125/500 train_loss:5.5101 train_time:31065ms step_avg:270.13ms
step:125/500 val_loss:5.3157 train_time:31066ms step_avg:270.14ms
step:126/500 train_loss:5.3315 train_time:31338ms step_avg:270.16ms
step:127/500 train_loss:5.3413 train_time:31616ms step_avg:270.22ms
step:128/500 train_loss:5.3807 train_time:31890ms step_avg:270.26ms
step:129/500 train_loss:5.2649 train_time:32159ms step_avg:270.24ms
step:130/500 train_loss:5.4988 train_time:32426ms step_avg:270.22ms
step:131/500 train_loss:5.3281 train_time:32702ms step_avg:270.26ms
step:132/500 train_loss:5.2902 train_time:32974ms step_avg:270.27ms
step:133/500 train_loss:5.2594 train_time:33243ms step_avg:270.27ms
step:134/500 train_loss:5.2897 train_time:33514ms step_avg:270.27ms
step:135/500 train_loss:5.2265 train_time:33784ms step_avg:270.27ms
step:136/500 train_loss:5.2962 train_time:34057ms step_avg:270.29ms
step:137/500 train_loss:5.1059 train_time:34328ms step_avg:270.30ms
step:138/500 train_loss:5.2582 train_time:34601ms step_avg:270.32ms
step:139/500 train_loss:5.2100 train_time:34870ms step_avg:270.31ms
step:140/500 train_loss:5.2285 train_time:35144ms step_avg:270.33ms
step:141/500 train_loss:5.2525 train_time:35414ms step_avg:270.33ms
step:142/500 train_loss:5.1974 train_time:35684ms step_avg:270.34ms
step:143/500 train_loss:5.2421 train_time:35956ms step_avg:270.35ms
step:144/500 train_loss:5.1031 train_time:36228ms step_avg:270.36ms
step:145/500 train_loss:5.2136 train_time:36500ms step_avg:270.37ms
step:146/500 train_loss:5.1784 train_time:36770ms step_avg:270.37ms
step:147/500 train_loss:5.0663 train_time:37042ms step_avg:270.38ms
step:148/500 train_loss:5.2116 train_time:37315ms step_avg:270.40ms
step:149/500 train_loss:5.1642 train_time:37588ms step_avg:270.41ms
step:150/500 train_loss:5.2404 train_time:37859ms step_avg:270.42ms
step:151/500 train_loss:5.2293 train_time:38130ms step_avg:270.43ms w_mean:1.000 w_std:0.465 w_min:0.485 w_max:1.941
step:152/500 train_loss:5.1876 train_time:38404ms step_avg:270.45ms
step:153/500 train_loss:5.1265 train_time:38673ms step_avg:270.44ms
step:154/500 train_loss:5.2244 train_time:38946ms step_avg:270.46ms
step:155/500 train_loss:5.1461 train_time:39220ms step_avg:270.48ms
step:156/500 train_loss:5.1411 train_time:39491ms step_avg:270.49ms
step:157/500 train_loss:5.1567 train_time:39761ms step_avg:270.48ms
step:158/500 train_loss:5.2574 train_time:40031ms step_avg:270.48ms
step:159/500 train_loss:5.0894 train_time:40305ms step_avg:270.51ms
step:160/500 train_loss:5.1037 train_time:40576ms step_avg:270.51ms
step:161/500 train_loss:5.0070 train_time:40846ms step_avg:270.50ms
step:162/500 train_loss:5.1095 train_time:41119ms step_avg:270.52ms
step:163/500 train_loss:5.1820 train_time:41392ms step_avg:270.54ms
step:164/500 train_loss:5.1379 train_time:41661ms step_avg:270.53ms
step:165/500 train_loss:5.0038 train_time:41931ms step_avg:270.52ms
step:166/500 train_loss:5.0747 train_time:42204ms step_avg:270.54ms
step:167/500 train_loss:5.2588 train_time:42474ms step_avg:270.53ms
step:168/500 train_loss:5.0247 train_time:42745ms step_avg:270.54ms
step:169/500 train_loss:5.1196 train_time:43017ms step_avg:270.55ms
step:170/500 train_loss:4.9707 train_time:43290ms step_avg:270.56ms
step:171/500 train_loss:4.9530 train_time:43562ms step_avg:270.57ms
step:172/500 train_loss:5.0269 train_time:43833ms step_avg:270.57ms
step:173/500 train_loss:5.0120 train_time:44105ms step_avg:270.58ms
step:174/500 train_loss:5.0589 train_time:44377ms step_avg:270.59ms
step:175/500 train_loss:5.1971 train_time:44645ms step_avg:270.57ms
step:176/500 train_loss:5.0836 train_time:44916ms step_avg:270.58ms
step:177/500 train_loss:4.9355 train_time:45189ms step_avg:270.59ms
step:178/500 train_loss:4.9038 train_time:45462ms step_avg:270.61ms
step:179/500 train_loss:4.9543 train_time:45733ms step_avg:270.61ms
step:180/500 train_loss:4.9855 train_time:46005ms step_avg:270.62ms
step:181/500 train_loss:4.9753 train_time:46276ms step_avg:270.62ms
step:182/500 train_loss:5.0830 train_time:46545ms step_avg:270.61ms
step:183/500 train_loss:4.9763 train_time:46819ms step_avg:270.63ms
step:184/500 train_loss:4.9143 train_time:47091ms step_avg:270.64ms
step:185/500 train_loss:4.9271 train_time:47361ms step_avg:270.64ms
step:186/500 train_loss:5.0776 train_time:47633ms step_avg:270.64ms
step:187/500 train_loss:4.9244 train_time:47907ms step_avg:270.66ms
step:188/500 train_loss:5.1776 train_time:48178ms step_avg:270.66ms
step:189/500 train_loss:4.9683 train_time:48704ms step_avg:272.09ms
step:190/500 train_loss:4.9121 train_time:49247ms step_avg:273.59ms
step:191/500 train_loss:5.0418 train_time:49514ms step_avg:273.56ms
step:192/500 train_loss:4.9048 train_time:49781ms step_avg:273.52ms
step:193/500 train_loss:4.8139 train_time:50046ms step_avg:273.48ms
step:194/500 train_loss:5.0189 train_time:50326ms step_avg:273.51ms
step:195/500 train_loss:4.9560 train_time:50599ms step_avg:273.51ms
step:196/500 train_loss:5.1316 train_time:50866ms step_avg:273.47ms
step:197/500 train_loss:5.0286 train_time:51136ms step_avg:273.46ms
step:198/500 train_loss:4.8790 train_time:51411ms step_avg:273.46ms
step:199/500 train_loss:4.9184 train_time:51683ms step_avg:273.45ms
step:200/500 train_loss:4.8148 train_time:51951ms step_avg:273.42ms
step:201/500 train_loss:4.8976 train_time:52221ms step_avg:273.41ms w_mean:1.000 w_std:0.471 w_min:0.486 w_max:1.943
step:202/500 train_loss:4.8180 train_time:52494ms step_avg:273.41ms
step:203/500 train_loss:5.0278 train_time:52765ms step_avg:273.39ms
step:204/500 train_loss:4.9602 train_time:53033ms step_avg:273.37ms
step:205/500 train_loss:4.9038 train_time:53305ms step_avg:273.36ms
step:206/500 train_loss:5.0693 train_time:53577ms step_avg:273.35ms
step:207/500 train_loss:4.7411 train_time:53846ms step_avg:273.33ms
step:208/500 train_loss:4.8980 train_time:54117ms step_avg:273.32ms
step:209/500 train_loss:4.8530 train_time:54387ms step_avg:273.30ms
step:210/500 train_loss:5.0039 train_time:54660ms step_avg:273.30ms
step:211/500 train_loss:4.9464 train_time:54932ms step_avg:273.29ms
step:212/500 train_loss:4.8175 train_time:55202ms step_avg:273.28ms
step:213/500 train_loss:4.9548 train_time:55474ms step_avg:273.27ms
step:214/500 train_loss:4.7911 train_time:55746ms step_avg:273.26ms
step:215/500 train_loss:4.8855 train_time:56019ms step_avg:273.26ms
step:216/500 train_loss:4.7585 train_time:56289ms step_avg:273.25ms
step:217/500 train_loss:4.8654 train_time:56560ms step_avg:273.24ms
step:218/500 train_loss:4.8435 train_time:56832ms step_avg:273.23ms
step:219/500 train_loss:4.8190 train_time:57104ms step_avg:273.23ms
step:220/500 train_loss:4.8333 train_time:57373ms step_avg:273.20ms
step:221/500 train_loss:4.8631 train_time:57644ms step_avg:273.19ms
step:222/500 train_loss:4.8834 train_time:57916ms step_avg:273.19ms
step:223/500 train_loss:4.8441 train_time:58187ms step_avg:273.18ms
step:224/500 train_loss:4.8320 train_time:58459ms step_avg:273.17ms
step:225/500 train_loss:4.9650 train_time:58731ms step_avg:273.17ms
step:226/500 train_loss:4.6877 train_time:59004ms step_avg:273.17ms
step:227/500 train_loss:4.7559 train_time:59276ms step_avg:273.16ms
step:228/500 train_loss:4.7177 train_time:59544ms step_avg:273.14ms
step:229/500 train_loss:4.8929 train_time:59817ms step_avg:273.14ms
step:230/500 train_loss:4.7221 train_time:60093ms step_avg:273.15ms
step:231/500 train_loss:4.8664 train_time:60364ms step_avg:273.14ms
step:232/500 train_loss:4.7429 train_time:60634ms step_avg:273.13ms
step:233/500 train_loss:4.7014 train_time:60904ms step_avg:273.11ms
step:234/500 train_loss:4.9017 train_time:61178ms step_avg:273.12ms
step:235/500 train_loss:4.7367 train_time:61448ms step_avg:273.10ms
step:236/500 train_loss:4.6801 train_time:61720ms step_avg:273.10ms
step:237/500 train_loss:4.9218 train_time:61991ms step_avg:273.09ms
step:238/500 train_loss:4.8046 train_time:62262ms step_avg:273.08ms
step:239/500 train_loss:4.7378 train_time:62534ms step_avg:273.08ms
step:240/500 train_loss:4.8540 train_time:62807ms step_avg:273.07ms
step:241/500 train_loss:4.8535 train_time:63079ms step_avg:273.07ms
step:242/500 train_loss:4.7496 train_time:63346ms step_avg:273.04ms
step:243/500 train_loss:4.9093 train_time:63620ms step_avg:273.05ms
step:244/500 train_loss:4.7361 train_time:63892ms step_avg:273.04ms
step:245/500 train_loss:4.7733 train_time:64161ms step_avg:273.03ms
step:246/500 train_loss:4.8171 train_time:64434ms step_avg:273.03ms
step:247/500 train_loss:4.7891 train_time:64711ms step_avg:273.04ms
step:248/500 train_loss:4.7363 train_time:64982ms step_avg:273.03ms
step:249/500 train_loss:4.9098 train_time:65251ms step_avg:273.01ms
step:250/500 train_loss:4.6451 train_time:65523ms step_avg:273.01ms
step:250/500 val_loss:4.7474 train_time:65524ms step_avg:273.02ms
step:251/500 train_loss:4.6850 train_time:65795ms step_avg:273.01ms w_mean:1.000 w_std:0.481 w_min:0.485 w_max:1.942
step:252/500 train_loss:4.8067 train_time:66069ms step_avg:273.01ms
step:253/500 train_loss:4.8057 train_time:66344ms step_avg:273.02ms
step:254/500 train_loss:4.6915 train_time:66613ms step_avg:273.00ms
step:255/500 train_loss:4.6930 train_time:66883ms step_avg:272.99ms
step:256/500 train_loss:4.8180 train_time:67153ms step_avg:272.98ms
step:257/500 train_loss:4.7792 train_time:67427ms step_avg:272.98ms
step:258/500 train_loss:4.7519 train_time:67697ms step_avg:272.97ms
step:259/500 train_loss:4.6982 train_time:67965ms step_avg:272.95ms
step:260/500 train_loss:4.6944 train_time:68235ms step_avg:272.94ms
step:261/500 train_loss:4.7787 train_time:68515ms step_avg:272.97ms
step:262/500 train_loss:4.7593 train_time:68784ms step_avg:272.95ms
step:263/500 train_loss:4.6899 train_time:69054ms step_avg:272.94ms
step:264/500 train_loss:4.6312 train_time:69324ms step_avg:272.93ms
step:265/500 train_loss:4.6875 train_time:69596ms step_avg:272.92ms
step:266/500 train_loss:4.5604 train_time:69867ms step_avg:272.92ms
step:267/500 train_loss:4.5968 train_time:70138ms step_avg:272.91ms
step:268/500 train_loss:4.6513 train_time:70411ms step_avg:272.91ms
step:269/500 train_loss:4.5946 train_time:70684ms step_avg:272.91ms
step:270/500 train_loss:4.5841 train_time:70953ms step_avg:272.90ms
step:271/500 train_loss:4.7741 train_time:71222ms step_avg:272.88ms
step:272/500 train_loss:4.7259 train_time:71495ms step_avg:272.88ms
step:273/500 train_loss:4.5813 train_time:71767ms step_avg:272.88ms
step:274/500 train_loss:4.6393 train_time:72035ms step_avg:272.86ms
step:275/500 train_loss:4.7381 train_time:72309ms step_avg:272.87ms
step:276/500 train_loss:4.7566 train_time:72582ms step_avg:272.87ms
step:277/500 train_loss:4.9465 train_time:72852ms step_avg:272.86ms
step:278/500 train_loss:4.6969 train_time:73122ms step_avg:272.84ms
step:279/500 train_loss:4.8361 train_time:73396ms step_avg:272.85ms
step:280/500 train_loss:4.6659 train_time:73668ms step_avg:272.84ms
step:281/500 train_loss:4.7559 train_time:73938ms step_avg:272.83ms
step:282/500 train_loss:4.6292 train_time:74211ms step_avg:272.84ms
step:283/500 train_loss:4.7517 train_time:74484ms step_avg:272.84ms
step:284/500 train_loss:4.5738 train_time:74753ms step_avg:272.82ms
step:285/500 train_loss:4.7395 train_time:75025ms step_avg:272.82ms
step:286/500 train_loss:4.7246 train_time:75296ms step_avg:272.81ms
step:287/500 train_loss:4.7594 train_time:75569ms step_avg:272.81ms
step:288/500 train_loss:4.6162 train_time:75836ms step_avg:272.79ms
step:289/500 train_loss:4.6855 train_time:76109ms step_avg:272.79ms
step:290/500 train_loss:4.5438 train_time:76384ms step_avg:272.80ms
step:291/500 train_loss:4.5529 train_time:76655ms step_avg:272.79ms
step:292/500 train_loss:4.6692 train_time:76925ms step_avg:272.78ms
step:293/500 train_loss:4.5727 train_time:77195ms step_avg:272.78ms
step:294/500 train_loss:4.6019 train_time:77467ms step_avg:272.77ms
step:295/500 train_loss:4.6417 train_time:77738ms step_avg:272.76ms
step:296/500 train_loss:4.5073 train_time:78012ms step_avg:272.77ms
step:297/500 train_loss:4.5025 train_time:78285ms step_avg:272.77ms
step:298/500 train_loss:4.5300 train_time:78554ms step_avg:272.76ms
step:299/500 train_loss:4.6116 train_time:78828ms step_avg:272.76ms
step:300/500 train_loss:4.5202 train_time:79099ms step_avg:272.76ms
step:301/500 train_loss:4.6697 train_time:79370ms step_avg:272.75ms w_mean:1.000 w_std:0.483 w_min:0.486 w_max:1.942
step:302/500 train_loss:4.6537 train_time:79640ms step_avg:272.74ms
step:303/500 train_loss:4.5726 train_time:79913ms step_avg:272.74ms
step:304/500 train_loss:4.6392 train_time:80187ms step_avg:272.74ms
step:305/500 train_loss:4.6210 train_time:80455ms step_avg:272.73ms
step:306/500 train_loss:5.0748 train_time:80731ms step_avg:272.74ms
step:307/500 train_loss:4.5836 train_time:81003ms step_avg:272.74ms
step:308/500 train_loss:4.4954 train_time:81275ms step_avg:272.73ms
step:309/500 train_loss:4.6624 train_time:81549ms step_avg:272.74ms
step:310/500 train_loss:4.4868 train_time:81820ms step_avg:272.73ms
step:311/500 train_loss:4.7013 train_time:82093ms step_avg:272.73ms
step:312/500 train_loss:4.6305 train_time:82362ms step_avg:272.72ms
step:313/500 train_loss:4.5262 train_time:82635ms step_avg:272.72ms
step:314/500 train_loss:4.6564 train_time:82909ms step_avg:272.73ms
step:315/500 train_loss:4.7685 train_time:83181ms step_avg:272.72ms
step:316/500 train_loss:4.6203 train_time:83450ms step_avg:272.71ms
step:317/500 train_loss:4.5046 train_time:83721ms step_avg:272.71ms
step:318/500 train_loss:4.5297 train_time:83995ms step_avg:272.71ms
step:319/500 train_loss:4.5454 train_time:84265ms step_avg:272.70ms
step:320/500 train_loss:4.4995 train_time:84534ms step_avg:272.69ms
step:321/500 train_loss:4.5955 train_time:84809ms step_avg:272.70ms
step:322/500 train_loss:4.5878 train_time:85078ms step_avg:272.68ms
step:323/500 train_loss:4.5572 train_time:85352ms step_avg:272.69ms
step:324/500 train_loss:4.6292 train_time:85622ms step_avg:272.68ms
step:325/500 train_loss:4.6211 train_time:85895ms step_avg:272.68ms
step:326/500 train_loss:4.6816 train_time:86166ms step_avg:272.68ms
step:327/500 train_loss:4.5339 train_time:86436ms step_avg:272.67ms
step:328/500 train_loss:4.9622 train_time:86709ms step_avg:272.67ms
step:329/500 train_loss:4.6874 train_time:86982ms step_avg:272.67ms
step:330/500 train_loss:4.4963 train_time:87252ms step_avg:272.66ms
step:331/500 train_loss:4.4441 train_time:87524ms step_avg:272.66ms
step:332/500 train_loss:4.5985 train_time:87797ms step_avg:272.66ms
step:333/500 train_loss:4.5303 train_time:88067ms step_avg:272.65ms
step:334/500 train_loss:4.5070 train_time:88337ms step_avg:272.65ms
step:335/500 train_loss:4.4797 train_time:88610ms step_avg:272.65ms
step:336/500 train_loss:4.6447 train_time:88884ms step_avg:272.65ms
step:337/500 train_loss:4.6028 train_time:89155ms step_avg:272.64ms
step:338/500 train_loss:5.0976 train_time:89429ms step_avg:272.65ms
step:339/500 train_loss:4.5765 train_time:89697ms step_avg:272.64ms
step:340/500 train_loss:4.5259 train_time:89969ms step_avg:272.63ms
step:341/500 train_loss:4.5289 train_time:90240ms step_avg:272.63ms
step:342/500 train_loss:4.4610 train_time:90513ms step_avg:272.63ms
step:343/500 train_loss:4.4500 train_time:90786ms step_avg:272.63ms
step:344/500 train_loss:4.5053 train_time:91056ms step_avg:272.62ms
step:345/500 train_loss:4.5956 train_time:91329ms step_avg:272.63ms
step:346/500 train_loss:4.4795 train_time:91605ms step_avg:272.63ms
step:347/500 train_loss:4.4351 train_time:91876ms step_avg:272.63ms
step:348/500 train_loss:4.4744 train_time:92150ms step_avg:272.63ms
step:349/500 train_loss:4.4837 train_time:92422ms step_avg:272.63ms
step:350/500 train_loss:4.4224 train_time:92695ms step_avg:272.63ms
step:351/500 train_loss:4.1279 train_time:92966ms step_avg:272.63ms w_mean:1.000 w_std:0.493 w_min:0.490 w_max:1.959
step:352/500 train_loss:4.3998 train_time:93236ms step_avg:272.62ms
step:353/500 train_loss:4.7245 train_time:93509ms step_avg:272.62ms
step:354/500 train_loss:4.2868 train_time:93781ms step_avg:272.62ms
step:355/500 train_loss:4.5160 train_time:94053ms step_avg:272.62ms
step:356/500 train_loss:4.4286 train_time:94322ms step_avg:272.61ms
step:357/500 train_loss:4.5171 train_time:94595ms step_avg:272.61ms
step:358/500 train_loss:4.5237 train_time:94865ms step_avg:272.60ms
step:359/500 train_loss:4.4355 train_time:95138ms step_avg:272.60ms
step:360/500 train_loss:4.7016 train_time:95412ms step_avg:272.60ms
step:361/500 train_loss:4.1849 train_time:95685ms step_avg:272.61ms
step:362/500 train_loss:4.6372 train_time:95952ms step_avg:272.59ms
step:363/500 train_loss:4.5408 train_time:96227ms step_avg:272.60ms
step:364/500 train_loss:4.4359 train_time:96502ms step_avg:272.61ms
step:365/500 train_loss:4.3780 train_time:96774ms step_avg:272.60ms
step:366/500 train_loss:4.5254 train_time:97044ms step_avg:272.60ms
step:367/500 train_loss:4.4606 train_time:97317ms step_avg:272.60ms
step:368/500 train_loss:4.4429 train_time:97591ms step_avg:272.60ms
step:369/500 train_loss:4.4462 train_time:97861ms step_avg:272.59ms
step:370/500 train_loss:4.3549 train_time:98133ms step_avg:272.59ms
step:371/500 train_loss:4.4857 train_time:98406ms step_avg:272.59ms
step:372/500 train_loss:4.4228 train_time:98677ms step_avg:272.59ms
step:373/500 train_loss:4.3015 train_time:98951ms step_avg:272.59ms
step:374/500 train_loss:4.4910 train_time:99220ms step_avg:272.58ms
step:375/500 train_loss:4.4356 train_time:99494ms step_avg:272.59ms
step:375/500 val_loss:4.4399 train_time:99495ms step_avg:272.59ms
step:376/500 train_loss:4.4196 train_time:99767ms step_avg:272.59ms
step:377/500 train_loss:4.4924 train_time:100044ms step_avg:272.60ms
step:378/500 train_loss:4.3820 train_time:100566ms step_avg:273.28ms
step:379/500 train_loss:4.4235 train_time:100835ms step_avg:273.27ms
step:380/500 train_loss:4.5074 train_time:101364ms step_avg:273.96ms
step:381/500 train_loss:4.5219 train_time:101631ms step_avg:273.94ms
step:382/500 train_loss:4.4715 train_time:101896ms step_avg:273.91ms
step:383/500 train_loss:4.4541 train_time:102164ms step_avg:273.90ms
step:384/500 train_loss:4.3663 train_time:102446ms step_avg:273.92ms
step:385/500 train_loss:4.4686 train_time:102719ms step_avg:273.92ms
step:386/500 train_loss:4.3755 train_time:102986ms step_avg:273.90ms
step:387/500 train_loss:4.4980 train_time:103254ms step_avg:273.88ms
step:388/500 train_loss:4.6914 train_time:103525ms step_avg:273.88ms
step:389/500 train_loss:4.3994 train_time:103804ms step_avg:273.89ms
step:390/500 train_loss:4.3675 train_time:104073ms step_avg:273.88ms
step:391/500 train_loss:4.4760 train_time:104343ms step_avg:273.87ms
step:392/500 train_loss:4.4093 train_time:104616ms step_avg:273.86ms
step:393/500 train_loss:4.5028 train_time:104887ms step_avg:273.86ms
step:394/500 train_loss:4.3399 train_time:105163ms step_avg:273.86ms
step:395/500 train_loss:4.4729 train_time:105431ms step_avg:273.85ms
step:396/500 train_loss:4.2582 train_time:105704ms step_avg:273.85ms
step:397/500 train_loss:4.4166 train_time:105974ms step_avg:273.83ms
step:398/500 train_loss:4.5051 train_time:106246ms step_avg:273.83ms
step:399/500 train_loss:4.4426 train_time:106520ms step_avg:273.83ms
step:400/500 train_loss:4.3871 train_time:106791ms step_avg:273.82ms
step:401/500 train_loss:4.4426 train_time:107061ms step_avg:273.81ms w_mean:1.000 w_std:0.492 w_min:0.485 w_max:1.941
step:402/500 train_loss:4.4839 train_time:107332ms step_avg:273.81ms
step:403/500 train_loss:4.4579 train_time:107605ms step_avg:273.80ms
step:404/500 train_loss:4.5377 train_time:107875ms step_avg:273.79ms
step:405/500 train_loss:4.3168 train_time:108146ms step_avg:273.79ms
step:406/500 train_loss:4.3742 train_time:108418ms step_avg:273.78ms
step:407/500 train_loss:4.6325 train_time:108691ms step_avg:273.78ms
step:408/500 train_loss:4.4181 train_time:108960ms step_avg:273.77ms
step:409/500 train_loss:4.4073 train_time:109231ms step_avg:273.76ms
step:410/500 train_loss:4.4555 train_time:109505ms step_avg:273.76ms
step:411/500 train_loss:4.3452 train_time:109777ms step_avg:273.76ms
step:412/500 train_loss:4.3572 train_time:110048ms step_avg:273.75ms
step:413/500 train_loss:4.7582 train_time:110322ms step_avg:273.75ms
step:414/500 train_loss:4.2407 train_time:110596ms step_avg:273.75ms
step:415/500 train_loss:4.5799 train_time:110864ms step_avg:273.74ms
step:416/500 train_loss:4.3698 train_time:111136ms step_avg:273.73ms
step:417/500 train_loss:4.3566 train_time:111406ms step_avg:273.72ms
step:418/500 train_loss:4.5345 train_time:111677ms step_avg:273.72ms
step:419/500 train_loss:4.2793 train_time:111947ms step_avg:273.71ms
step:420/500 train_loss:4.3786 train_time:112215ms step_avg:273.70ms
step:421/500 train_loss:4.3540 train_time:112487ms step_avg:273.69ms
step:422/500 train_loss:4.2483 train_time:112761ms step_avg:273.69ms
step:423/500 train_loss:4.3525 train_time:113033ms step_avg:273.69ms
step:424/500 train_loss:4.4515 train_time:113303ms step_avg:273.68ms
step:425/500 train_loss:4.2607 train_time:113573ms step_avg:273.67ms
step:426/500 train_loss:4.4342 train_time:113846ms step_avg:273.67ms
step:427/500 train_loss:4.3053 train_time:114119ms step_avg:273.67ms
step:428/500 train_loss:4.4678 train_time:114387ms step_avg:273.65ms
step:429/500 train_loss:4.4347 train_time:114660ms step_avg:273.65ms
step:430/500 train_loss:4.3443 train_time:114931ms step_avg:273.65ms
step:431/500 train_loss:4.3258 train_time:115205ms step_avg:273.65ms
step:432/500 train_loss:4.2694 train_time:115474ms step_avg:273.63ms
step:433/500 train_loss:4.3611 train_time:115745ms step_avg:273.63ms
step:434/500 train_loss:4.4336 train_time:116018ms step_avg:273.63ms
step:435/500 train_loss:4.3591 train_time:116288ms step_avg:273.62ms
step:436/500 train_loss:4.4100 train_time:116562ms step_avg:273.62ms
step:437/500 train_loss:4.4220 train_time:116831ms step_avg:273.61ms
step:438/500 train_loss:4.3096 train_time:117104ms step_avg:273.61ms
step:439/500 train_loss:4.3259 train_time:117374ms step_avg:273.60ms
step:440/500 train_loss:4.2772 train_time:117646ms step_avg:273.60ms
step:441/500 train_loss:4.4687 train_time:117923ms step_avg:273.60ms
step:442/500 train_loss:4.3789 train_time:118187ms step_avg:273.58ms
step:443/500 train_loss:4.3530 train_time:118461ms step_avg:273.58ms
step:444/500 train_loss:4.2556 train_time:118734ms step_avg:273.58ms
step:445/500 train_loss:4.4860 train_time:119006ms step_avg:273.58ms
step:446/500 train_loss:4.4284 train_time:119276ms step_avg:273.57ms
step:447/500 train_loss:4.4241 train_time:119546ms step_avg:273.56ms
step:448/500 train_loss:4.3444 train_time:119820ms step_avg:273.56ms
step:449/500 train_loss:4.4229 train_time:120089ms step_avg:273.55ms
step:450/500 train_loss:4.2660 train_time:120362ms step_avg:273.55ms
step:451/500 train_loss:4.3127 train_time:120631ms step_avg:273.54ms w_mean:1.000 w_std:0.494 w_min:0.487 w_max:1.947
step:452/500 train_loss:4.2007 train_time:120904ms step_avg:273.54ms
step:453/500 train_loss:4.2999 train_time:121176ms step_avg:273.53ms
step:454/500 train_loss:4.2832 train_time:121445ms step_avg:273.53ms
step:455/500 train_loss:4.2551 train_time:121716ms step_avg:273.52ms
step:456/500 train_loss:4.4455 train_time:121986ms step_avg:273.51ms
step:457/500 train_loss:4.3113 train_time:122260ms step_avg:273.51ms
step:458/500 train_loss:4.3949 train_time:122530ms step_avg:273.50ms
step:459/500 train_loss:4.4327 train_time:122803ms step_avg:273.50ms
step:460/500 train_loss:4.2370 train_time:123072ms step_avg:273.49ms
step:461/500 train_loss:4.4063 train_time:123344ms step_avg:273.49ms
step:462/500 train_loss:4.3079 train_time:123616ms step_avg:273.49ms
step:463/500 train_loss:4.2950 train_time:123885ms step_avg:273.48ms
step:464/500 train_loss:4.3837 train_time:124158ms step_avg:273.47ms
step:465/500 train_loss:4.3183 train_time:124428ms step_avg:273.47ms
step:466/500 train_loss:4.3161 train_time:124702ms step_avg:273.47ms
step:467/500 train_loss:4.4361 train_time:124971ms step_avg:273.46ms
step:468/500 train_loss:4.4308 train_time:125243ms step_avg:273.46ms
step:469/500 train_loss:4.4027 train_time:125514ms step_avg:273.45ms
step:470/500 train_loss:4.3123 train_time:125788ms step_avg:273.45ms
step:471/500 train_loss:4.4029 train_time:126061ms step_avg:273.45ms
step:472/500 train_loss:4.4514 train_time:126330ms step_avg:273.44ms
step:473/500 train_loss:4.3632 train_time:126604ms step_avg:273.44ms
step:474/500 train_loss:4.3298 train_time:126873ms step_avg:273.43ms
step:475/500 train_loss:4.2160 train_time:127144ms step_avg:273.43ms
step:476/500 train_loss:4.6168 train_time:127413ms step_avg:273.42ms
step:477/500 train_loss:4.3818 train_time:127685ms step_avg:273.42ms
step:478/500 train_loss:4.1966 train_time:127958ms step_avg:273.42ms
step:479/500 train_loss:4.3927 train_time:128227ms step_avg:273.40ms
step:480/500 train_loss:4.3781 train_time:128498ms step_avg:273.40ms
step:481/500 train_loss:4.4979 train_time:128769ms step_avg:273.39ms
step:482/500 train_loss:4.3352 train_time:129042ms step_avg:273.39ms
step:483/500 train_loss:4.1645 train_time:129312ms step_avg:273.39ms
step:484/500 train_loss:4.4215 train_time:129584ms step_avg:273.38ms
step:485/500 train_loss:4.2732 train_time:129856ms step_avg:273.38ms
step:486/500 train_loss:4.2978 train_time:130132ms step_avg:273.39ms
step:487/500 train_loss:4.2497 train_time:130405ms step_avg:273.39ms
step:488/500 train_loss:4.2711 train_time:130675ms step_avg:273.38ms
step:489/500 train_loss:4.4692 train_time:130947ms step_avg:273.38ms
step:490/500 train_loss:4.3383 train_time:131221ms step_avg:273.38ms
step:491/500 train_loss:4.2311 train_time:131492ms step_avg:273.37ms
step:492/500 train_loss:4.2479 train_time:131763ms step_avg:273.37ms
step:493/500 train_loss:4.3550 train_time:132034ms step_avg:273.36ms
step:494/500 train_loss:4.1996 train_time:132306ms step_avg:273.36ms
step:495/500 train_loss:4.3512 train_time:132576ms step_avg:273.35ms
step:496/500 train_loss:4.2700 train_time:132845ms step_avg:273.34ms
step:497/500 train_loss:4.2131 train_time:133115ms step_avg:273.34ms
step:498/500 train_loss:4.3493 train_time:133386ms step_avg:273.33ms
step:499/500 train_loss:4.4328 train_time:133660ms step_avg:273.33ms
step:500/500 train_loss:4.4799 train_time:133928ms step_avg:273.32ms
step:500/500 val_loss:4.3380 train_time:133929ms step_avg:273.33ms
