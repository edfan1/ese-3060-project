====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock and keep timing accurate, but skip checkpoint writes to save disk
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        if master_process:
            print("Checkpoint saving disabled; skipping state serialization.")
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 22:59:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   45C    P0             83W /  310W |    2363MiB /  81559MiB |     15%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   45C    P0             86W /  310W |    2363MiB /  81559MiB |     30%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   50C    P0             89W /  310W |    2363MiB /  81559MiB |     31%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             87W /  310W |    2363MiB /  81559MiB |     34%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   43C    P0             81W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   49C    P0             86W /  310W |    2363MiB /  81559MiB |     28%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   53C    P0             88W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           82227      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           82228      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           82229      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           82230      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           82231      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           82232      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           82233      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           82234      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 123 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: focal
  clamp: [0.4, 2.0]
  schedule: constant
  focal_gamma: 2.0
====================================================================================================
step:0/500 val_loss:16.0073 train_time:261ms step_avg:nanms
step:1/500 train_loss:15.9981 train_time:59539ms step_avg:nanms w_mean:1.000 w_std:0.081 w_min:0.533 w_max:1.332
step:2/500 train_loss:9.3712 train_time:59806ms step_avg:nanms
step:3/500 train_loss:8.9661 train_time:60074ms step_avg:nanms
step:4/500 train_loss:8.6866 train_time:60340ms step_avg:nanms
step:5/500 train_loss:8.1149 train_time:60618ms step_avg:nanms
step:6/500 train_loss:7.7497 train_time:60888ms step_avg:nanms
step:7/500 train_loss:7.3114 train_time:61157ms step_avg:nanms
step:8/500 train_loss:7.4822 train_time:61425ms step_avg:nanms
step:9/500 train_loss:7.2682 train_time:61696ms step_avg:nanms
step:10/500 train_loss:7.0518 train_time:61967ms step_avg:nanms
step:11/500 train_loss:7.0459 train_time:269ms step_avg:nanms
step:12/500 train_loss:7.0229 train_time:547ms step_avg:nanms
step:13/500 train_loss:6.8251 train_time:827ms step_avg:275.79ms
step:14/500 train_loss:6.8300 train_time:1107ms step_avg:276.80ms
step:15/500 train_loss:6.7936 train_time:1387ms step_avg:277.33ms
step:16/500 train_loss:6.7164 train_time:1667ms step_avg:277.78ms
step:17/500 train_loss:6.7190 train_time:1953ms step_avg:278.98ms
step:18/500 train_loss:6.7428 train_time:2233ms step_avg:279.12ms
step:19/500 train_loss:6.5663 train_time:2504ms step_avg:278.17ms
step:20/500 train_loss:6.5792 train_time:2771ms step_avg:277.05ms
step:21/500 train_loss:6.2449 train_time:3046ms step_avg:276.94ms
step:22/500 train_loss:6.6294 train_time:3313ms step_avg:276.11ms
step:23/500 train_loss:6.8740 train_time:3586ms step_avg:275.85ms
step:24/500 train_loss:6.5086 train_time:3856ms step_avg:275.40ms
step:25/500 train_loss:6.6207 train_time:4129ms step_avg:275.26ms
step:26/500 train_loss:6.3546 train_time:4398ms step_avg:274.87ms
step:27/500 train_loss:6.2652 train_time:4667ms step_avg:274.56ms
step:28/500 train_loss:6.4368 train_time:4940ms step_avg:274.43ms
step:29/500 train_loss:6.0952 train_time:5210ms step_avg:274.19ms
step:30/500 train_loss:6.3765 train_time:5481ms step_avg:274.06ms
step:31/500 train_loss:6.2225 train_time:5753ms step_avg:273.96ms
step:32/500 train_loss:6.1880 train_time:6021ms step_avg:273.67ms
step:33/500 train_loss:5.9999 train_time:6293ms step_avg:273.60ms
step:34/500 train_loss:6.3426 train_time:6561ms step_avg:273.39ms
step:35/500 train_loss:6.2496 train_time:6832ms step_avg:273.30ms
step:36/500 train_loss:6.4091 train_time:7101ms step_avg:273.12ms
step:37/500 train_loss:6.3343 train_time:7370ms step_avg:272.97ms
step:38/500 train_loss:6.2210 train_time:7639ms step_avg:272.82ms
step:39/500 train_loss:6.1116 train_time:7912ms step_avg:272.84ms
step:40/500 train_loss:6.1683 train_time:8185ms step_avg:272.82ms
step:41/500 train_loss:6.0709 train_time:8455ms step_avg:272.75ms
step:42/500 train_loss:6.1094 train_time:8725ms step_avg:272.64ms
step:43/500 train_loss:5.9752 train_time:8998ms step_avg:272.68ms
step:44/500 train_loss:6.0636 train_time:9270ms step_avg:272.65ms
step:45/500 train_loss:6.0519 train_time:9544ms step_avg:272.69ms
step:46/500 train_loss:6.2382 train_time:9815ms step_avg:272.64ms
step:47/500 train_loss:6.0366 train_time:10088ms step_avg:272.65ms
step:48/500 train_loss:5.8804 train_time:10358ms step_avg:272.57ms
step:49/500 train_loss:6.1276 train_time:10631ms step_avg:272.60ms
step:50/500 train_loss:5.9939 train_time:10900ms step_avg:272.50ms
step:51/500 train_loss:6.1477 train_time:11171ms step_avg:272.46ms w_mean:1.000 w_std:0.067 w_min:0.987 w_max:2.467
step:52/500 train_loss:6.0106 train_time:11441ms step_avg:272.41ms
step:53/500 train_loss:5.8602 train_time:11714ms step_avg:272.42ms
step:54/500 train_loss:5.9882 train_time:11985ms step_avg:272.38ms
step:55/500 train_loss:5.9066 train_time:12255ms step_avg:272.33ms
step:56/500 train_loss:6.2131 train_time:12525ms step_avg:272.28ms
step:57/500 train_loss:5.9016 train_time:12797ms step_avg:272.28ms
step:58/500 train_loss:5.7823 train_time:13069ms step_avg:272.27ms
step:59/500 train_loss:5.9396 train_time:13338ms step_avg:272.20ms
step:60/500 train_loss:5.8807 train_time:13610ms step_avg:272.19ms
step:61/500 train_loss:5.9695 train_time:13882ms step_avg:272.20ms
step:62/500 train_loss:5.7664 train_time:14152ms step_avg:272.16ms
step:63/500 train_loss:5.8704 train_time:14423ms step_avg:272.13ms
step:64/500 train_loss:5.8323 train_time:14695ms step_avg:272.14ms
step:65/500 train_loss:5.7537 train_time:14966ms step_avg:272.11ms
step:66/500 train_loss:5.6621 train_time:15237ms step_avg:272.09ms
step:67/500 train_loss:5.8347 train_time:15508ms step_avg:272.08ms
step:68/500 train_loss:5.6975 train_time:15781ms step_avg:272.09ms
step:69/500 train_loss:5.9391 train_time:16054ms step_avg:272.10ms
step:70/500 train_loss:5.6122 train_time:16324ms step_avg:272.07ms
step:71/500 train_loss:5.6364 train_time:16598ms step_avg:272.09ms
step:72/500 train_loss:5.8450 train_time:16867ms step_avg:272.06ms
step:73/500 train_loss:5.7817 train_time:17141ms step_avg:272.08ms
step:74/500 train_loss:5.6751 train_time:17418ms step_avg:272.15ms
step:75/500 train_loss:5.7839 train_time:17693ms step_avg:272.21ms
step:76/500 train_loss:5.7454 train_time:17962ms step_avg:272.15ms
step:77/500 train_loss:5.7129 train_time:18235ms step_avg:272.16ms
step:78/500 train_loss:5.8011 train_time:18508ms step_avg:272.18ms
step:79/500 train_loss:5.8251 train_time:18781ms step_avg:272.18ms
step:80/500 train_loss:5.6802 train_time:19053ms step_avg:272.18ms
step:81/500 train_loss:5.7744 train_time:19326ms step_avg:272.19ms
step:82/500 train_loss:5.5354 train_time:19600ms step_avg:272.22ms
step:83/500 train_loss:5.7102 train_time:19870ms step_avg:272.19ms
step:84/500 train_loss:5.6772 train_time:20142ms step_avg:272.19ms
step:85/500 train_loss:5.6357 train_time:20414ms step_avg:272.19ms
step:86/500 train_loss:5.5055 train_time:20688ms step_avg:272.21ms
step:87/500 train_loss:5.7158 train_time:20956ms step_avg:272.15ms
step:88/500 train_loss:5.6184 train_time:21230ms step_avg:272.18ms
step:89/500 train_loss:5.6842 train_time:21501ms step_avg:272.16ms
step:90/500 train_loss:5.6592 train_time:21774ms step_avg:272.17ms
step:91/500 train_loss:5.5690 train_time:22043ms step_avg:272.14ms
step:92/500 train_loss:5.5775 train_time:22317ms step_avg:272.16ms
step:93/500 train_loss:5.6801 train_time:22591ms step_avg:272.19ms
step:94/500 train_loss:5.5265 train_time:22861ms step_avg:272.16ms
step:95/500 train_loss:5.5148 train_time:23135ms step_avg:272.18ms
step:96/500 train_loss:5.5384 train_time:23408ms step_avg:272.19ms
step:97/500 train_loss:5.4516 train_time:23679ms step_avg:272.18ms
step:98/500 train_loss:5.5284 train_time:23954ms step_avg:272.20ms
step:99/500 train_loss:5.4466 train_time:24226ms step_avg:272.21ms
step:100/500 train_loss:5.5729 train_time:24501ms step_avg:272.23ms
step:101/500 train_loss:5.5332 train_time:24772ms step_avg:272.22ms w_mean:1.000 w_std:0.061 w_min:0.990 w_max:2.474
step:102/500 train_loss:5.4313 train_time:25041ms step_avg:272.18ms
step:103/500 train_loss:5.5380 train_time:25314ms step_avg:272.19ms
step:104/500 train_loss:5.4991 train_time:25588ms step_avg:272.21ms
step:105/500 train_loss:5.3334 train_time:25859ms step_avg:272.20ms
step:106/500 train_loss:5.4440 train_time:26134ms step_avg:272.23ms
step:107/500 train_loss:5.6379 train_time:26405ms step_avg:272.22ms
step:108/500 train_loss:5.4288 train_time:26676ms step_avg:272.21ms
step:109/500 train_loss:5.1918 train_time:26952ms step_avg:272.24ms
step:110/500 train_loss:5.3944 train_time:27225ms step_avg:272.25ms
step:111/500 train_loss:5.3605 train_time:27497ms step_avg:272.25ms
step:112/500 train_loss:5.3393 train_time:27769ms step_avg:272.24ms
step:113/500 train_loss:5.4404 train_time:28039ms step_avg:272.22ms
step:114/500 train_loss:5.3723 train_time:28314ms step_avg:272.25ms
step:115/500 train_loss:5.2256 train_time:28586ms step_avg:272.25ms
step:116/500 train_loss:5.3967 train_time:28856ms step_avg:272.23ms
step:117/500 train_loss:5.2580 train_time:29128ms step_avg:272.23ms
step:118/500 train_loss:5.2488 train_time:29406ms step_avg:272.27ms
step:119/500 train_loss:5.3750 train_time:29676ms step_avg:272.25ms
step:120/500 train_loss:5.3528 train_time:29947ms step_avg:272.25ms
step:121/500 train_loss:5.2851 train_time:30221ms step_avg:272.27ms
step:122/500 train_loss:5.1776 train_time:30497ms step_avg:272.30ms
step:123/500 train_loss:5.2706 train_time:30768ms step_avg:272.28ms
step:124/500 train_loss:5.1421 train_time:31039ms step_avg:272.27ms
step:125/500 train_loss:5.4362 train_time:31316ms step_avg:272.31ms
step:125/500 val_loss:5.2649 train_time:31317ms step_avg:272.32ms
step:126/500 train_loss:5.2873 train_time:31589ms step_avg:272.32ms
step:127/500 train_loss:5.2634 train_time:31869ms step_avg:272.39ms
step:128/500 train_loss:5.3262 train_time:32144ms step_avg:272.40ms
step:129/500 train_loss:5.1879 train_time:32410ms step_avg:272.36ms
step:130/500 train_loss:5.4613 train_time:32682ms step_avg:272.35ms
step:131/500 train_loss:5.2468 train_time:32960ms step_avg:272.40ms
step:132/500 train_loss:5.2416 train_time:33235ms step_avg:272.42ms
step:133/500 train_loss:5.1931 train_time:33506ms step_avg:272.41ms
step:134/500 train_loss:5.2298 train_time:33779ms step_avg:272.41ms
step:135/500 train_loss:5.1611 train_time:34051ms step_avg:272.41ms
step:136/500 train_loss:5.2305 train_time:34324ms step_avg:272.41ms
step:137/500 train_loss:5.0321 train_time:34596ms step_avg:272.41ms
step:138/500 train_loss:5.1933 train_time:34868ms step_avg:272.41ms
step:139/500 train_loss:5.1488 train_time:35141ms step_avg:272.41ms
step:140/500 train_loss:5.1602 train_time:35410ms step_avg:272.39ms
step:141/500 train_loss:5.2100 train_time:35686ms step_avg:272.41ms
step:142/500 train_loss:5.1122 train_time:35959ms step_avg:272.42ms
step:143/500 train_loss:5.1909 train_time:36229ms step_avg:272.40ms
step:144/500 train_loss:5.0012 train_time:36502ms step_avg:272.40ms
step:145/500 train_loss:5.1562 train_time:36777ms step_avg:272.42ms
step:146/500 train_loss:5.1052 train_time:37049ms step_avg:272.42ms
step:147/500 train_loss:5.0105 train_time:37324ms step_avg:272.43ms
step:148/500 train_loss:5.1298 train_time:37598ms step_avg:272.45ms
step:149/500 train_loss:5.1055 train_time:37870ms step_avg:272.45ms
step:150/500 train_loss:5.1614 train_time:38142ms step_avg:272.44ms
step:151/500 train_loss:5.1786 train_time:38412ms step_avg:272.42ms w_mean:1.000 w_std:0.065 w_min:0.989 w_max:2.473
step:152/500 train_loss:5.0959 train_time:38688ms step_avg:272.45ms
step:153/500 train_loss:5.0763 train_time:38964ms step_avg:272.48ms
step:154/500 train_loss:5.1514 train_time:39234ms step_avg:272.46ms
step:155/500 train_loss:5.0892 train_time:39507ms step_avg:272.46ms
step:156/500 train_loss:5.0735 train_time:39781ms step_avg:272.47ms
step:157/500 train_loss:5.0856 train_time:40052ms step_avg:272.46ms
step:158/500 train_loss:5.2093 train_time:40325ms step_avg:272.47ms
step:159/500 train_loss:4.9954 train_time:40599ms step_avg:272.47ms
step:160/500 train_loss:5.0523 train_time:40871ms step_avg:272.47ms
step:161/500 train_loss:4.9162 train_time:41142ms step_avg:272.46ms
step:162/500 train_loss:5.0621 train_time:41413ms step_avg:272.45ms
step:163/500 train_loss:5.0988 train_time:41686ms step_avg:272.46ms
step:164/500 train_loss:5.0877 train_time:41959ms step_avg:272.46ms
step:165/500 train_loss:4.9110 train_time:42229ms step_avg:272.45ms
step:166/500 train_loss:5.0325 train_time:42504ms step_avg:272.46ms
step:167/500 train_loss:5.1861 train_time:42779ms step_avg:272.48ms
step:168/500 train_loss:4.9625 train_time:43050ms step_avg:272.47ms
step:169/500 train_loss:5.0431 train_time:43323ms step_avg:272.47ms
step:170/500 train_loss:4.9115 train_time:43598ms step_avg:272.49ms
step:171/500 train_loss:4.8522 train_time:43871ms step_avg:272.49ms
step:172/500 train_loss:4.9599 train_time:44143ms step_avg:272.49ms
step:173/500 train_loss:4.9378 train_time:44413ms step_avg:272.47ms
step:174/500 train_loss:4.9879 train_time:44686ms step_avg:272.48ms
step:175/500 train_loss:5.1318 train_time:44960ms step_avg:272.49ms
step:176/500 train_loss:5.0124 train_time:45229ms step_avg:272.46ms
step:177/500 train_loss:4.8497 train_time:45503ms step_avg:272.48ms
step:178/500 train_loss:4.8339 train_time:45778ms step_avg:272.49ms
step:179/500 train_loss:4.8724 train_time:46049ms step_avg:272.48ms
step:180/500 train_loss:4.9173 train_time:46324ms step_avg:272.49ms
step:181/500 train_loss:4.9016 train_time:46594ms step_avg:272.48ms
step:182/500 train_loss:5.0142 train_time:46868ms step_avg:272.49ms
step:183/500 train_loss:4.9052 train_time:47140ms step_avg:272.48ms
step:184/500 train_loss:4.8305 train_time:47410ms step_avg:272.47ms
step:185/500 train_loss:4.8578 train_time:47685ms step_avg:272.49ms
step:186/500 train_loss:4.9840 train_time:47960ms step_avg:272.50ms
step:187/500 train_loss:4.8694 train_time:48232ms step_avg:272.49ms
step:188/500 train_loss:5.1097 train_time:48506ms step_avg:272.51ms
step:189/500 train_loss:4.9022 train_time:49044ms step_avg:273.99ms
step:190/500 train_loss:4.8223 train_time:49596ms step_avg:275.53ms
step:191/500 train_loss:4.9810 train_time:49865ms step_avg:275.50ms
step:192/500 train_loss:4.8191 train_time:50134ms step_avg:275.46ms
step:193/500 train_loss:4.7428 train_time:50406ms step_avg:275.44ms
step:194/500 train_loss:4.9414 train_time:50680ms step_avg:275.44ms
step:195/500 train_loss:4.8817 train_time:50951ms step_avg:275.41ms
step:196/500 train_loss:5.0647 train_time:51223ms step_avg:275.39ms
step:197/500 train_loss:4.9599 train_time:51492ms step_avg:275.36ms
step:198/500 train_loss:4.7953 train_time:51768ms step_avg:275.36ms
step:199/500 train_loss:4.8397 train_time:52039ms step_avg:275.34ms
step:200/500 train_loss:4.7332 train_time:52309ms step_avg:275.31ms
step:201/500 train_loss:4.8191 train_time:52581ms step_avg:275.29ms w_mean:1.000 w_std:0.052 w_min:0.993 w_max:2.483
step:202/500 train_loss:4.7391 train_time:52851ms step_avg:275.27ms
step:203/500 train_loss:4.9644 train_time:53127ms step_avg:275.27ms
step:204/500 train_loss:4.8702 train_time:53397ms step_avg:275.24ms
step:205/500 train_loss:4.8344 train_time:53670ms step_avg:275.23ms
step:206/500 train_loss:4.9901 train_time:53942ms step_avg:275.21ms
step:207/500 train_loss:4.6684 train_time:54212ms step_avg:275.19ms
step:208/500 train_loss:4.8142 train_time:54487ms step_avg:275.19ms
step:209/500 train_loss:4.7654 train_time:54761ms step_avg:275.18ms
step:210/500 train_loss:4.9343 train_time:55028ms step_avg:275.14ms
step:211/500 train_loss:4.8549 train_time:55303ms step_avg:275.14ms
step:212/500 train_loss:4.7485 train_time:55577ms step_avg:275.13ms
step:213/500 train_loss:4.8842 train_time:55849ms step_avg:275.12ms
step:214/500 train_loss:4.7176 train_time:56120ms step_avg:275.10ms
step:215/500 train_loss:4.8104 train_time:56391ms step_avg:275.08ms
step:216/500 train_loss:4.6603 train_time:56667ms step_avg:275.08ms
step:217/500 train_loss:4.7821 train_time:56936ms step_avg:275.06ms
step:218/500 train_loss:4.7640 train_time:57208ms step_avg:275.04ms
step:219/500 train_loss:4.7393 train_time:57485ms step_avg:275.05ms
step:220/500 train_loss:4.7545 train_time:57759ms step_avg:275.04ms
step:221/500 train_loss:4.7794 train_time:58029ms step_avg:275.02ms
step:222/500 train_loss:4.8139 train_time:58303ms step_avg:275.01ms
step:223/500 train_loss:4.7539 train_time:58577ms step_avg:275.01ms
step:224/500 train_loss:4.7621 train_time:58850ms step_avg:275.00ms
step:225/500 train_loss:4.8811 train_time:59122ms step_avg:274.98ms
step:226/500 train_loss:4.6219 train_time:59391ms step_avg:274.96ms
step:227/500 train_loss:4.6571 train_time:59668ms step_avg:274.97ms
step:228/500 train_loss:4.6506 train_time:59937ms step_avg:274.94ms
step:229/500 train_loss:4.8100 train_time:60211ms step_avg:274.93ms
step:230/500 train_loss:4.6426 train_time:60483ms step_avg:274.92ms
step:231/500 train_loss:4.7959 train_time:60756ms step_avg:274.91ms
step:232/500 train_loss:4.6558 train_time:61027ms step_avg:274.90ms
step:233/500 train_loss:4.6167 train_time:61299ms step_avg:274.88ms
step:234/500 train_loss:4.8242 train_time:61571ms step_avg:274.87ms
step:235/500 train_loss:4.6588 train_time:61843ms step_avg:274.86ms
step:236/500 train_loss:4.6005 train_time:62117ms step_avg:274.85ms
step:237/500 train_loss:4.8505 train_time:62390ms step_avg:274.85ms
step:238/500 train_loss:4.7284 train_time:62666ms step_avg:274.85ms
step:239/500 train_loss:4.6414 train_time:62935ms step_avg:274.83ms
step:240/500 train_loss:4.7831 train_time:63209ms step_avg:274.82ms
step:241/500 train_loss:4.7639 train_time:63485ms step_avg:274.83ms
step:242/500 train_loss:4.6755 train_time:63759ms step_avg:274.82ms
step:243/500 train_loss:4.8286 train_time:64029ms step_avg:274.80ms
step:244/500 train_loss:4.6658 train_time:64302ms step_avg:274.80ms
step:245/500 train_loss:4.6717 train_time:64575ms step_avg:274.79ms
step:246/500 train_loss:4.7464 train_time:64846ms step_avg:274.77ms
step:247/500 train_loss:4.7011 train_time:65117ms step_avg:274.76ms
step:248/500 train_loss:4.6585 train_time:65391ms step_avg:274.75ms
step:249/500 train_loss:4.8208 train_time:65666ms step_avg:274.75ms
step:250/500 train_loss:4.5639 train_time:65937ms step_avg:274.74ms
step:250/500 val_loss:4.6684 train_time:65938ms step_avg:274.74ms
step:251/500 train_loss:4.6080 train_time:66214ms step_avg:274.75ms w_mean:1.000 w_std:0.066 w_min:0.990 w_max:2.475
step:252/500 train_loss:4.7368 train_time:66494ms step_avg:274.77ms
step:253/500 train_loss:4.7275 train_time:66768ms step_avg:274.77ms
step:254/500 train_loss:4.6029 train_time:67039ms step_avg:274.75ms
step:255/500 train_loss:4.6285 train_time:67308ms step_avg:274.73ms
step:256/500 train_loss:4.7590 train_time:67588ms step_avg:274.75ms
step:257/500 train_loss:4.7017 train_time:67864ms step_avg:274.75ms
step:258/500 train_loss:4.6739 train_time:68134ms step_avg:274.73ms
step:259/500 train_loss:4.6053 train_time:68406ms step_avg:274.72ms
step:260/500 train_loss:4.6177 train_time:68681ms step_avg:274.72ms
step:261/500 train_loss:4.6916 train_time:68953ms step_avg:274.71ms
step:262/500 train_loss:4.6961 train_time:69227ms step_avg:274.71ms
step:263/500 train_loss:4.6056 train_time:69497ms step_avg:274.69ms
step:264/500 train_loss:4.5480 train_time:69772ms step_avg:274.69ms
step:265/500 train_loss:4.6094 train_time:70047ms step_avg:274.69ms
step:266/500 train_loss:4.4599 train_time:70318ms step_avg:274.68ms
step:267/500 train_loss:4.5205 train_time:70590ms step_avg:274.67ms
step:268/500 train_loss:4.5636 train_time:70868ms step_avg:274.68ms
step:269/500 train_loss:4.5181 train_time:71141ms step_avg:274.68ms
step:270/500 train_loss:4.4870 train_time:71409ms step_avg:274.65ms
step:271/500 train_loss:4.7073 train_time:71685ms step_avg:274.65ms
step:272/500 train_loss:4.6467 train_time:71960ms step_avg:274.66ms
step:273/500 train_loss:4.5028 train_time:72229ms step_avg:274.64ms
step:274/500 train_loss:4.5484 train_time:72503ms step_avg:274.63ms
step:275/500 train_loss:4.6712 train_time:72777ms step_avg:274.63ms
step:276/500 train_loss:4.6784 train_time:73051ms step_avg:274.63ms
step:277/500 train_loss:4.8804 train_time:73322ms step_avg:274.61ms
step:278/500 train_loss:4.6236 train_time:73591ms step_avg:274.59ms
step:279/500 train_loss:4.7597 train_time:73866ms step_avg:274.60ms
step:280/500 train_loss:4.6079 train_time:74140ms step_avg:274.59ms
step:281/500 train_loss:4.6664 train_time:74409ms step_avg:274.57ms
step:282/500 train_loss:4.5656 train_time:74686ms step_avg:274.58ms
step:283/500 train_loss:4.6830 train_time:74960ms step_avg:274.58ms
step:284/500 train_loss:4.5036 train_time:75230ms step_avg:274.56ms
step:285/500 train_loss:4.6682 train_time:75505ms step_avg:274.56ms
step:286/500 train_loss:4.6523 train_time:75777ms step_avg:274.56ms
step:287/500 train_loss:4.6830 train_time:76052ms step_avg:274.55ms
step:288/500 train_loss:4.5520 train_time:76323ms step_avg:274.54ms
step:289/500 train_loss:4.6125 train_time:76594ms step_avg:274.53ms
step:290/500 train_loss:4.4757 train_time:76870ms step_avg:274.54ms
step:291/500 train_loss:4.4743 train_time:77147ms step_avg:274.54ms
step:292/500 train_loss:4.5986 train_time:77417ms step_avg:274.53ms
step:293/500 train_loss:4.4826 train_time:77688ms step_avg:274.52ms
step:294/500 train_loss:4.5361 train_time:77963ms step_avg:274.52ms
step:295/500 train_loss:4.5567 train_time:78237ms step_avg:274.52ms
step:296/500 train_loss:4.4221 train_time:78508ms step_avg:274.50ms
step:297/500 train_loss:4.4133 train_time:78787ms step_avg:274.52ms
step:298/500 train_loss:4.4391 train_time:79060ms step_avg:274.52ms
step:299/500 train_loss:4.5489 train_time:79329ms step_avg:274.49ms
step:300/500 train_loss:4.4316 train_time:79603ms step_avg:274.49ms
step:301/500 train_loss:4.6086 train_time:79876ms step_avg:274.49ms w_mean:1.000 w_std:0.047 w_min:0.994 w_max:2.485
step:302/500 train_loss:4.5833 train_time:80150ms step_avg:274.49ms
step:303/500 train_loss:4.5076 train_time:80421ms step_avg:274.47ms
step:304/500 train_loss:4.5717 train_time:80692ms step_avg:274.46ms
step:305/500 train_loss:4.5519 train_time:80967ms step_avg:274.46ms
step:306/500 train_loss:5.0201 train_time:81241ms step_avg:274.46ms
step:307/500 train_loss:4.5130 train_time:81510ms step_avg:274.45ms
step:308/500 train_loss:4.4130 train_time:81787ms step_avg:274.45ms
step:309/500 train_loss:4.6029 train_time:82061ms step_avg:274.45ms
step:310/500 train_loss:4.4024 train_time:82332ms step_avg:274.44ms
step:311/500 train_loss:4.6382 train_time:82611ms step_avg:274.46ms
step:312/500 train_loss:4.5473 train_time:82884ms step_avg:274.45ms
step:313/500 train_loss:4.4632 train_time:83156ms step_avg:274.44ms
step:314/500 train_loss:4.5866 train_time:83430ms step_avg:274.44ms
step:315/500 train_loss:4.7199 train_time:83704ms step_avg:274.44ms
step:316/500 train_loss:4.5536 train_time:83977ms step_avg:274.44ms
step:317/500 train_loss:4.4427 train_time:84251ms step_avg:274.43ms
step:318/500 train_loss:4.4577 train_time:84524ms step_avg:274.43ms
step:319/500 train_loss:4.4739 train_time:84799ms step_avg:274.43ms
step:320/500 train_loss:4.4208 train_time:85073ms step_avg:274.43ms
step:321/500 train_loss:4.5149 train_time:85347ms step_avg:274.43ms
step:322/500 train_loss:4.5289 train_time:85620ms step_avg:274.42ms
step:323/500 train_loss:4.4888 train_time:85890ms step_avg:274.41ms
step:324/500 train_loss:4.5602 train_time:86167ms step_avg:274.42ms
step:325/500 train_loss:4.5534 train_time:86444ms step_avg:274.43ms
step:326/500 train_loss:4.6278 train_time:86715ms step_avg:274.41ms
step:327/500 train_loss:4.4785 train_time:86988ms step_avg:274.41ms
step:328/500 train_loss:4.9115 train_time:87263ms step_avg:274.41ms
step:329/500 train_loss:4.6285 train_time:87533ms step_avg:274.40ms
step:330/500 train_loss:4.4169 train_time:87806ms step_avg:274.39ms
step:331/500 train_loss:4.3923 train_time:88078ms step_avg:274.39ms
step:332/500 train_loss:4.5329 train_time:88351ms step_avg:274.38ms
step:333/500 train_loss:4.4520 train_time:88624ms step_avg:274.38ms
step:334/500 train_loss:4.4432 train_time:88894ms step_avg:274.36ms
step:335/500 train_loss:4.3995 train_time:89168ms step_avg:274.36ms
step:336/500 train_loss:4.5979 train_time:89443ms step_avg:274.37ms
step:337/500 train_loss:4.5315 train_time:89714ms step_avg:274.35ms
step:338/500 train_loss:5.0737 train_time:89988ms step_avg:274.35ms
step:339/500 train_loss:4.5053 train_time:90261ms step_avg:274.35ms
step:340/500 train_loss:4.4787 train_time:90529ms step_avg:274.33ms
step:341/500 train_loss:4.4514 train_time:90804ms step_avg:274.33ms
step:342/500 train_loss:4.3965 train_time:91078ms step_avg:274.33ms
step:343/500 train_loss:4.3713 train_time:91352ms step_avg:274.33ms
step:344/500 train_loss:4.4371 train_time:91622ms step_avg:274.32ms
step:345/500 train_loss:4.5295 train_time:91891ms step_avg:274.30ms
step:346/500 train_loss:4.4232 train_time:92167ms step_avg:274.31ms
step:347/500 train_loss:4.3655 train_time:92441ms step_avg:274.31ms
step:348/500 train_loss:4.4230 train_time:92711ms step_avg:274.29ms
step:349/500 train_loss:4.4186 train_time:92986ms step_avg:274.29ms
step:350/500 train_loss:4.3422 train_time:93259ms step_avg:274.29ms
step:351/500 train_loss:4.0288 train_time:93530ms step_avg:274.28ms w_mean:1.000 w_std:0.052 w_min:0.994 w_max:2.485
step:352/500 train_loss:4.3196 train_time:93803ms step_avg:274.28ms
step:353/500 train_loss:4.6665 train_time:94070ms step_avg:274.26ms
step:354/500 train_loss:4.2080 train_time:94347ms step_avg:274.27ms
step:355/500 train_loss:4.4553 train_time:94619ms step_avg:274.26ms
step:356/500 train_loss:4.3609 train_time:94892ms step_avg:274.25ms
step:357/500 train_loss:4.4563 train_time:95164ms step_avg:274.25ms
step:358/500 train_loss:4.4671 train_time:95436ms step_avg:274.24ms
step:359/500 train_loss:4.3684 train_time:95706ms step_avg:274.23ms
step:360/500 train_loss:4.7005 train_time:95979ms step_avg:274.23ms
step:361/500 train_loss:4.1041 train_time:96251ms step_avg:274.22ms
step:362/500 train_loss:4.5866 train_time:96523ms step_avg:274.21ms
step:363/500 train_loss:4.4887 train_time:96792ms step_avg:274.20ms
step:364/500 train_loss:4.3687 train_time:97068ms step_avg:274.20ms
step:365/500 train_loss:4.3048 train_time:97342ms step_avg:274.20ms
step:366/500 train_loss:4.4591 train_time:97612ms step_avg:274.19ms
step:367/500 train_loss:4.3871 train_time:97883ms step_avg:274.18ms
step:368/500 train_loss:4.3732 train_time:98155ms step_avg:274.18ms
step:369/500 train_loss:4.3808 train_time:98427ms step_avg:274.17ms
step:370/500 train_loss:4.2710 train_time:98698ms step_avg:274.16ms
step:371/500 train_loss:4.4220 train_time:98971ms step_avg:274.16ms
step:372/500 train_loss:4.3575 train_time:99246ms step_avg:274.16ms
step:373/500 train_loss:4.2367 train_time:99517ms step_avg:274.15ms
step:374/500 train_loss:4.4279 train_time:99790ms step_avg:274.15ms
step:375/500 train_loss:4.3578 train_time:100065ms step_avg:274.15ms
step:375/500 val_loss:4.3742 train_time:100066ms step_avg:274.15ms
step:376/500 train_loss:4.3553 train_time:100342ms step_avg:274.16ms
step:377/500 train_loss:4.4106 train_time:100621ms step_avg:274.17ms
step:378/500 train_loss:4.3046 train_time:101150ms step_avg:274.86ms
step:379/500 train_loss:4.3583 train_time:101422ms step_avg:274.86ms
step:380/500 train_loss:4.4330 train_time:101960ms step_avg:275.57ms
step:381/500 train_loss:4.4665 train_time:102226ms step_avg:275.54ms
step:382/500 train_loss:4.4084 train_time:102496ms step_avg:275.53ms
step:383/500 train_loss:4.3886 train_time:102764ms step_avg:275.51ms
step:384/500 train_loss:4.2865 train_time:103044ms step_avg:275.52ms
step:385/500 train_loss:4.3906 train_time:103317ms step_avg:275.51ms
step:386/500 train_loss:4.3058 train_time:103586ms step_avg:275.49ms
step:387/500 train_loss:4.4361 train_time:103856ms step_avg:275.48ms
step:388/500 train_loss:4.6299 train_time:104129ms step_avg:275.47ms
step:389/500 train_loss:4.3258 train_time:104402ms step_avg:275.47ms
step:390/500 train_loss:4.2865 train_time:104673ms step_avg:275.46ms
step:391/500 train_loss:4.4145 train_time:104945ms step_avg:275.45ms
step:392/500 train_loss:4.3390 train_time:105220ms step_avg:275.45ms
step:393/500 train_loss:4.4392 train_time:105490ms step_avg:275.43ms
step:394/500 train_loss:4.2652 train_time:105763ms step_avg:275.42ms
step:395/500 train_loss:4.3980 train_time:106035ms step_avg:275.42ms
step:396/500 train_loss:4.1830 train_time:106307ms step_avg:275.41ms
step:397/500 train_loss:4.3435 train_time:106583ms step_avg:275.41ms
step:398/500 train_loss:4.4392 train_time:106854ms step_avg:275.40ms
step:399/500 train_loss:4.3989 train_time:107127ms step_avg:275.39ms
step:400/500 train_loss:4.3059 train_time:107398ms step_avg:275.38ms
step:401/500 train_loss:4.3791 train_time:107670ms step_avg:275.37ms w_mean:1.000 w_std:0.031 w_min:0.998 w_max:2.494
step:402/500 train_loss:4.4103 train_time:107944ms step_avg:275.37ms
step:403/500 train_loss:4.3779 train_time:108219ms step_avg:275.37ms
step:404/500 train_loss:4.4734 train_time:108486ms step_avg:275.35ms
step:405/500 train_loss:4.2587 train_time:108762ms step_avg:275.35ms
step:406/500 train_loss:4.3047 train_time:109035ms step_avg:275.34ms
step:407/500 train_loss:4.5785 train_time:109307ms step_avg:275.33ms
step:408/500 train_loss:4.3421 train_time:109581ms step_avg:275.33ms
step:409/500 train_loss:4.3377 train_time:109851ms step_avg:275.32ms
step:410/500 train_loss:4.3860 train_time:110126ms step_avg:275.32ms
step:411/500 train_loss:4.2704 train_time:110399ms step_avg:275.31ms
step:412/500 train_loss:4.2886 train_time:110669ms step_avg:275.29ms
step:413/500 train_loss:4.7082 train_time:110945ms step_avg:275.30ms
step:414/500 train_loss:4.1536 train_time:111219ms step_avg:275.29ms
step:415/500 train_loss:4.5154 train_time:111488ms step_avg:275.28ms
step:416/500 train_loss:4.2966 train_time:111762ms step_avg:275.27ms
step:417/500 train_loss:4.2875 train_time:112038ms step_avg:275.28ms
step:418/500 train_loss:4.4697 train_time:112308ms step_avg:275.26ms
step:419/500 train_loss:4.2035 train_time:112581ms step_avg:275.26ms
step:420/500 train_loss:4.3049 train_time:112850ms step_avg:275.24ms
step:421/500 train_loss:4.2773 train_time:113124ms step_avg:275.24ms
step:422/500 train_loss:4.1601 train_time:113396ms step_avg:275.23ms
step:423/500 train_loss:4.2681 train_time:113667ms step_avg:275.22ms
step:424/500 train_loss:4.3811 train_time:113941ms step_avg:275.22ms
step:425/500 train_loss:4.1895 train_time:114214ms step_avg:275.21ms
step:426/500 train_loss:4.3430 train_time:114484ms step_avg:275.20ms
step:427/500 train_loss:4.2324 train_time:114757ms step_avg:275.20ms
step:428/500 train_loss:4.4065 train_time:115030ms step_avg:275.19ms
step:429/500 train_loss:4.3566 train_time:115302ms step_avg:275.18ms
step:430/500 train_loss:4.2699 train_time:115571ms step_avg:275.17ms
step:431/500 train_loss:4.2471 train_time:115845ms step_avg:275.17ms
step:432/500 train_loss:4.2054 train_time:116119ms step_avg:275.16ms
step:433/500 train_loss:4.2749 train_time:116388ms step_avg:275.15ms
step:434/500 train_loss:4.3512 train_time:116661ms step_avg:275.14ms
step:435/500 train_loss:4.2885 train_time:116936ms step_avg:275.14ms
step:436/500 train_loss:4.3291 train_time:117207ms step_avg:275.13ms
step:437/500 train_loss:4.3472 train_time:117482ms step_avg:275.13ms
step:438/500 train_loss:4.2331 train_time:117752ms step_avg:275.12ms
step:439/500 train_loss:4.2454 train_time:118025ms step_avg:275.12ms
step:440/500 train_loss:4.2115 train_time:118297ms step_avg:275.11ms
step:441/500 train_loss:4.4004 train_time:118568ms step_avg:275.10ms
step:442/500 train_loss:4.3021 train_time:118842ms step_avg:275.10ms
step:443/500 train_loss:4.2752 train_time:119115ms step_avg:275.09ms
step:444/500 train_loss:4.1656 train_time:119386ms step_avg:275.08ms
step:445/500 train_loss:4.4301 train_time:119658ms step_avg:275.08ms
step:446/500 train_loss:4.3457 train_time:119933ms step_avg:275.08ms
step:447/500 train_loss:4.3470 train_time:120205ms step_avg:275.07ms
step:448/500 train_loss:4.2672 train_time:120478ms step_avg:275.06ms
step:449/500 train_loss:4.3482 train_time:120749ms step_avg:275.05ms
step:450/500 train_loss:4.1872 train_time:121024ms step_avg:275.06ms
step:451/500 train_loss:4.2256 train_time:121295ms step_avg:275.05ms w_mean:1.000 w_std:0.030 w_min:0.997 w_max:2.494
step:452/500 train_loss:4.1179 train_time:121566ms step_avg:275.04ms
step:453/500 train_loss:4.2160 train_time:121840ms step_avg:275.03ms
step:454/500 train_loss:4.1954 train_time:122110ms step_avg:275.02ms
step:455/500 train_loss:4.1708 train_time:122383ms step_avg:275.02ms
step:456/500 train_loss:4.3775 train_time:122653ms step_avg:275.01ms
step:457/500 train_loss:4.2315 train_time:122927ms step_avg:275.00ms
step:458/500 train_loss:4.3213 train_time:123198ms step_avg:275.00ms
step:459/500 train_loss:4.3546 train_time:123467ms step_avg:274.98ms
step:460/500 train_loss:4.1538 train_time:123741ms step_avg:274.98ms
step:461/500 train_loss:4.3282 train_time:124012ms step_avg:274.97ms
step:462/500 train_loss:4.2325 train_time:124283ms step_avg:274.96ms
step:463/500 train_loss:4.2145 train_time:124558ms step_avg:274.96ms
step:464/500 train_loss:4.3011 train_time:124828ms step_avg:274.95ms
step:465/500 train_loss:4.2408 train_time:125102ms step_avg:274.95ms
step:466/500 train_loss:4.2379 train_time:125371ms step_avg:274.94ms
step:467/500 train_loss:4.3620 train_time:125645ms step_avg:274.93ms
step:468/500 train_loss:4.3755 train_time:125919ms step_avg:274.93ms
step:469/500 train_loss:4.3314 train_time:126188ms step_avg:274.92ms
step:470/500 train_loss:4.2423 train_time:126462ms step_avg:274.92ms
step:471/500 train_loss:4.3268 train_time:126733ms step_avg:274.91ms
step:472/500 train_loss:4.3750 train_time:127006ms step_avg:274.90ms
step:473/500 train_loss:4.2800 train_time:127277ms step_avg:274.90ms
step:474/500 train_loss:4.2595 train_time:127550ms step_avg:274.89ms
step:475/500 train_loss:4.1272 train_time:127824ms step_avg:274.89ms
step:476/500 train_loss:4.5585 train_time:128095ms step_avg:274.88ms
step:477/500 train_loss:4.3085 train_time:128367ms step_avg:274.88ms
step:478/500 train_loss:4.1201 train_time:128641ms step_avg:274.87ms
step:479/500 train_loss:4.3107 train_time:128909ms step_avg:274.86ms
step:480/500 train_loss:4.2977 train_time:129183ms step_avg:274.86ms
step:481/500 train_loss:4.4212 train_time:129455ms step_avg:274.85ms
step:482/500 train_loss:4.2551 train_time:129728ms step_avg:274.85ms
step:483/500 train_loss:4.0677 train_time:130000ms step_avg:274.84ms
step:484/500 train_loss:4.3440 train_time:130268ms step_avg:274.83ms
step:485/500 train_loss:4.1901 train_time:130543ms step_avg:274.83ms
step:486/500 train_loss:4.2212 train_time:130817ms step_avg:274.82ms
step:487/500 train_loss:4.1684 train_time:131087ms step_avg:274.82ms
step:488/500 train_loss:4.1866 train_time:131360ms step_avg:274.81ms
step:489/500 train_loss:4.4010 train_time:131633ms step_avg:274.81ms
step:490/500 train_loss:4.2554 train_time:131905ms step_avg:274.80ms
step:491/500 train_loss:4.1586 train_time:132178ms step_avg:274.80ms
step:492/500 train_loss:4.1624 train_time:132447ms step_avg:274.79ms
step:493/500 train_loss:4.2707 train_time:132723ms step_avg:274.79ms
step:494/500 train_loss:4.1159 train_time:132994ms step_avg:274.78ms
step:495/500 train_loss:4.2711 train_time:133267ms step_avg:274.78ms
step:496/500 train_loss:4.1830 train_time:133538ms step_avg:274.77ms
step:497/500 train_loss:4.1305 train_time:133808ms step_avg:274.76ms
step:498/500 train_loss:4.2758 train_time:134084ms step_avg:274.76ms
step:499/500 train_loss:4.3631 train_time:134356ms step_avg:274.76ms
step:500/500 train_loss:4.4140 train_time:134627ms step_avg:274.75ms
step:500/500 val_loss:4.2596 train_time:134628ms step_avg:274.75ms
