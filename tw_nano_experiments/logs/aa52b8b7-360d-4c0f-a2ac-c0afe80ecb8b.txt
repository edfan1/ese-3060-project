====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # also save the seed for reproducibility
        if args.seed is not None:
            log['seed'] = args.seed
        # save token weighting config
        if tw_config.enabled:
            log['tw_config'] = {
                'function': tw_config.function,
                'clamp_min': tw_config.clamp_min,
                'clamp_max': tw_config.clamp_max,
                'schedule': tw_config.schedule,
            }
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 21:13:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |     30%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     19%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   49C    P0             88W /  310W |    2363MiB /  81559MiB |     37%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             86W /  310W |    2363MiB /  81559MiB |     33%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   43C    P0             81W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   48C    P0             86W /  310W |    2363MiB /  81559MiB |     35%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   52C    P0             88W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   44C    P0             83W /  310W |    2363MiB /  81559MiB |      7%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           58360      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           58361      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           58362      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           58363      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           58364      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           58365      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           58366      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           58367      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 456 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: linear
  clamp: [0.5, 2.0]
  schedule: constant
====================================================================================================
step:0/500 val_loss:16.0011 train_time:284ms step_avg:nanms
step:1/500 train_loss:15.9963 train_time:61526ms step_avg:nanms w_mean:1.000 w_std:0.041 w_min:0.500 w_max:1.173
step:2/500 train_loss:9.3794 train_time:61798ms step_avg:nanms
step:3/500 train_loss:9.0266 train_time:62063ms step_avg:nanms
step:4/500 train_loss:8.7459 train_time:62330ms step_avg:nanms
step:5/500 train_loss:8.2197 train_time:62602ms step_avg:nanms
step:6/500 train_loss:7.7201 train_time:62871ms step_avg:nanms
step:7/500 train_loss:7.4747 train_time:63141ms step_avg:nanms
step:8/500 train_loss:7.5925 train_time:63408ms step_avg:nanms
step:9/500 train_loss:7.3602 train_time:63679ms step_avg:nanms
step:10/500 train_loss:7.1009 train_time:63948ms step_avg:nanms
step:11/500 train_loss:7.0948 train_time:271ms step_avg:nanms
step:12/500 train_loss:7.0103 train_time:539ms step_avg:nanms
step:13/500 train_loss:6.8883 train_time:811ms step_avg:270.25ms
step:14/500 train_loss:6.8443 train_time:1082ms step_avg:270.38ms
step:15/500 train_loss:6.8641 train_time:1350ms step_avg:270.03ms
step:16/500 train_loss:6.7513 train_time:1618ms step_avg:269.66ms
step:17/500 train_loss:6.8044 train_time:1889ms step_avg:269.82ms
step:18/500 train_loss:6.7804 train_time:2161ms step_avg:270.09ms
step:19/500 train_loss:6.6517 train_time:2432ms step_avg:270.28ms
step:20/500 train_loss:6.6346 train_time:2703ms step_avg:270.29ms
step:21/500 train_loss:6.3130 train_time:2973ms step_avg:270.26ms
step:22/500 train_loss:6.6970 train_time:3243ms step_avg:270.21ms
step:23/500 train_loss:6.9092 train_time:3513ms step_avg:270.20ms
step:24/500 train_loss:6.5769 train_time:3786ms step_avg:270.42ms
step:25/500 train_loss:6.6573 train_time:4054ms step_avg:270.27ms
step:26/500 train_loss:6.4275 train_time:4325ms step_avg:270.30ms
step:27/500 train_loss:6.3176 train_time:4594ms step_avg:270.23ms
step:28/500 train_loss:6.5247 train_time:4869ms step_avg:270.49ms
step:29/500 train_loss:6.1509 train_time:5139ms step_avg:270.45ms
step:30/500 train_loss:6.4469 train_time:5410ms step_avg:270.51ms
step:31/500 train_loss:6.2872 train_time:5680ms step_avg:270.48ms
step:32/500 train_loss:6.2498 train_time:5950ms step_avg:270.46ms
step:33/500 train_loss:6.0803 train_time:6222ms step_avg:270.51ms
step:34/500 train_loss:6.4028 train_time:6492ms step_avg:270.49ms
step:35/500 train_loss:6.3074 train_time:6768ms step_avg:270.73ms
step:36/500 train_loss:6.4427 train_time:7036ms step_avg:270.60ms
step:37/500 train_loss:6.3969 train_time:7313ms step_avg:270.84ms
step:38/500 train_loss:6.2631 train_time:7586ms step_avg:270.94ms
step:39/500 train_loss:6.1726 train_time:7857ms step_avg:270.92ms
step:40/500 train_loss:6.2362 train_time:8130ms step_avg:270.98ms
step:41/500 train_loss:6.1211 train_time:8399ms step_avg:270.95ms
step:42/500 train_loss:6.1715 train_time:8673ms step_avg:271.02ms
step:43/500 train_loss:6.0292 train_time:8944ms step_avg:271.04ms
step:44/500 train_loss:6.1303 train_time:9217ms step_avg:271.10ms
step:45/500 train_loss:6.1019 train_time:9489ms step_avg:271.13ms
step:46/500 train_loss:6.2853 train_time:9760ms step_avg:271.12ms
step:47/500 train_loss:6.0985 train_time:10032ms step_avg:271.14ms
step:48/500 train_loss:5.9425 train_time:10308ms step_avg:271.27ms
step:49/500 train_loss:6.1805 train_time:10579ms step_avg:271.26ms
step:50/500 train_loss:6.0481 train_time:10850ms step_avg:271.24ms
step:51/500 train_loss:6.1857 train_time:11121ms step_avg:271.25ms w_mean:1.000 w_std:0.425 w_min:0.487 w_max:1.949
step:52/500 train_loss:6.0661 train_time:11392ms step_avg:271.24ms
step:53/500 train_loss:5.9228 train_time:11664ms step_avg:271.25ms
step:54/500 train_loss:6.0350 train_time:11933ms step_avg:271.21ms
step:55/500 train_loss:5.9752 train_time:12204ms step_avg:271.19ms
step:56/500 train_loss:6.2441 train_time:12477ms step_avg:271.23ms
step:57/500 train_loss:5.9823 train_time:12747ms step_avg:271.21ms
step:58/500 train_loss:5.8275 train_time:13018ms step_avg:271.21ms
step:59/500 train_loss:6.0190 train_time:13287ms step_avg:271.17ms
step:60/500 train_loss:5.9203 train_time:13560ms step_avg:271.20ms
step:61/500 train_loss:6.0471 train_time:13831ms step_avg:271.19ms
step:62/500 train_loss:5.8198 train_time:14102ms step_avg:271.20ms
step:63/500 train_loss:5.9463 train_time:14373ms step_avg:271.18ms
step:64/500 train_loss:5.8854 train_time:14645ms step_avg:271.20ms
step:65/500 train_loss:5.8715 train_time:14916ms step_avg:271.19ms
step:66/500 train_loss:5.7247 train_time:15187ms step_avg:271.19ms
step:67/500 train_loss:5.9185 train_time:15460ms step_avg:271.23ms
step:68/500 train_loss:5.7544 train_time:15731ms step_avg:271.22ms
step:69/500 train_loss:6.0078 train_time:16003ms step_avg:271.24ms
step:70/500 train_loss:5.6768 train_time:16273ms step_avg:271.22ms
step:71/500 train_loss:5.7092 train_time:16547ms step_avg:271.26ms
step:72/500 train_loss:5.8908 train_time:16819ms step_avg:271.28ms
step:73/500 train_loss:5.8530 train_time:17091ms step_avg:271.28ms
step:74/500 train_loss:5.7357 train_time:17363ms step_avg:271.29ms
step:75/500 train_loss:5.8533 train_time:17638ms step_avg:271.35ms
step:76/500 train_loss:5.8106 train_time:17907ms step_avg:271.32ms
step:77/500 train_loss:5.7768 train_time:18182ms step_avg:271.37ms
step:78/500 train_loss:5.8676 train_time:18453ms step_avg:271.37ms
step:79/500 train_loss:5.8565 train_time:18724ms step_avg:271.37ms
step:80/500 train_loss:5.7709 train_time:18994ms step_avg:271.35ms
step:81/500 train_loss:5.7964 train_time:19271ms step_avg:271.42ms
step:82/500 train_loss:5.6375 train_time:19541ms step_avg:271.40ms
step:83/500 train_loss:5.7602 train_time:19812ms step_avg:271.40ms
step:84/500 train_loss:5.7595 train_time:20085ms step_avg:271.41ms
step:85/500 train_loss:5.7009 train_time:20361ms step_avg:271.49ms
step:86/500 train_loss:5.5798 train_time:20632ms step_avg:271.48ms
step:87/500 train_loss:5.7786 train_time:20904ms step_avg:271.48ms
step:88/500 train_loss:5.6820 train_time:21175ms step_avg:271.48ms
step:89/500 train_loss:5.7418 train_time:21448ms step_avg:271.50ms
step:90/500 train_loss:5.7244 train_time:21721ms step_avg:271.51ms
step:91/500 train_loss:5.6381 train_time:21991ms step_avg:271.49ms
step:92/500 train_loss:5.6481 train_time:22264ms step_avg:271.51ms
step:93/500 train_loss:5.7389 train_time:22533ms step_avg:271.48ms
step:94/500 train_loss:5.5875 train_time:22804ms step_avg:271.48ms
step:95/500 train_loss:5.5878 train_time:23075ms step_avg:271.47ms
step:96/500 train_loss:5.5865 train_time:23348ms step_avg:271.49ms
step:97/500 train_loss:5.5490 train_time:23620ms step_avg:271.49ms
step:98/500 train_loss:5.5802 train_time:23890ms step_avg:271.48ms
step:99/500 train_loss:5.5393 train_time:24162ms step_avg:271.49ms
step:100/500 train_loss:5.6158 train_time:24433ms step_avg:271.48ms
step:101/500 train_loss:5.6210 train_time:24705ms step_avg:271.48ms w_mean:1.000 w_std:0.440 w_min:0.487 w_max:1.948
step:102/500 train_loss:5.4921 train_time:24977ms step_avg:271.49ms
step:103/500 train_loss:5.6246 train_time:25247ms step_avg:271.47ms
step:104/500 train_loss:5.5652 train_time:25521ms step_avg:271.50ms
step:105/500 train_loss:5.4203 train_time:25792ms step_avg:271.50ms
step:106/500 train_loss:5.5247 train_time:26065ms step_avg:271.52ms
step:107/500 train_loss:5.6805 train_time:26336ms step_avg:271.51ms
step:108/500 train_loss:5.5360 train_time:26609ms step_avg:271.52ms
step:109/500 train_loss:5.2517 train_time:26881ms step_avg:271.53ms
step:110/500 train_loss:5.5012 train_time:27153ms step_avg:271.53ms
step:111/500 train_loss:5.4253 train_time:27423ms step_avg:271.51ms
step:112/500 train_loss:5.4382 train_time:27696ms step_avg:271.53ms
step:113/500 train_loss:5.4967 train_time:27969ms step_avg:271.54ms
step:114/500 train_loss:5.4589 train_time:28240ms step_avg:271.53ms
step:115/500 train_loss:5.2969 train_time:28510ms step_avg:271.53ms
step:116/500 train_loss:5.4877 train_time:28786ms step_avg:271.56ms
step:117/500 train_loss:5.3201 train_time:29060ms step_avg:271.59ms
step:118/500 train_loss:5.3566 train_time:29329ms step_avg:271.57ms
step:119/500 train_loss:5.4187 train_time:29600ms step_avg:271.56ms
step:120/500 train_loss:5.4512 train_time:29873ms step_avg:271.57ms
step:121/500 train_loss:5.3472 train_time:30148ms step_avg:271.60ms
step:122/500 train_loss:5.2794 train_time:30420ms step_avg:271.60ms
step:123/500 train_loss:5.3286 train_time:30689ms step_avg:271.58ms
step:124/500 train_loss:5.2476 train_time:30963ms step_avg:271.60ms
step:125/500 train_loss:5.4940 train_time:31233ms step_avg:271.59ms
step:125/500 val_loss:5.3679 train_time:31234ms step_avg:271.60ms
step:126/500 train_loss:5.3838 train_time:31510ms step_avg:271.64ms
step:127/500 train_loss:5.3336 train_time:31786ms step_avg:271.67ms
step:128/500 train_loss:5.4302 train_time:32061ms step_avg:271.70ms
step:129/500 train_loss:5.2581 train_time:32330ms step_avg:271.68ms
step:130/500 train_loss:5.5436 train_time:32600ms step_avg:271.67ms
step:131/500 train_loss:5.3144 train_time:32874ms step_avg:271.69ms
step:132/500 train_loss:5.3399 train_time:33146ms step_avg:271.69ms
step:133/500 train_loss:5.2545 train_time:33415ms step_avg:271.67ms
step:134/500 train_loss:5.3208 train_time:33685ms step_avg:271.65ms
step:135/500 train_loss:5.2393 train_time:33959ms step_avg:271.67ms
step:136/500 train_loss:5.3146 train_time:34232ms step_avg:271.68ms
step:137/500 train_loss:5.1275 train_time:34503ms step_avg:271.67ms
step:138/500 train_loss:5.2500 train_time:34774ms step_avg:271.67ms
step:139/500 train_loss:5.2493 train_time:35050ms step_avg:271.71ms
step:140/500 train_loss:5.2254 train_time:35321ms step_avg:271.70ms
step:141/500 train_loss:5.2941 train_time:35592ms step_avg:271.69ms
step:142/500 train_loss:5.1876 train_time:35862ms step_avg:271.68ms
step:143/500 train_loss:5.2829 train_time:36136ms step_avg:271.70ms
step:144/500 train_loss:5.1031 train_time:36407ms step_avg:271.69ms
step:145/500 train_loss:5.2523 train_time:36679ms step_avg:271.69ms
step:146/500 train_loss:5.1756 train_time:36951ms step_avg:271.70ms
step:147/500 train_loss:5.1073 train_time:37222ms step_avg:271.70ms
step:148/500 train_loss:5.2007 train_time:37495ms step_avg:271.71ms
step:149/500 train_loss:5.1964 train_time:37768ms step_avg:271.71ms
step:150/500 train_loss:5.2345 train_time:38041ms step_avg:271.72ms
step:151/500 train_loss:5.2661 train_time:38312ms step_avg:271.71ms w_mean:1.000 w_std:0.459 w_min:0.486 w_max:1.945
step:152/500 train_loss:5.1854 train_time:38584ms step_avg:271.71ms
step:153/500 train_loss:5.1564 train_time:38857ms step_avg:271.73ms
step:154/500 train_loss:5.2358 train_time:39128ms step_avg:271.72ms
step:155/500 train_loss:5.1636 train_time:39399ms step_avg:271.72ms
step:156/500 train_loss:5.1716 train_time:39669ms step_avg:271.71ms
step:157/500 train_loss:5.1530 train_time:39944ms step_avg:271.73ms
step:158/500 train_loss:5.3069 train_time:40215ms step_avg:271.72ms
step:159/500 train_loss:5.0721 train_time:40485ms step_avg:271.71ms
step:160/500 train_loss:5.1504 train_time:40758ms step_avg:271.72ms
step:161/500 train_loss:4.9929 train_time:41030ms step_avg:271.72ms
step:162/500 train_loss:5.1568 train_time:41303ms step_avg:271.73ms
step:163/500 train_loss:5.1789 train_time:41573ms step_avg:271.72ms
step:164/500 train_loss:5.1663 train_time:41846ms step_avg:271.73ms
step:165/500 train_loss:5.0195 train_time:42118ms step_avg:271.73ms
step:166/500 train_loss:5.0932 train_time:42388ms step_avg:271.72ms
step:167/500 train_loss:5.2720 train_time:42663ms step_avg:271.74ms
step:168/500 train_loss:5.0403 train_time:42935ms step_avg:271.74ms
step:169/500 train_loss:5.1324 train_time:43206ms step_avg:271.74ms
step:170/500 train_loss:4.9945 train_time:43479ms step_avg:271.74ms
step:171/500 train_loss:4.9544 train_time:43752ms step_avg:271.75ms
step:172/500 train_loss:5.0510 train_time:44023ms step_avg:271.74ms
step:173/500 train_loss:5.0058 train_time:44294ms step_avg:271.74ms
step:174/500 train_loss:5.0996 train_time:44566ms step_avg:271.75ms
step:175/500 train_loss:5.1902 train_time:44840ms step_avg:271.76ms
step:176/500 train_loss:5.1206 train_time:45112ms step_avg:271.76ms
step:177/500 train_loss:4.9375 train_time:45383ms step_avg:271.76ms
step:178/500 train_loss:4.9363 train_time:45656ms step_avg:271.76ms
step:179/500 train_loss:4.9497 train_time:45927ms step_avg:271.76ms
step:180/500 train_loss:5.0050 train_time:46202ms step_avg:271.77ms
step:181/500 train_loss:4.9883 train_time:46473ms step_avg:271.77ms
step:182/500 train_loss:5.1034 train_time:46743ms step_avg:271.76ms
step:183/500 train_loss:4.9857 train_time:47016ms step_avg:271.77ms
step:184/500 train_loss:4.9365 train_time:47287ms step_avg:271.76ms
step:185/500 train_loss:4.9411 train_time:47560ms step_avg:271.77ms
step:186/500 train_loss:5.0912 train_time:47831ms step_avg:271.77ms
step:187/500 train_loss:4.9442 train_time:48102ms step_avg:271.76ms
step:188/500 train_loss:5.1962 train_time:48373ms step_avg:271.76ms
step:189/500 train_loss:4.9912 train_time:48910ms step_avg:273.24ms
step:190/500 train_loss:4.9215 train_time:49453ms step_avg:274.74ms
step:191/500 train_loss:5.0612 train_time:49722ms step_avg:274.71ms
step:192/500 train_loss:4.9196 train_time:49995ms step_avg:274.70ms
step:193/500 train_loss:4.8200 train_time:50263ms step_avg:274.66ms
step:194/500 train_loss:5.0443 train_time:50546ms step_avg:274.71ms
step:195/500 train_loss:4.9612 train_time:50818ms step_avg:274.69ms
step:196/500 train_loss:5.1602 train_time:51087ms step_avg:274.66ms
step:197/500 train_loss:5.0327 train_time:51357ms step_avg:274.64ms
step:198/500 train_loss:4.9085 train_time:51628ms step_avg:274.62ms
step:199/500 train_loss:4.9171 train_time:51901ms step_avg:274.61ms
step:200/500 train_loss:4.8487 train_time:52172ms step_avg:274.59ms
step:201/500 train_loss:4.8956 train_time:52443ms step_avg:274.57ms w_mean:1.000 w_std:0.475 w_min:0.485 w_max:1.941
step:202/500 train_loss:4.8526 train_time:52714ms step_avg:274.55ms
step:203/500 train_loss:5.0305 train_time:52986ms step_avg:274.54ms
step:204/500 train_loss:4.9821 train_time:53257ms step_avg:274.52ms
step:205/500 train_loss:4.9057 train_time:53526ms step_avg:274.49ms
step:206/500 train_loss:5.0928 train_time:53799ms step_avg:274.49ms
step:207/500 train_loss:4.7620 train_time:54074ms step_avg:274.49ms
step:208/500 train_loss:4.9165 train_time:54344ms step_avg:274.47ms
step:209/500 train_loss:4.8595 train_time:54613ms step_avg:274.44ms
step:210/500 train_loss:5.0282 train_time:54886ms step_avg:274.43ms
step:211/500 train_loss:4.9565 train_time:55158ms step_avg:274.42ms
step:212/500 train_loss:4.8382 train_time:55428ms step_avg:274.40ms
step:213/500 train_loss:4.9706 train_time:55698ms step_avg:274.38ms
step:214/500 train_loss:4.8140 train_time:55969ms step_avg:274.36ms
step:215/500 train_loss:4.8968 train_time:56243ms step_avg:274.36ms
step:216/500 train_loss:4.7857 train_time:56514ms step_avg:274.34ms
step:217/500 train_loss:4.8811 train_time:56785ms step_avg:274.32ms
step:218/500 train_loss:4.8763 train_time:57059ms step_avg:274.32ms
step:219/500 train_loss:4.8182 train_time:57328ms step_avg:274.30ms
step:220/500 train_loss:4.8640 train_time:57601ms step_avg:274.29ms
step:221/500 train_loss:4.8613 train_time:57873ms step_avg:274.28ms
step:222/500 train_loss:4.9125 train_time:58146ms step_avg:274.27ms
step:223/500 train_loss:4.8485 train_time:58415ms step_avg:274.25ms
step:224/500 train_loss:4.8504 train_time:58686ms step_avg:274.24ms
step:225/500 train_loss:4.9725 train_time:58958ms step_avg:274.22ms
step:226/500 train_loss:4.7101 train_time:59233ms step_avg:274.23ms
step:227/500 train_loss:4.7666 train_time:59504ms step_avg:274.21ms
step:228/500 train_loss:4.7355 train_time:59773ms step_avg:274.19ms
step:229/500 train_loss:4.9110 train_time:60046ms step_avg:274.18ms
step:230/500 train_loss:4.7415 train_time:60318ms step_avg:274.17ms
step:231/500 train_loss:4.8873 train_time:60589ms step_avg:274.16ms
step:232/500 train_loss:4.7506 train_time:60862ms step_avg:274.15ms
step:233/500 train_loss:4.7166 train_time:61134ms step_avg:274.15ms
step:234/500 train_loss:4.9117 train_time:61406ms step_avg:274.14ms
step:235/500 train_loss:4.7543 train_time:61678ms step_avg:274.13ms
step:236/500 train_loss:4.7057 train_time:61948ms step_avg:274.11ms
step:237/500 train_loss:4.9345 train_time:62220ms step_avg:274.10ms
step:238/500 train_loss:4.8296 train_time:62493ms step_avg:274.09ms
step:239/500 train_loss:4.7376 train_time:62766ms step_avg:274.09ms
step:240/500 train_loss:4.8856 train_time:63037ms step_avg:274.07ms
step:241/500 train_loss:4.8511 train_time:63307ms step_avg:274.05ms
step:242/500 train_loss:4.7824 train_time:63581ms step_avg:274.06ms
step:243/500 train_loss:4.9125 train_time:63854ms step_avg:274.05ms
step:244/500 train_loss:4.7629 train_time:64126ms step_avg:274.04ms
step:245/500 train_loss:4.7733 train_time:64396ms step_avg:274.03ms
step:246/500 train_loss:4.8406 train_time:64668ms step_avg:274.02ms
step:247/500 train_loss:4.7931 train_time:64943ms step_avg:274.02ms
step:248/500 train_loss:4.7645 train_time:65214ms step_avg:274.01ms
step:249/500 train_loss:4.9064 train_time:65483ms step_avg:273.99ms
step:250/500 train_loss:4.6782 train_time:65758ms step_avg:273.99ms
step:250/500 val_loss:4.7518 train_time:65759ms step_avg:273.99ms
step:251/500 train_loss:4.6895 train_time:66031ms step_avg:273.99ms w_mean:1.000 w_std:0.483 w_min:0.485 w_max:1.940
step:252/500 train_loss:4.8460 train_time:66313ms step_avg:274.02ms
step:253/500 train_loss:4.8103 train_time:66589ms step_avg:274.03ms
step:254/500 train_loss:4.7227 train_time:66859ms step_avg:274.01ms
step:255/500 train_loss:4.7203 train_time:67129ms step_avg:274.00ms
step:256/500 train_loss:4.8489 train_time:67404ms step_avg:274.00ms
step:257/500 train_loss:4.7942 train_time:67679ms step_avg:274.00ms
step:258/500 train_loss:4.7767 train_time:67950ms step_avg:273.99ms
step:259/500 train_loss:4.7082 train_time:68224ms step_avg:273.99ms
step:260/500 train_loss:4.7139 train_time:68497ms step_avg:273.99ms
step:261/500 train_loss:4.7904 train_time:68770ms step_avg:273.98ms
step:262/500 train_loss:4.7828 train_time:69043ms step_avg:273.98ms
step:263/500 train_loss:4.7080 train_time:69317ms step_avg:273.98ms
step:264/500 train_loss:4.6432 train_time:69588ms step_avg:273.97ms
step:265/500 train_loss:4.7124 train_time:69858ms step_avg:273.95ms
step:266/500 train_loss:4.5617 train_time:70131ms step_avg:273.95ms
step:267/500 train_loss:4.6190 train_time:70401ms step_avg:273.93ms
step:268/500 train_loss:4.6652 train_time:70677ms step_avg:273.94ms
step:269/500 train_loss:4.6161 train_time:70949ms step_avg:273.93ms
step:270/500 train_loss:4.5988 train_time:71222ms step_avg:273.93ms
step:271/500 train_loss:4.7964 train_time:71492ms step_avg:273.91ms
step:272/500 train_loss:4.7426 train_time:71766ms step_avg:273.92ms
step:273/500 train_loss:4.6112 train_time:72039ms step_avg:273.91ms
step:274/500 train_loss:4.6437 train_time:72311ms step_avg:273.90ms
step:275/500 train_loss:4.7725 train_time:72580ms step_avg:273.89ms
step:276/500 train_loss:4.7618 train_time:72854ms step_avg:273.89ms
step:277/500 train_loss:4.9742 train_time:73123ms step_avg:273.87ms
step:278/500 train_loss:4.7189 train_time:73394ms step_avg:273.86ms
step:279/500 train_loss:4.8623 train_time:73664ms step_avg:273.84ms
step:280/500 train_loss:4.6939 train_time:73938ms step_avg:273.85ms
step:281/500 train_loss:4.7561 train_time:74210ms step_avg:273.84ms
step:282/500 train_loss:4.6608 train_time:74479ms step_avg:273.82ms
step:283/500 train_loss:4.7766 train_time:74752ms step_avg:273.82ms
step:284/500 train_loss:4.6030 train_time:75021ms step_avg:273.80ms
step:285/500 train_loss:4.7523 train_time:75293ms step_avg:273.79ms
step:286/500 train_loss:4.7620 train_time:75562ms step_avg:273.77ms
step:287/500 train_loss:4.7623 train_time:75836ms step_avg:273.78ms
step:288/500 train_loss:4.6617 train_time:76108ms step_avg:273.77ms
step:289/500 train_loss:4.6929 train_time:76381ms step_avg:273.77ms
step:290/500 train_loss:4.5814 train_time:76652ms step_avg:273.76ms
step:291/500 train_loss:4.5631 train_time:76925ms step_avg:273.75ms
step:292/500 train_loss:4.7104 train_time:77195ms step_avg:273.74ms
step:293/500 train_loss:4.5836 train_time:77465ms step_avg:273.73ms
step:294/500 train_loss:4.6434 train_time:77740ms step_avg:273.73ms
step:295/500 train_loss:4.6532 train_time:78016ms step_avg:273.74ms
step:296/500 train_loss:4.5336 train_time:78285ms step_avg:273.73ms
step:297/500 train_loss:4.5206 train_time:78558ms step_avg:273.72ms
step:298/500 train_loss:4.5471 train_time:78831ms step_avg:273.72ms
step:299/500 train_loss:4.6390 train_time:79103ms step_avg:273.71ms
step:300/500 train_loss:4.5385 train_time:79374ms step_avg:273.70ms
step:301/500 train_loss:4.7043 train_time:79645ms step_avg:273.69ms w_mean:1.000 w_std:0.481 w_min:0.485 w_max:1.942
step:302/500 train_loss:4.6832 train_time:79920ms step_avg:273.70ms
step:303/500 train_loss:4.6014 train_time:80190ms step_avg:273.69ms
step:304/500 train_loss:4.6677 train_time:80461ms step_avg:273.68ms
step:305/500 train_loss:4.6579 train_time:80735ms step_avg:273.68ms
step:306/500 train_loss:5.1043 train_time:81007ms step_avg:273.67ms
step:307/500 train_loss:4.6192 train_time:81278ms step_avg:273.66ms
step:308/500 train_loss:4.5076 train_time:81549ms step_avg:273.65ms
step:309/500 train_loss:4.7060 train_time:81824ms step_avg:273.66ms
step:310/500 train_loss:4.5090 train_time:82095ms step_avg:273.65ms
step:311/500 train_loss:4.7278 train_time:82365ms step_avg:273.64ms
step:312/500 train_loss:4.6614 train_time:82637ms step_avg:273.63ms
step:313/500 train_loss:4.5540 train_time:82911ms step_avg:273.63ms
step:314/500 train_loss:4.6906 train_time:83182ms step_avg:273.62ms
step:315/500 train_loss:4.8125 train_time:83455ms step_avg:273.62ms
step:316/500 train_loss:4.6616 train_time:83725ms step_avg:273.61ms
step:317/500 train_loss:4.5459 train_time:83997ms step_avg:273.61ms
step:318/500 train_loss:4.5678 train_time:84269ms step_avg:273.60ms
step:319/500 train_loss:4.5751 train_time:84540ms step_avg:273.59ms
step:320/500 train_loss:4.5286 train_time:84811ms step_avg:273.58ms
step:321/500 train_loss:4.6167 train_time:85081ms step_avg:273.57ms
step:322/500 train_loss:4.6306 train_time:85355ms step_avg:273.57ms
step:323/500 train_loss:4.5885 train_time:85628ms step_avg:273.57ms
step:324/500 train_loss:4.6643 train_time:85900ms step_avg:273.57ms
step:325/500 train_loss:4.6463 train_time:86173ms step_avg:273.56ms
step:326/500 train_loss:4.7290 train_time:86444ms step_avg:273.56ms
step:327/500 train_loss:4.5641 train_time:86718ms step_avg:273.56ms
step:328/500 train_loss:5.0171 train_time:86989ms step_avg:273.55ms
step:329/500 train_loss:4.7234 train_time:87261ms step_avg:273.54ms
step:330/500 train_loss:4.5314 train_time:87534ms step_avg:273.54ms
step:331/500 train_loss:4.4945 train_time:87803ms step_avg:273.53ms
step:332/500 train_loss:4.6271 train_time:88076ms step_avg:273.53ms
step:333/500 train_loss:4.5714 train_time:88349ms step_avg:273.53ms
step:334/500 train_loss:4.5415 train_time:88623ms step_avg:273.53ms
step:335/500 train_loss:4.5252 train_time:88893ms step_avg:273.52ms
step:336/500 train_loss:4.6960 train_time:89162ms step_avg:273.50ms
step:337/500 train_loss:4.6439 train_time:89435ms step_avg:273.50ms
step:338/500 train_loss:5.1514 train_time:89709ms step_avg:273.50ms
step:339/500 train_loss:4.6160 train_time:89980ms step_avg:273.49ms
step:340/500 train_loss:4.5769 train_time:90251ms step_avg:273.49ms
step:341/500 train_loss:4.5628 train_time:90527ms step_avg:273.50ms
step:342/500 train_loss:4.5051 train_time:90798ms step_avg:273.49ms
step:343/500 train_loss:4.4867 train_time:91068ms step_avg:273.48ms
step:344/500 train_loss:4.5475 train_time:91340ms step_avg:273.47ms
step:345/500 train_loss:4.6297 train_time:91614ms step_avg:273.48ms
step:346/500 train_loss:4.5274 train_time:91883ms step_avg:273.46ms
step:347/500 train_loss:4.4838 train_time:92154ms step_avg:273.45ms
step:348/500 train_loss:4.5450 train_time:92424ms step_avg:273.44ms
step:349/500 train_loss:4.5263 train_time:92697ms step_avg:273.44ms
step:350/500 train_loss:4.4545 train_time:92969ms step_avg:273.44ms
step:351/500 train_loss:4.1608 train_time:93242ms step_avg:273.44ms w_mean:1.000 w_std:0.491 w_min:0.490 w_max:1.959
step:352/500 train_loss:4.4320 train_time:93516ms step_avg:273.44ms
step:353/500 train_loss:4.7686 train_time:93786ms step_avg:273.43ms
step:354/500 train_loss:4.3302 train_time:94058ms step_avg:273.43ms
step:355/500 train_loss:4.5515 train_time:94331ms step_avg:273.42ms
step:356/500 train_loss:4.4782 train_time:94602ms step_avg:273.41ms
step:357/500 train_loss:4.5622 train_time:94874ms step_avg:273.41ms
step:358/500 train_loss:4.5850 train_time:95143ms step_avg:273.40ms
step:359/500 train_loss:4.4731 train_time:95417ms step_avg:273.40ms
step:360/500 train_loss:4.8091 train_time:95687ms step_avg:273.39ms
step:361/500 train_loss:4.2257 train_time:95960ms step_avg:273.39ms
step:362/500 train_loss:4.6889 train_time:96230ms step_avg:273.38ms
step:363/500 train_loss:4.5940 train_time:96501ms step_avg:273.37ms
step:364/500 train_loss:4.4759 train_time:96774ms step_avg:273.37ms
step:365/500 train_loss:4.4162 train_time:97044ms step_avg:273.36ms
step:366/500 train_loss:4.5704 train_time:97318ms step_avg:273.36ms
step:367/500 train_loss:4.4959 train_time:97589ms step_avg:273.36ms
step:368/500 train_loss:4.4829 train_time:97861ms step_avg:273.36ms
step:369/500 train_loss:4.4887 train_time:98136ms step_avg:273.36ms
step:370/500 train_loss:4.3880 train_time:98408ms step_avg:273.35ms
step:371/500 train_loss:4.5174 train_time:98679ms step_avg:273.35ms
step:372/500 train_loss:4.4800 train_time:98951ms step_avg:273.35ms
step:373/500 train_loss:4.3477 train_time:99222ms step_avg:273.34ms
step:374/500 train_loss:4.5259 train_time:99492ms step_avg:273.33ms
step:375/500 train_loss:4.4694 train_time:99766ms step_avg:273.33ms
step:375/500 val_loss:4.4851 train_time:99767ms step_avg:273.33ms
step:376/500 train_loss:4.4641 train_time:100040ms step_avg:273.33ms
step:377/500 train_loss:4.5287 train_time:100319ms step_avg:273.35ms
step:378/500 train_loss:4.4199 train_time:100852ms step_avg:274.05ms
step:379/500 train_loss:4.4613 train_time:101118ms step_avg:274.03ms
step:380/500 train_loss:4.5555 train_time:101650ms step_avg:274.73ms
step:381/500 train_loss:4.5690 train_time:101919ms step_avg:274.71ms
step:382/500 train_loss:4.5245 train_time:102186ms step_avg:274.69ms
step:383/500 train_loss:4.5096 train_time:102452ms step_avg:274.67ms
step:384/500 train_loss:4.4033 train_time:102730ms step_avg:274.68ms
step:385/500 train_loss:4.4986 train_time:103002ms step_avg:274.67ms
step:386/500 train_loss:4.4185 train_time:103271ms step_avg:274.66ms
step:387/500 train_loss:4.5473 train_time:103541ms step_avg:274.65ms
step:388/500 train_loss:4.7377 train_time:103815ms step_avg:274.64ms
step:389/500 train_loss:4.4363 train_time:104088ms step_avg:274.64ms
step:390/500 train_loss:4.3943 train_time:104357ms step_avg:274.62ms
step:391/500 train_loss:4.5203 train_time:104629ms step_avg:274.62ms
step:392/500 train_loss:4.4527 train_time:104902ms step_avg:274.61ms
step:393/500 train_loss:4.5475 train_time:105171ms step_avg:274.60ms
step:394/500 train_loss:4.3865 train_time:105442ms step_avg:274.59ms
step:395/500 train_loss:4.5054 train_time:105713ms step_avg:274.58ms
step:396/500 train_loss:4.2953 train_time:105986ms step_avg:274.57ms
step:397/500 train_loss:4.4543 train_time:106257ms step_avg:274.56ms
step:398/500 train_loss:4.5599 train_time:106530ms step_avg:274.56ms
step:399/500 train_loss:4.4975 train_time:106803ms step_avg:274.56ms
step:400/500 train_loss:4.4247 train_time:107073ms step_avg:274.55ms
step:401/500 train_loss:4.5010 train_time:107345ms step_avg:274.54ms w_mean:1.000 w_std:0.488 w_min:0.486 w_max:1.944
step:402/500 train_loss:4.5208 train_time:107617ms step_avg:274.53ms
step:403/500 train_loss:4.4960 train_time:107891ms step_avg:274.53ms
step:404/500 train_loss:4.5810 train_time:108163ms step_avg:274.53ms
step:405/500 train_loss:4.3865 train_time:108434ms step_avg:274.52ms
step:406/500 train_loss:4.4203 train_time:108709ms step_avg:274.52ms
step:407/500 train_loss:4.6828 train_time:108979ms step_avg:274.51ms
step:408/500 train_loss:4.4556 train_time:109250ms step_avg:274.50ms
step:409/500 train_loss:4.4384 train_time:109523ms step_avg:274.49ms
step:410/500 train_loss:4.4921 train_time:109795ms step_avg:274.49ms
step:411/500 train_loss:4.3887 train_time:110068ms step_avg:274.48ms
step:412/500 train_loss:4.4005 train_time:110337ms step_avg:274.47ms
step:413/500 train_loss:4.8143 train_time:110608ms step_avg:274.46ms
step:414/500 train_loss:4.2772 train_time:110880ms step_avg:274.45ms
step:415/500 train_loss:4.6196 train_time:111151ms step_avg:274.45ms
step:416/500 train_loss:4.4105 train_time:111426ms step_avg:274.45ms
step:417/500 train_loss:4.3968 train_time:111697ms step_avg:274.44ms
step:418/500 train_loss:4.5689 train_time:111969ms step_avg:274.43ms
step:419/500 train_loss:4.3178 train_time:112240ms step_avg:274.43ms
step:420/500 train_loss:4.4121 train_time:112519ms step_avg:274.44ms
step:421/500 train_loss:4.4036 train_time:112791ms step_avg:274.43ms
step:422/500 train_loss:4.2806 train_time:113063ms step_avg:274.42ms
step:423/500 train_loss:4.3806 train_time:113334ms step_avg:274.42ms
step:424/500 train_loss:4.4900 train_time:113608ms step_avg:274.42ms
step:425/500 train_loss:4.3226 train_time:113879ms step_avg:274.41ms
step:426/500 train_loss:4.4787 train_time:114149ms step_avg:274.40ms
step:427/500 train_loss:4.3472 train_time:114423ms step_avg:274.40ms
step:428/500 train_loss:4.5177 train_time:114695ms step_avg:274.39ms
step:429/500 train_loss:4.4797 train_time:114967ms step_avg:274.39ms
step:430/500 train_loss:4.3816 train_time:115237ms step_avg:274.37ms
step:431/500 train_loss:4.3651 train_time:115508ms step_avg:274.37ms
step:432/500 train_loss:4.3216 train_time:115780ms step_avg:274.36ms
step:433/500 train_loss:4.3973 train_time:116050ms step_avg:274.35ms
step:434/500 train_loss:4.4785 train_time:116324ms step_avg:274.35ms
step:435/500 train_loss:4.4075 train_time:116595ms step_avg:274.34ms
step:436/500 train_loss:4.4432 train_time:116867ms step_avg:274.34ms
step:437/500 train_loss:4.4579 train_time:117139ms step_avg:274.33ms
step:438/500 train_loss:4.3532 train_time:117420ms step_avg:274.35ms
step:439/500 train_loss:4.3636 train_time:117692ms step_avg:274.34ms
step:440/500 train_loss:4.3137 train_time:117964ms step_avg:274.33ms
step:441/500 train_loss:4.5151 train_time:118235ms step_avg:274.33ms
step:442/500 train_loss:4.4234 train_time:118509ms step_avg:274.33ms
step:443/500 train_loss:4.3881 train_time:118779ms step_avg:274.32ms
step:444/500 train_loss:4.2903 train_time:119051ms step_avg:274.31ms
step:445/500 train_loss:4.5324 train_time:119322ms step_avg:274.30ms
step:446/500 train_loss:4.4570 train_time:119592ms step_avg:274.29ms
step:447/500 train_loss:4.4562 train_time:119865ms step_avg:274.29ms
step:448/500 train_loss:4.3868 train_time:120138ms step_avg:274.29ms
step:449/500 train_loss:4.4603 train_time:120410ms step_avg:274.28ms
step:450/500 train_loss:4.3038 train_time:120679ms step_avg:274.27ms
step:451/500 train_loss:4.3482 train_time:120951ms step_avg:274.26ms w_mean:1.000 w_std:0.493 w_min:0.487 w_max:1.947
step:452/500 train_loss:4.2406 train_time:121224ms step_avg:274.26ms
step:453/500 train_loss:4.3393 train_time:121495ms step_avg:274.25ms
step:454/500 train_loss:4.3036 train_time:121766ms step_avg:274.25ms
step:455/500 train_loss:4.2944 train_time:122038ms step_avg:274.24ms
step:456/500 train_loss:4.4840 train_time:122312ms step_avg:274.24ms
step:457/500 train_loss:4.3399 train_time:122583ms step_avg:274.23ms
step:458/500 train_loss:4.4374 train_time:122851ms step_avg:274.22ms
step:459/500 train_loss:4.4685 train_time:123125ms step_avg:274.22ms
step:460/500 train_loss:4.2768 train_time:123397ms step_avg:274.22ms
step:461/500 train_loss:4.4397 train_time:123669ms step_avg:274.21ms
step:462/500 train_loss:4.3561 train_time:123937ms step_avg:274.20ms
step:463/500 train_loss:4.3229 train_time:124209ms step_avg:274.19ms
step:464/500 train_loss:4.4196 train_time:124481ms step_avg:274.19ms
step:465/500 train_loss:4.3494 train_time:124754ms step_avg:274.19ms
step:466/500 train_loss:4.3542 train_time:125025ms step_avg:274.18ms
step:467/500 train_loss:4.4846 train_time:125296ms step_avg:274.17ms
step:468/500 train_loss:4.4921 train_time:125568ms step_avg:274.17ms
step:469/500 train_loss:4.4388 train_time:125838ms step_avg:274.16ms
step:470/500 train_loss:4.3586 train_time:126111ms step_avg:274.15ms
step:471/500 train_loss:4.4485 train_time:126380ms step_avg:274.14ms
step:472/500 train_loss:4.4881 train_time:126654ms step_avg:274.14ms
step:473/500 train_loss:4.3934 train_time:126925ms step_avg:274.14ms
step:474/500 train_loss:4.3670 train_time:127197ms step_avg:274.13ms
step:475/500 train_loss:4.2448 train_time:127469ms step_avg:274.13ms
step:476/500 train_loss:4.6698 train_time:127742ms step_avg:274.13ms
step:477/500 train_loss:4.4186 train_time:128013ms step_avg:274.12ms
step:478/500 train_loss:4.2336 train_time:128284ms step_avg:274.11ms
step:479/500 train_loss:4.4170 train_time:128555ms step_avg:274.10ms
step:480/500 train_loss:4.4084 train_time:128830ms step_avg:274.11ms
step:481/500 train_loss:4.5267 train_time:129102ms step_avg:274.10ms
step:482/500 train_loss:4.3713 train_time:129371ms step_avg:274.09ms
step:483/500 train_loss:4.1969 train_time:129645ms step_avg:274.09ms
step:484/500 train_loss:4.4604 train_time:129917ms step_avg:274.09ms
step:485/500 train_loss:4.3155 train_time:130188ms step_avg:274.08ms
step:486/500 train_loss:4.3395 train_time:130459ms step_avg:274.07ms
step:487/500 train_loss:4.2969 train_time:130730ms step_avg:274.07ms
step:488/500 train_loss:4.3001 train_time:131007ms step_avg:274.07ms
step:489/500 train_loss:4.5128 train_time:131277ms step_avg:274.06ms
step:490/500 train_loss:4.3718 train_time:131549ms step_avg:274.06ms
step:491/500 train_loss:4.2766 train_time:131820ms step_avg:274.05ms
step:492/500 train_loss:4.2800 train_time:132092ms step_avg:274.05ms
step:493/500 train_loss:4.3908 train_time:132366ms step_avg:274.05ms
step:494/500 train_loss:4.2353 train_time:132638ms step_avg:274.05ms
step:495/500 train_loss:4.3801 train_time:132911ms step_avg:274.04ms
step:496/500 train_loss:4.3060 train_time:133180ms step_avg:274.03ms
step:497/500 train_loss:4.2675 train_time:133450ms step_avg:274.03ms
step:498/500 train_loss:4.3886 train_time:133723ms step_avg:274.02ms
step:499/500 train_loss:4.4742 train_time:133996ms step_avg:274.02ms
step:500/500 train_loss:4.5273 train_time:134265ms step_avg:274.01ms
step:500/500 val_loss:4.3751 train_time:134266ms step_avg:274.01ms
