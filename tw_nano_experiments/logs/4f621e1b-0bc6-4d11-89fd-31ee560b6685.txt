====================================================================================================
# NOTE: record from https://github.com/KellerJordan/modded-nanogpt/blob/master/records/track_1_short/2024-10-14_ModernArch/dabaaddd-237c-4ec9-939d-6608a9ed5e27.txt
# ====================================================================================================
import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import glob
import time
import math
import random
import argparse
from dataclasses import dataclass, field
from typing import Optional, Tuple, Dict, Any

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
import torch._inductor.config as config
from torch.nn.parallel import DistributedDataParallel as DDP

# -----------------------------------------------------------------------------
# Token Importance Weighting Configuration

@dataclass
class TokenWeightingConfig:
    """Configuration for token importance weighting ablations."""
    enabled: bool = False
    function: str = "linear"  # linear, sqrt, log, focal
    clamp_min: float = 0.5
    clamp_max: float = 2.0
    schedule: str = "constant"  # constant, warmup, anneal, cyclical, adaptive
    warmup_steps: int = 1000
    anneal_final_strength: float = 0.3  # for anneal schedule
    cyclical_period: int = 500  # for cyclical schedule
    focal_gamma: float = 2.0  # for focal loss style weighting
    percentile_clamp: bool = False  # use percentile-based clamping instead of fixed bounds
    percentile_low: float = 0.05  # lower percentile for clamping
    percentile_high: float = 0.95  # upper percentile for clamping
    log_weights: bool = True  # whether to log weight statistics
    log_every: int = 50  # log weight stats every N steps

def compute_schedule_strength(
    step: int,
    total_steps: int,
    config: TokenWeightingConfig,
    val_loss: Optional[float] = None
) -> float:
    """
    Compute the weighting strength multiplier based on the schedule.
    Returns a value in [0, 1] that scales the deviation from uniform weighting.
    """
    if config.schedule == "constant":
        return 1.0
    
    elif config.schedule == "warmup":
        # Linear warmup: start uniform, gradually introduce weighting
        return min(step / config.warmup_steps, 1.0)
    
    elif config.schedule == "anneal":
        # Annealing: start with strong weighting, decay toward uniform
        # decay_ratio goes from 1.0 to anneal_final_strength
        progress = step / total_steps
        return 1.0 - progress * (1.0 - config.anneal_final_strength)
    
    elif config.schedule == "cyclical":
        # Sine wave oscillation between weak and strong weighting
        return 0.5 + 0.5 * math.sin(step / config.cyclical_period * 2 * math.pi)
    
    elif config.schedule == "adaptive":
        # Validation-loss-based: strong when loss is high, weaker as loss decreases
        if val_loss is None:
            return 1.0  # Default to full strength if no val loss available
        # Normalize by approximate initial loss (~4.0 for GPT training)
        return min(val_loss / 4.0, 1.0)
    
    else:
        raise ValueError(f"Unknown schedule: {config.schedule}")

def compute_token_weights(
    per_token_loss: torch.Tensor,
    config: TokenWeightingConfig,
    step: int,
    total_steps: int,
    val_loss: Optional[float] = None
) -> Tuple[torch.Tensor, Dict[str, float]]:
    """
    Compute importance weights for tokens based on their loss.
    
    Args:
        per_token_loss: Tensor of shape (batch_size * seq_len,) with per-token losses
        config: TokenWeightingConfig with weighting parameters
        step: Current training step
        total_steps: Total number of training steps
        val_loss: Current validation loss (for adaptive schedule)
    
    Returns:
        weights: Tensor of same shape as per_token_loss with importance weights
        stats: Dictionary with weight statistics for logging
    """
    if not config.enabled:
        return torch.ones_like(per_token_loss), {}
    
    # Detach to prevent gradient flow through weights
    loss_detached = per_token_loss.detach()
    
    # Compute raw weights based on weighting function
    if config.function == "linear":
        # Relative weighting: weight = loss / mean_loss
        mean_loss = loss_detached.mean()
        weights = loss_detached / (mean_loss + 1e-8)
    
    elif config.function == "sqrt":
        # Square root weighting: more moderate than linear
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        weights = torch.sqrt(normalized_loss)
    
    elif config.function == "log":
        # Logarithmic weighting: most conservative
        mean_loss = loss_detached.mean()
        normalized_loss = loss_detached / (mean_loss + 1e-8)
        # log1p for numerical stability (log(1 + x))
        weights = torch.log1p(normalized_loss)
    
    elif config.function == "focal":
        # Focal loss style: (loss / max_loss)^gamma
        max_loss = loss_detached.max()
        normalized_loss = loss_detached / (max_loss + 1e-8)
        weights = torch.pow(normalized_loss, config.focal_gamma)
    
    else:
        raise ValueError(f"Unknown weighting function: {config.function}")
    
    # Apply clamping
    if config.percentile_clamp:
        # Dynamic bounds based on percentiles
        p_low = torch.quantile(weights, config.percentile_low)
        p_high = torch.quantile(weights, config.percentile_high)
        weights = weights.clamp(p_low, p_high)
    else:
        # Fixed bounds
        weights = weights.clamp(config.clamp_min, config.clamp_max)
    
    # Apply schedule (interpolate between uniform and computed weights)
    strength = compute_schedule_strength(step, total_steps, config, val_loss)
    # weights = 1.0 + (weights - 1.0) * strength
    # This interpolates: at strength=0, weights=1 (uniform); at strength=1, weights=computed
    weights = 1.0 + (weights - 1.0) * strength
    
    # Normalize weights to have mean 1.0 (preserve expected gradient magnitude)
    weights = weights / (weights.mean() + 1e-8)
    
    # Compute statistics for logging
    stats = {}
    if config.log_weights:
        stats = {
            'weight_mean': weights.mean().item(),
            'weight_std': weights.std().item(),
            'weight_min': weights.min().item(),
            'weight_max': weights.max().item(),
            'weight_gt_1.5': (weights > 1.5).float().mean().item() * 100,  # percentage
            'weight_lt_0.5': (weights < 0.5).float().mean().item() * 100,  # percentage
            'schedule_strength': strength,
        }
    
    return weights, stats

# -----------------------------------------------------------------------------
# Random seed utilities for reproducibility

def set_seed(seed: int, rank: int = 0, deterministic: bool = True):
    """
    Set random seeds for reproducibility across distributed training.
    
    Args:
        seed: Base random seed
        rank: DDP rank (used to offset data sampling seeds)
        deterministic: If True, enable deterministic algorithms (may impact performance)
    """
    # Set Python random seed
    random.seed(seed)
    
    # Set NumPy seed (offset by rank for different data sampling per GPU)
    np.random.seed(seed + rank)
    
    # Set PyTorch seeds
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # for multi-GPU
    
    # Configure deterministic behavior
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Enable deterministic algorithms (PyTorch 1.8+)
        if hasattr(torch, 'use_deterministic_algorithms'):
            try:
                torch.use_deterministic_algorithms(True, warn_only=True)
            except Exception:
                pass  # Some operations may not have deterministic implementations
    else:
        # For maximum performance, allow non-deterministic algorithms
        torch.backends.cudnn.deterministic = False
        torch.backends.cudnn.benchmark = True

# -----------------------------------------------------------------------------
# Muon optimizer

def zeropower_via_svd(G, steps=None):
    U, S, V = G.svd()
    return U @ V.T

@torch.compile
def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' \sim Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    X /= (X.norm() + eps) # ensure top singular value <= 1
    if G.size(0) > G.size(1):
        X = X.T
    for _ in range(steps):
        A = X @ X.T
        B = A @ X
        X = a * X + b * B + c * A @ B
    if G.size(0) > G.size(1):
        X = X.T
    return X

zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')
        backend_steps: The number of iteration steps to use in the backend, if it is iterative.
    """
    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True, backend='newtonschulz5', backend_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            momentum = group['momentum']
            zeropower_backend = zeropower_backends[group['backend']]
            for p in group['params']:
                g = p.grad
                if g is None:
                    continue
                state = self.state[p]
                if 'momentum_buffer' not in state:
                    state['momentum_buffer'] = torch.zeros_like(g)
                buf = state['momentum_buffer']
                buf.mul_(momentum).add_(g)
                if group['nesterov']:
                    g = g.add(buf, alpha=momentum)
                if g.size(0) == 3 * g.size(1): # split grouped QKV parameters
                    g = torch.cat([zeropower_backend(g1, steps=group['backend_steps']) for g1 in g.split(g.size(1))])
                    scale = g.size(1)**0.5
                else:
                    g = zeropower_backend(g, steps=group['backend_steps'])
                    scale = max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1
                p.data.add_(g, alpha=-lr * scale)

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

class Rotary(torch.nn.Module):

    def __init__(self, dim, base=10000):
        super().__init__()
        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None

    def forward(self, x):
        seq_len = x.shape[1]
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.outer(t, self.inv_freq).to(x.device)
            self.cos_cached = freqs.cos().bfloat16()
            self.sin_cached = freqs.sin().bfloat16()
        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]

def apply_rotary_emb(x, cos, sin):
    assert x.ndim == 4 # multihead attention
    d = x.shape[3]//2
    x1 = x[..., :d]
    x2 = x[..., d:]
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat([y1, y2], 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        assert self.n_embd % self.n_head == 0
        self.c_q = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_k = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_v = nn.Linear(self.n_embd, self.n_embd, bias=False)
        # output projection
        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.rotary = Rotary(self.head_dim)

    def forward(self, x):
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)
        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)
        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)
        cos, sin = self.rotary(q)
        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)
        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977
        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.attn = CausalSelfAttention(config)
        self.mlp = MLP(config)

    def forward(self, x):
        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))
        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))
        return x

# -----------------------------------------------------------------------------
# The main GPT-2 model

@dataclass
class GPTConfig:
    vocab_size : int = 50304
    n_layer : int = 12
    n_head : int = 6 # head dim 128 suggested by @Grad62304977
    n_embd : int = 768

class GPT(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ))
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying

    def forward(self, idx, targets=None, return_logits=True, return_per_token_loss=False):
        """
        Forward pass through the GPT model.
        
        Args:
            idx: Input token indices of shape (batch_size, seq_len)
            targets: Target token indices of shape (batch_size, seq_len)
            return_logits: Whether to return logits
            return_per_token_loss: Whether to return per-token loss (for token weighting)
        
        Returns:
            logits: Output logits (if return_logits=True)
            loss: Scalar loss (mean) or per-token loss tensor (if return_per_token_loss=True)
        """
        # forward the GPT model itself
        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
        for block in self.transformer.h:
            x = block(x)
        x = F.rms_norm(x, (x.size(-1),))

        if targets is not None:
            # if we are given some desired targets also calculate the loss
            logits = self.lm_head(x)
            logits = logits.float() # use tf32/fp32 for logits
            
            if return_per_token_loss:
                # Return per-token loss for token importance weighting
                # Shape: (batch_size * seq_len,)
                per_token_loss = F.cross_entropy(
                    logits.view(-1, logits.size(-1)), 
                    targets.view(-1), 
                    ignore_index=-1,
                    reduction='none'
                )
                loss = per_token_loss  # Return unreduced loss
            else:
                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
        else:
            # inference-time mini-optimization: only forward the lm_head on the very last position
            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim
            logits = logits.float() # use tf32/fp32 for logits
            loss = None

        # there are performance reasons why not returning logits is prudent, if not needed
        if not return_logits:
            logits = None

        return logits, loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _peek_data_shard(filename):
    # only reads the header, returns header data
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
    if header[0] != 20240520:
        print("ERROR: magic number mismatch in the data .bin file!")
        print("---> HINT: Are you passing in a correct file with --input_bin?")
        print("---> HINT: Dataset encoding changed recently, re-run data prepro or refer again to README")
        print("---> HINT: For example re-run: `python dev/data/tinyshakespeare.py`, then re-try")
        exit(1)
    assert header[1] == 1, "unsupported version"
    ntok = header[2] # number of tokens (claimed)
    return ntok # for now just return the number of tokens

def _load_data_shard(filename):
    with open(filename, "rb") as f:
        # first read the header, which is 256 int32 integers (4 bytes each)
        header = np.frombuffer(f.read(256*4), dtype=np.int32)
        assert header[0] == 20240520, "magic number mismatch in the data .bin file"
        assert header[1] == 1, "unsupported version"
        ntok = header[2] # number of tokens (claimed)
        # the rest of it are tokens, stored as uint16
        tokens = np.frombuffer(f.read(), dtype=np.uint16)
    assert len(tokens) == ntok, "number of tokens read does not match header?"
    return tokens

class DistributedDataLoader:
    def __init__(self, filename_pattern, B, T, process_rank, num_processes, seed=None):
        self.process_rank = process_rank
        self.num_processes = num_processes
        self.B = B
        self.T = T
        self.seed = seed

        # glob files that match the pattern
        self.files = sorted(glob.glob(filename_pattern))
        assert len(self.files) > 0, f"did not find any files that match the pattern {filename_pattern}"

        # load and validate all data shards, count number of tokens in total
        ntok_total = 0
        for fname in self.files:
            shard_ntok = _peek_data_shard(fname)
            assert shard_ntok >= num_processes * B * T + 1
            ntok_total += int(shard_ntok)
        self.ntok_total = ntok_total

        # kick things off
        self.reset()

    def reset(self):
        self.current_shard = 0
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def advance(self): # advance to next data shard
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = self.process_rank * self.B * self.T
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self):
        B = self.B
        T = self.T
        buf = self.tokens[self.current_position : self.current_position+B*T+1]
        buf = torch.tensor(buf.astype(np.int32), dtype=torch.long)
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T) # targets
        # advance current position and load next shard if necessary
        self.current_position += B * T * self.num_processes
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.advance()
        return x.cuda(), y.cuda()

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data hyperparams
    input_bin : str = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    input_val_bin : str = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    # optimization hyperparams
    batch_size : int = 8*64 # batch size, in sequences, across all devices
    device_batch_size : int = 64 # batch size, in sequences, per device
    sequence_length : int = 1024 # sequence length, in tokens
    num_iterations : int = 5100 # number of iterations to run
    learning_rate : float = 0.0036
    warmup_iters : int = 0
    warmdown_iters : int = 1450 # number of iterations of linear warmup/warmdown for triangular or trapezoidal schedule
    weight_decay : float = 0
    # evaluation and logging hyperparams
    val_loss_every : int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    val_tokens : int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    save_every : int = 0 # every how many steps to save the checkpoint? 0 for only at the end
    # reproducibility hyperparams
    seed : int = None # random seed for reproducibility (None = no seeding, uses system entropy)
    deterministic : bool = False # if True, use deterministic algorithms (slower but reproducible)
    # token weighting hyperparams (for ablations)
    tw_enabled : bool = False  # enable token importance weighting
    tw_function : str = "linear"  # weighting function: linear, sqrt, log, focal
    tw_clamp_min : float = 0.5  # minimum weight clamp
    tw_clamp_max : float = 2.0  # maximum weight clamp
    tw_schedule : str = "constant"  # schedule: constant, warmup, anneal, cyclical, adaptive
    tw_warmup_steps : int = 1000  # warmup steps for warmup schedule
    tw_anneal_final : float = 0.3  # final strength for anneal schedule
    tw_cyclical_period : int = 500  # period for cyclical schedule
    tw_focal_gamma : float = 2.0  # gamma for focal loss style weighting
    tw_percentile_clamp : bool = False  # use percentile-based clamping
    tw_percentile_low : float = 0.05  # lower percentile for clamping
    tw_percentile_high : float = 0.95  # upper percentile for clamping
    tw_log_weights : bool = True  # log weight statistics
    tw_log_every : int = 50  # log weight stats every N steps

def parse_args():
    """Parse command line arguments and return Hyperparameters and TokenWeightingConfig."""
    parser = argparse.ArgumentParser(description='NanoGPT Training with Token Importance Weighting')
    
    # Data hyperparams
    parser.add_argument('--input_bin', type=str, default='../fineweb10B/fineweb_train_*.bin')
    parser.add_argument('--input_val_bin', type=str, default='../fineweb10B/fineweb_val_*.bin')
    
    # Optimization hyperparams
    parser.add_argument('--batch_size', type=int, default=8*64)
    parser.add_argument('--device_batch_size', type=int, default=64)
    parser.add_argument('--sequence_length', type=int, default=1024)
    parser.add_argument('--num_iterations', type=int, default=5100)
    parser.add_argument('--learning_rate', type=float, default=0.0036)
    parser.add_argument('--warmup_iters', type=int, default=0)
    parser.add_argument('--warmdown_iters', type=int, default=1450)
    parser.add_argument('--weight_decay', type=float, default=0)
    
    # Evaluation and logging hyperparams
    parser.add_argument('--val_loss_every', type=int, default=125)
    parser.add_argument('--val_tokens', type=int, default=10485760)
    parser.add_argument('--save_every', type=int, default=0)
    
    # Reproducibility hyperparams
    parser.add_argument('--seed', type=int, default=None)
    parser.add_argument('--deterministic', action='store_true')
    
    # Token weighting hyperparams (for ablations)
    parser.add_argument('--tw_enabled', action='store_true', help='Enable token importance weighting')
    parser.add_argument('--tw_function', type=str, default='linear', 
                        choices=['linear', 'sqrt', 'log', 'focal'],
                        help='Weighting function')
    parser.add_argument('--tw_clamp_min', type=float, default=0.5, help='Minimum weight clamp')
    parser.add_argument('--tw_clamp_max', type=float, default=2.0, help='Maximum weight clamp')
    parser.add_argument('--tw_schedule', type=str, default='constant',
                        choices=['constant', 'warmup', 'anneal', 'cyclical', 'adaptive'],
                        help='Weighting schedule')
    parser.add_argument('--tw_warmup_steps', type=int, default=1000, help='Warmup steps')
    parser.add_argument('--tw_anneal_final', type=float, default=0.3, help='Final strength for anneal')
    parser.add_argument('--tw_cyclical_period', type=int, default=500, help='Period for cyclical')
    parser.add_argument('--tw_focal_gamma', type=float, default=2.0, help='Gamma for focal loss')
    parser.add_argument('--tw_percentile_clamp', action='store_true', help='Use percentile clamping')
    parser.add_argument('--tw_percentile_low', type=float, default=0.05, help='Lower percentile')
    parser.add_argument('--tw_percentile_high', type=float, default=0.95, help='Upper percentile')
    parser.add_argument('--tw_log_weights', action='store_true', help='Log weight statistics')
    parser.add_argument('--tw_log_every', type=int, default=50, help='Log weights every N steps')
    
    parsed_args = parser.parse_args()
    
    # Create Hyperparameters
    args = Hyperparameters(
        input_bin=parsed_args.input_bin,
        input_val_bin=parsed_args.input_val_bin,
        batch_size=parsed_args.batch_size,
        device_batch_size=parsed_args.device_batch_size,
        sequence_length=parsed_args.sequence_length,
        num_iterations=parsed_args.num_iterations,
        learning_rate=parsed_args.learning_rate,
        warmup_iters=parsed_args.warmup_iters,
        warmdown_iters=parsed_args.warmdown_iters,
        weight_decay=parsed_args.weight_decay,
        val_loss_every=parsed_args.val_loss_every,
        val_tokens=parsed_args.val_tokens,
        save_every=parsed_args.save_every,
        seed=parsed_args.seed,
        deterministic=parsed_args.deterministic,
        tw_enabled=parsed_args.tw_enabled,
        tw_function=parsed_args.tw_function,
        tw_clamp_min=parsed_args.tw_clamp_min,
        tw_clamp_max=parsed_args.tw_clamp_max,
        tw_schedule=parsed_args.tw_schedule,
        tw_warmup_steps=parsed_args.tw_warmup_steps,
        tw_anneal_final=parsed_args.tw_anneal_final,
        tw_cyclical_period=parsed_args.tw_cyclical_period,
        tw_focal_gamma=parsed_args.tw_focal_gamma,
        tw_percentile_clamp=parsed_args.tw_percentile_clamp,
        tw_percentile_low=parsed_args.tw_percentile_low,
        tw_percentile_high=parsed_args.tw_percentile_high,
        tw_log_weights=parsed_args.tw_log_weights,
        tw_log_every=parsed_args.tw_log_every,
    )
    
    # Create TokenWeightingConfig
    tw_config = TokenWeightingConfig(
        enabled=parsed_args.tw_enabled,
        function=parsed_args.tw_function,
        clamp_min=parsed_args.tw_clamp_min,
        clamp_max=parsed_args.tw_clamp_max,
        schedule=parsed_args.tw_schedule,
        warmup_steps=parsed_args.tw_warmup_steps,
        anneal_final_strength=parsed_args.tw_anneal_final,
        cyclical_period=parsed_args.tw_cyclical_period,
        focal_gamma=parsed_args.tw_focal_gamma,
        percentile_clamp=parsed_args.tw_percentile_clamp,
        percentile_low=parsed_args.tw_percentile_low,
        percentile_high=parsed_args.tw_percentile_high,
        log_weights=parsed_args.tw_log_weights,
        log_every=parsed_args.tw_log_every,
    )
    
    return args, tw_config

# Parse arguments
args, tw_config = parse_args()

# set up DDP (distributed data parallel). torchrun sets this env variable
assert torch.cuda.is_available()
dist.init_process_group(backend='nccl')
ddp_rank = int(os.environ['RANK'])
ddp_local_rank = int(os.environ['LOCAL_RANK'])
ddp_world_size = int(os.environ['WORLD_SIZE'])
device = f'cuda:{ddp_local_rank}'
torch.cuda.set_device(device)
print(f"using device: {device}")
master_process = (ddp_rank == 0) # this process will do logging, checkpointing etc.

# Set random seeds for reproducibility (must be done after DDP init but before model creation)
if args.seed is not None:
    set_seed(args.seed, rank=ddp_rank, deterministic=args.deterministic)
    if master_process:
        print(f"Random seed set to {args.seed} (deterministic={args.deterministic})")

# convenience variables
B, T = args.device_batch_size, args.sequence_length
# calculate the number of steps to take in the val loop.
assert args.val_tokens % (B * T * ddp_world_size) == 0
val_steps = args.val_tokens // (B * T * ddp_world_size)
# calculate the steps of gradient accumulation required to attain the desired global batch size.
assert args.batch_size % (B * ddp_world_size) == 0
train_accumulation_steps = args.batch_size // (B * ddp_world_size)

# load tokens
train_loader = DistributedDataLoader(args.input_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
val_loader = DistributedDataLoader(args.input_val_bin, B, T, ddp_rank, ddp_world_size, seed=args.seed)
if master_process:
    print(f"Training DataLoader: total number of tokens: {train_loader.ntok_total} across {len(train_loader.files)} files")
    print(f"Validation DataLoader: total number of tokens: {val_loader.ntok_total} across {len(val_loader.files)} files")
x, y = train_loader.next_batch()

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
num_vocab = 50304
model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))
model = model.cuda()
if hasattr(config, "coordinate_descent_tuning"):
    config.coordinate_descent_tuning = True # suggested by @Chillee
model = torch.compile(model)
# here we wrap model into DDP container
model = DDP(model, device_ids=[ddp_local_rank])
raw_model = model.module # always contains the "raw" unwrapped model
ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)

# init the optimizer(s)
optimizer1 = torch.optim.AdamW(raw_model.lm_head.parameters(), lr=args.learning_rate, betas=(0.9, 0.95),
                               weight_decay=args.weight_decay, fused=True)
optimizer2 = Muon(raw_model.transformer.h.parameters(), lr=0.1*args.learning_rate, momentum=0.95)
optimizers = [optimizer1, optimizer2]
# learning rate decay scheduler (linear warmup and warmdown)
def get_lr(it):
    assert it <= args.num_iterations
    # 1) linear warmup for warmup_iters steps
    if it < args.warmup_iters:
        return (it+1) / args.warmup_iters
    # 2) constant lr for a while
    elif it < args.num_iterations - args.warmdown_iters:
        return 1.0
    # 3) linear warmdown
    else:
        decay_ratio = (args.num_iterations - it) / args.warmdown_iters
        return decay_ratio
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# begin logging
if master_process:
    run_id = str(uuid.uuid4())
    logdir = 'logs/%s/' % run_id
    os.makedirs(logdir, exist_ok=True)
    logfile = 'logs/%s.txt' % run_id
    # create the log file
    with open(logfile, "w") as f:
        # begin the log by printing this file (the Python code)
        f.write('='*100 + '\n')
        f.write(code)
        f.write('='*100 + '\n')
        # log information about the hardware/software environment this is running on
        # and print the full `nvidia-smi` to file
        f.write(f"Running pytorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}\nnvidia-smi:\n")
        import subprocess
        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        f.write(f'{result.stdout}\n')
        f.write('='*100 + '\n')
        # log the random seed if set
        if args.seed is not None:
            f.write(f"Random seed: {args.seed} (deterministic={args.deterministic})\n")
            f.write('='*100 + '\n')
        # log token weighting configuration
        if tw_config.enabled:
            f.write(f"Token Weighting Configuration:\n")
            f.write(f"  function: {tw_config.function}\n")
            f.write(f"  clamp: [{tw_config.clamp_min}, {tw_config.clamp_max}]\n")
            f.write(f"  schedule: {tw_config.schedule}\n")
            if tw_config.schedule == 'warmup':
                f.write(f"  warmup_steps: {tw_config.warmup_steps}\n")
            elif tw_config.schedule == 'anneal':
                f.write(f"  anneal_final_strength: {tw_config.anneal_final_strength}\n")
            elif tw_config.schedule == 'cyclical':
                f.write(f"  cyclical_period: {tw_config.cyclical_period}\n")
            if tw_config.function == 'focal':
                f.write(f"  focal_gamma: {tw_config.focal_gamma}\n")
            if tw_config.percentile_clamp:
                f.write(f"  percentile_clamp: [{tw_config.percentile_low}, {tw_config.percentile_high}]\n")
            f.write('='*100 + '\n')
        else:
            f.write("Token Weighting: DISABLED (baseline run)\n")
            f.write('='*100 + '\n')
    
    # Also log token weighting config to console
    if tw_config.enabled:
        print(f"Token Weighting: ENABLED")
        print(f"  function={tw_config.function}, clamp=[{tw_config.clamp_min}, {tw_config.clamp_max}], schedule={tw_config.schedule}")
    else:
        print(f"Token Weighting: DISABLED (baseline run)")

# Track validation loss for adaptive schedule
current_val_loss = None

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.time()
# begin training
train_loader.reset()
for step in range(args.num_iterations + 1):
    last_step = (step == args.num_iterations)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.time()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # once in a while evaluate the validation dataset
    if (last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        for _ in range(val_steps):
            x_val, y_val = val_loader.next_batch()
            with ctx: # of course, we'd like to use no_grad() here too, but that creates a torch.compile error for some reason
                _, loss = model(x_val, y_val, return_logits=False)
                val_loss += loss.detach()
                del loss
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        current_val_loss = val_loss.item()  # Store for adaptive schedule
        # log val loss to console and to logfile
        if master_process:
            print(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms')
            with open(logfile, "a") as f:
                f.write(f'step:{step}/{args.num_iterations} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms\n')
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    if master_process and (last_step or (args.save_every > 0 and step % args.save_every == 0)):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.time() - t0)
        # save the state of the training process
        log = dict(step=step, code=code, model=raw_model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
        # also save the seed for reproducibility
        if args.seed is not None:
            log['seed'] = args.seed
        # save token weighting config
        if tw_config.enabled:
            log['tw_config'] = {
                'function': tw_config.function,
                'clamp_min': tw_config.clamp_min,
                'clamp_max': tw_config.clamp_max,
                'schedule': tw_config.schedule,
            }
        torch.save(log, 'logs/%s/state_step%06d.pt' % (run_id, step))
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.time()

    # bit confusing: we want to make sure to eval on 0th iteration
    # but also after the very last iteration. so we loop for step <= num_iterations
    # instead of just < num_iterations (one extra due to <=), only to do
    # the validation/sampling one last time, and then we break right here as we're done.
    if last_step:
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    model.train()
    
    # Accumulate weight statistics across accumulation steps
    accumulated_weight_stats = {}
    
    for i in range(1, train_accumulation_steps+1):
        # forward pass
        with ctx:
            if tw_config.enabled:
                # Get per-token loss for token weighting
                _, per_token_loss = model(x, y, return_logits=False, return_per_token_loss=True)
                
                # Compute token importance weights
                weights, weight_stats = compute_token_weights(
                    per_token_loss,
                    tw_config,
                    step,
                    args.num_iterations,
                    current_val_loss
                )
                
                # Apply weights and compute mean loss
                weighted_loss = per_token_loss * weights
                loss = weighted_loss.mean()
                train_loss = per_token_loss.mean().detach()  # Log unweighted loss for fair comparison
                
                # Accumulate weight stats
                if weight_stats and (step % tw_config.log_every == 0):
                    for k, v in weight_stats.items():
                        if k not in accumulated_weight_stats:
                            accumulated_weight_stats[k] = []
                        accumulated_weight_stats[k].append(v)
            else:
                # Standard training without token weighting
                _, loss = model(x, y, return_logits=False)
                train_loss = loss.detach()
        
        # advance the dataset for the next batch
        x, y = train_loader.next_batch()
        # backward pass
        if i < train_accumulation_steps:
            with model.no_sync(): # there's no need to sync gradients every accumulation step
                loss.backward()
        else:
            loss.backward() # just sync on the last step
    for p in model.parameters():
        p.grad /= train_accumulation_steps
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # --------------- TRAINING SECTION END -------------------
    # everything that follows now is just diagnostics, prints, logging, etc.

    #dist.all_reduce(train_loss, op=dist.ReduceOp.AVG) # all-reducing the training loss would be more correct in terms of logging, but slower
    if master_process:
        approx_time = training_time_ms + 1000 * (time.time() - t0)
        
        # Build log message
        log_msg = f"step:{step+1}/{args.num_iterations} train_loss:{train_loss.item():.4f} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms"
        
        # Add weight statistics if enabled and this is a logging step
        if tw_config.enabled and tw_config.log_weights and accumulated_weight_stats and (step % tw_config.log_every == 0):
            # Average the accumulated stats
            avg_weight_stats = {k: sum(v)/len(v) for k, v in accumulated_weight_stats.items()}
            weight_log = f" w_mean:{avg_weight_stats.get('weight_mean', 1.0):.3f} w_std:{avg_weight_stats.get('weight_std', 0.0):.3f} w_min:{avg_weight_stats.get('weight_min', 1.0):.3f} w_max:{avg_weight_stats.get('weight_max', 1.0):.3f}"
            log_msg += weight_log
        
        print(log_msg)
        with open(logfile, "a") as f:
            f.write(log_msg + "\n")

if master_process:
    print(f"peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB")====================================================================================================
Running pytorch 2.8.0+cu128 compiled for CUDA 12.8
nvidia-smi:
Tue Dec  9 21:22:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:08.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 PCIe               On  |   00000000:00:09.0 Off |                    0 |
| N/A   44C    P0             84W /  310W |    2363MiB /  81559MiB |     33%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 PCIe               On  |   00000000:00:0A.0 Off |                    0 |
| N/A   49C    P0             88W /  310W |    2363MiB /  81559MiB |     28%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 PCIe               On  |   00000000:00:0B.0 Off |                    0 |
| N/A   45C    P0             86W /  310W |    2363MiB /  81559MiB |     36%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 PCIe               On  |   00000000:00:0C.0 Off |                    0 |
| N/A   44C    P0             81W /  310W |    2363MiB /  81559MiB |     37%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 PCIe               On  |   00000000:00:0D.0 Off |                    0 |
| N/A   49C    P0             86W /  310W |    2363MiB /  81559MiB |     30%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 PCIe               On  |   00000000:00:0E.0 Off |                    0 |
| N/A   52C    P0             87W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   45C    P0             84W /  310W |    2363MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           62350      C   /usr/local/bin/python                  2354MiB |
|    1   N/A  N/A           62351      C   /usr/local/bin/python                  2354MiB |
|    2   N/A  N/A           62352      C   /usr/local/bin/python                  2354MiB |
|    3   N/A  N/A           62353      C   /usr/local/bin/python                  2354MiB |
|    4   N/A  N/A           62354      C   /usr/local/bin/python                  2354MiB |
|    5   N/A  N/A           62355      C   /usr/local/bin/python                  2354MiB |
|    6   N/A  N/A           62356      C   /usr/local/bin/python                  2354MiB |
|    7   N/A  N/A           62357      C   /usr/local/bin/python                  2354MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Random seed: 123 (deterministic=False)
====================================================================================================
Token Weighting Configuration:
  function: sqrt
  clamp: [0.7, 1.5]
  schedule: constant
====================================================================================================
step:0/500 val_loss:16.0073 train_time:284ms step_avg:nanms
step:1/500 train_loss:15.9981 train_time:52445ms step_avg:nanms w_mean:1.000 w_std:0.021 w_min:0.700 w_max:1.075
step:2/500 train_loss:9.3703 train_time:53041ms step_avg:nanms
step:3/500 train_loss:8.9866 train_time:53327ms step_avg:nanms
step:4/500 train_loss:8.6961 train_time:53608ms step_avg:nanms
step:5/500 train_loss:8.1305 train_time:53891ms step_avg:nanms
step:6/500 train_loss:7.7092 train_time:54174ms step_avg:nanms
step:7/500 train_loss:7.3903 train_time:54457ms step_avg:nanms
step:8/500 train_loss:7.5239 train_time:54741ms step_avg:nanms
step:9/500 train_loss:7.2731 train_time:55025ms step_avg:nanms
step:10/500 train_loss:7.0562 train_time:55307ms step_avg:nanms
step:11/500 train_loss:7.0576 train_time:282ms step_avg:nanms
step:12/500 train_loss:7.0223 train_time:564ms step_avg:nanms
step:13/500 train_loss:6.8184 train_time:847ms step_avg:282.45ms
step:14/500 train_loss:6.8396 train_time:1130ms step_avg:282.52ms
step:15/500 train_loss:6.7927 train_time:1413ms step_avg:282.70ms
step:16/500 train_loss:6.7381 train_time:1697ms step_avg:282.80ms
step:17/500 train_loss:6.7273 train_time:1981ms step_avg:283.01ms
step:18/500 train_loss:6.7749 train_time:2263ms step_avg:282.93ms
step:19/500 train_loss:6.5829 train_time:2546ms step_avg:282.92ms
step:20/500 train_loss:6.5999 train_time:2830ms step_avg:282.99ms
step:21/500 train_loss:6.2637 train_time:3114ms step_avg:283.11ms
step:22/500 train_loss:6.6406 train_time:3398ms step_avg:283.14ms
step:23/500 train_loss:6.8780 train_time:3679ms step_avg:283.01ms
step:24/500 train_loss:6.5138 train_time:3970ms step_avg:283.57ms
step:25/500 train_loss:6.6306 train_time:4256ms step_avg:283.71ms
step:26/500 train_loss:6.3558 train_time:4539ms step_avg:283.70ms
step:27/500 train_loss:6.2963 train_time:4822ms step_avg:283.63ms
step:28/500 train_loss:6.4343 train_time:5105ms step_avg:283.63ms
step:29/500 train_loss:6.1190 train_time:5391ms step_avg:283.73ms
step:30/500 train_loss:6.3781 train_time:5675ms step_avg:283.74ms
step:31/500 train_loss:6.2481 train_time:5959ms step_avg:283.76ms
step:32/500 train_loss:6.1976 train_time:6242ms step_avg:283.74ms
step:33/500 train_loss:6.0245 train_time:6525ms step_avg:283.68ms
step:34/500 train_loss:6.3522 train_time:6808ms step_avg:283.66ms
step:35/500 train_loss:6.2598 train_time:7092ms step_avg:283.68ms
step:36/500 train_loss:6.4121 train_time:7377ms step_avg:283.72ms
step:37/500 train_loss:6.3357 train_time:7662ms step_avg:283.76ms
step:38/500 train_loss:6.2323 train_time:7945ms step_avg:283.74ms
step:39/500 train_loss:6.1176 train_time:8228ms step_avg:283.72ms
step:40/500 train_loss:6.1807 train_time:8513ms step_avg:283.78ms
step:41/500 train_loss:6.0769 train_time:8800ms step_avg:283.87ms
step:42/500 train_loss:6.1195 train_time:9084ms step_avg:283.86ms
step:43/500 train_loss:5.9830 train_time:9366ms step_avg:283.83ms
step:44/500 train_loss:6.0734 train_time:9650ms step_avg:283.82ms
step:45/500 train_loss:6.0603 train_time:9935ms step_avg:283.85ms
step:46/500 train_loss:6.2403 train_time:10221ms step_avg:283.91ms
step:47/500 train_loss:6.0470 train_time:10504ms step_avg:283.90ms
step:48/500 train_loss:5.8907 train_time:10787ms step_avg:283.88ms
step:49/500 train_loss:6.1410 train_time:11073ms step_avg:283.91ms
step:50/500 train_loss:5.9923 train_time:11357ms step_avg:283.92ms
step:51/500 train_loss:6.1587 train_time:11641ms step_avg:283.93ms w_mean:1.000 w_std:0.229 w_min:0.706 w_max:1.514
step:52/500 train_loss:6.0105 train_time:11925ms step_avg:283.93ms
step:53/500 train_loss:5.8856 train_time:12210ms step_avg:283.96ms
step:54/500 train_loss:5.9871 train_time:12494ms step_avg:283.96ms
step:55/500 train_loss:5.9287 train_time:12779ms step_avg:283.99ms
step:56/500 train_loss:6.2068 train_time:13063ms step_avg:283.99ms
step:57/500 train_loss:5.9186 train_time:13349ms step_avg:284.01ms
step:58/500 train_loss:5.7889 train_time:13631ms step_avg:283.97ms
step:59/500 train_loss:5.9485 train_time:13915ms step_avg:283.97ms
step:60/500 train_loss:5.8915 train_time:14199ms step_avg:283.98ms
step:61/500 train_loss:5.9659 train_time:14482ms step_avg:283.96ms
step:62/500 train_loss:5.7990 train_time:14770ms step_avg:284.03ms
step:63/500 train_loss:5.8636 train_time:15054ms step_avg:284.03ms
step:64/500 train_loss:5.8630 train_time:15338ms step_avg:284.04ms
step:65/500 train_loss:5.7383 train_time:15622ms step_avg:284.03ms
step:66/500 train_loss:5.6948 train_time:15905ms step_avg:284.02ms
step:67/500 train_loss:5.8350 train_time:16189ms step_avg:284.02ms
step:68/500 train_loss:5.7238 train_time:16474ms step_avg:284.03ms
step:69/500 train_loss:5.9450 train_time:16759ms step_avg:284.05ms
step:70/500 train_loss:5.6339 train_time:17043ms step_avg:284.05ms
step:71/500 train_loss:5.6518 train_time:17327ms step_avg:284.04ms
step:72/500 train_loss:5.8449 train_time:17612ms step_avg:284.06ms
step:73/500 train_loss:5.8028 train_time:17896ms step_avg:284.06ms
step:74/500 train_loss:5.6803 train_time:18181ms step_avg:284.08ms
step:75/500 train_loss:5.7997 train_time:18464ms step_avg:284.06ms
step:76/500 train_loss:5.7471 train_time:18746ms step_avg:284.02ms
step:77/500 train_loss:5.7300 train_time:19030ms step_avg:284.02ms
step:78/500 train_loss:5.8085 train_time:19316ms step_avg:284.05ms
step:79/500 train_loss:5.8362 train_time:19600ms step_avg:284.06ms
step:80/500 train_loss:5.6983 train_time:19884ms step_avg:284.05ms
step:81/500 train_loss:5.7750 train_time:20169ms step_avg:284.07ms
step:82/500 train_loss:5.5635 train_time:20453ms step_avg:284.07ms
step:83/500 train_loss:5.7147 train_time:20738ms step_avg:284.09ms
step:84/500 train_loss:5.6996 train_time:21023ms step_avg:284.09ms
step:85/500 train_loss:5.6569 train_time:21305ms step_avg:284.07ms
step:86/500 train_loss:5.5188 train_time:21590ms step_avg:284.07ms
step:87/500 train_loss:5.7362 train_time:21875ms step_avg:284.10ms
step:88/500 train_loss:5.6242 train_time:22161ms step_avg:284.11ms
step:89/500 train_loss:5.7024 train_time:22444ms step_avg:284.10ms
step:90/500 train_loss:5.6633 train_time:22726ms step_avg:284.08ms
step:91/500 train_loss:5.5968 train_time:23012ms step_avg:284.09ms
step:92/500 train_loss:5.5865 train_time:23296ms step_avg:284.10ms
step:93/500 train_loss:5.6962 train_time:23581ms step_avg:284.11ms
step:94/500 train_loss:5.5321 train_time:23865ms step_avg:284.11ms
step:95/500 train_loss:5.5332 train_time:24149ms step_avg:284.11ms
step:96/500 train_loss:5.5486 train_time:24435ms step_avg:284.13ms
step:97/500 train_loss:5.4785 train_time:24720ms step_avg:284.14ms
step:98/500 train_loss:5.5398 train_time:25003ms step_avg:284.13ms
step:99/500 train_loss:5.4722 train_time:25286ms step_avg:284.11ms
step:100/500 train_loss:5.5793 train_time:25570ms step_avg:284.11ms
step:101/500 train_loss:5.5562 train_time:25855ms step_avg:284.12ms w_mean:1.000 w_std:0.241 w_min:0.706 w_max:1.514
step:102/500 train_loss:5.4356 train_time:26140ms step_avg:284.13ms
step:103/500 train_loss:5.5566 train_time:26424ms step_avg:284.13ms
step:104/500 train_loss:5.5046 train_time:26708ms step_avg:284.12ms
step:105/500 train_loss:5.3605 train_time:26994ms step_avg:284.14ms
step:106/500 train_loss:5.4710 train_time:27279ms step_avg:284.16ms
step:107/500 train_loss:5.6472 train_time:27563ms step_avg:284.15ms
step:108/500 train_loss:5.4687 train_time:27845ms step_avg:284.13ms
step:109/500 train_loss:5.2060 train_time:28129ms step_avg:284.13ms
step:110/500 train_loss:5.4322 train_time:28415ms step_avg:284.15ms
step:111/500 train_loss:5.3737 train_time:28700ms step_avg:284.16ms
step:112/500 train_loss:5.3742 train_time:28984ms step_avg:284.15ms
step:113/500 train_loss:5.4489 train_time:29267ms step_avg:284.15ms
step:114/500 train_loss:5.4024 train_time:29551ms step_avg:284.14ms
step:115/500 train_loss:5.2377 train_time:29836ms step_avg:284.15ms
step:116/500 train_loss:5.4247 train_time:30121ms step_avg:284.16ms
step:117/500 train_loss:5.2680 train_time:30404ms step_avg:284.15ms
step:118/500 train_loss:5.2805 train_time:30688ms step_avg:284.15ms
step:119/500 train_loss:5.3804 train_time:30972ms step_avg:284.15ms
step:120/500 train_loss:5.3815 train_time:31257ms step_avg:284.16ms
step:121/500 train_loss:5.3011 train_time:31543ms step_avg:284.17ms
step:122/500 train_loss:5.2079 train_time:31825ms step_avg:284.15ms
step:123/500 train_loss:5.2845 train_time:32109ms step_avg:284.15ms
step:124/500 train_loss:5.1696 train_time:32393ms step_avg:284.15ms
step:125/500 train_loss:5.4510 train_time:32679ms step_avg:284.17ms
step:125/500 val_loss:5.2895 train_time:32680ms step_avg:284.17ms
step:126/500 train_loss:5.3040 train_time:32969ms step_avg:284.22ms
step:127/500 train_loss:5.2832 train_time:33255ms step_avg:284.23ms
step:128/500 train_loss:5.3531 train_time:33539ms step_avg:284.23ms
step:129/500 train_loss:5.2092 train_time:33822ms step_avg:284.22ms
step:130/500 train_loss:5.4768 train_time:34108ms step_avg:284.23ms
step:131/500 train_loss:5.2617 train_time:34394ms step_avg:284.25ms
step:132/500 train_loss:5.2775 train_time:34678ms step_avg:284.24ms
step:133/500 train_loss:5.1955 train_time:34961ms step_avg:284.23ms
step:134/500 train_loss:5.2704 train_time:35245ms step_avg:284.24ms
step:135/500 train_loss:5.1689 train_time:35531ms step_avg:284.25ms
step:136/500 train_loss:5.2672 train_time:35816ms step_avg:284.26ms
step:137/500 train_loss:5.0391 train_time:36099ms step_avg:284.24ms
step:138/500 train_loss:5.2180 train_time:36382ms step_avg:284.23ms
step:139/500 train_loss:5.1631 train_time:36668ms step_avg:284.25ms
step:140/500 train_loss:5.1811 train_time:36954ms step_avg:284.26ms
step:141/500 train_loss:5.2225 train_time:37238ms step_avg:284.26ms
step:142/500 train_loss:5.1338 train_time:37522ms step_avg:284.26ms
step:143/500 train_loss:5.2206 train_time:37806ms step_avg:284.25ms
step:144/500 train_loss:5.0251 train_time:38091ms step_avg:284.26ms
step:145/500 train_loss:5.1883 train_time:38376ms step_avg:284.27ms
step:146/500 train_loss:5.1086 train_time:38660ms step_avg:284.27ms
step:147/500 train_loss:5.0392 train_time:38946ms step_avg:284.28ms
step:148/500 train_loss:5.1384 train_time:39231ms step_avg:284.28ms
step:149/500 train_loss:5.1304 train_time:39516ms step_avg:284.29ms
step:150/500 train_loss:5.1779 train_time:39800ms step_avg:284.28ms
step:151/500 train_loss:5.2018 train_time:40085ms step_avg:284.29ms w_mean:1.000 w_std:0.250 w_min:0.707 w_max:1.515
step:152/500 train_loss:5.1189 train_time:40370ms step_avg:284.29ms
step:153/500 train_loss:5.0927 train_time:40655ms step_avg:284.30ms
step:154/500 train_loss:5.1706 train_time:40939ms step_avg:284.30ms
step:155/500 train_loss:5.1086 train_time:41222ms step_avg:284.29ms
step:156/500 train_loss:5.0991 train_time:41506ms step_avg:284.29ms
step:157/500 train_loss:5.1053 train_time:41792ms step_avg:284.30ms
step:158/500 train_loss:5.2301 train_time:42077ms step_avg:284.30ms
step:159/500 train_loss:5.0257 train_time:42360ms step_avg:284.30ms
step:160/500 train_loss:5.0683 train_time:42644ms step_avg:284.30ms
step:161/500 train_loss:4.9487 train_time:42910ms step_avg:284.17ms
step:162/500 train_loss:5.0716 train_time:43192ms step_avg:284.16ms
step:163/500 train_loss:5.1313 train_time:43473ms step_avg:284.14ms
step:164/500 train_loss:5.0962 train_time:43756ms step_avg:284.13ms
step:165/500 train_loss:4.9514 train_time:44036ms step_avg:284.10ms
step:166/500 train_loss:5.0373 train_time:44316ms step_avg:284.08ms
step:167/500 train_loss:5.2105 train_time:44597ms step_avg:284.06ms
step:168/500 train_loss:4.9753 train_time:44877ms step_avg:284.03ms
step:169/500 train_loss:5.0686 train_time:45157ms step_avg:284.01ms
step:170/500 train_loss:4.9206 train_time:45437ms step_avg:283.98ms
step:171/500 train_loss:4.8836 train_time:45717ms step_avg:283.96ms
step:172/500 train_loss:4.9783 train_time:45998ms step_avg:283.94ms
step:173/500 train_loss:4.9580 train_time:46278ms step_avg:283.91ms
step:174/500 train_loss:5.0144 train_time:46557ms step_avg:283.88ms
step:175/500 train_loss:5.1460 train_time:46837ms step_avg:283.86ms
step:176/500 train_loss:5.0392 train_time:47117ms step_avg:283.84ms
step:177/500 train_loss:4.8820 train_time:47397ms step_avg:283.82ms
step:178/500 train_loss:4.8501 train_time:47677ms step_avg:283.79ms
step:179/500 train_loss:4.8974 train_time:47957ms step_avg:283.77ms
step:180/500 train_loss:4.9295 train_time:48237ms step_avg:283.75ms
step:181/500 train_loss:4.9269 train_time:48518ms step_avg:283.73ms
step:182/500 train_loss:5.0300 train_time:48797ms step_avg:283.71ms
step:183/500 train_loss:4.9353 train_time:49078ms step_avg:283.69ms
step:184/500 train_loss:4.8518 train_time:49357ms step_avg:283.66ms
step:185/500 train_loss:4.8903 train_time:49637ms step_avg:283.64ms
step:186/500 train_loss:5.0002 train_time:49917ms step_avg:283.62ms
step:187/500 train_loss:4.8945 train_time:50198ms step_avg:283.60ms
step:188/500 train_loss:5.1227 train_time:50477ms step_avg:283.58ms
step:189/500 train_loss:4.9264 train_time:51064ms step_avg:285.27ms
step:190/500 train_loss:4.8458 train_time:51650ms step_avg:286.95ms
step:191/500 train_loss:5.0027 train_time:51934ms step_avg:286.93ms
step:192/500 train_loss:4.8500 train_time:52215ms step_avg:286.90ms
step:193/500 train_loss:4.7594 train_time:52496ms step_avg:286.86ms
step:194/500 train_loss:4.9666 train_time:52776ms step_avg:286.83ms
step:195/500 train_loss:4.8990 train_time:53056ms step_avg:286.79ms
step:196/500 train_loss:5.0819 train_time:53336ms step_avg:286.75ms
step:197/500 train_loss:4.9698 train_time:53617ms step_avg:286.72ms
step:198/500 train_loss:4.8270 train_time:53897ms step_avg:286.69ms
step:199/500 train_loss:4.8583 train_time:54177ms step_avg:286.65ms
step:200/500 train_loss:4.7678 train_time:54457ms step_avg:286.62ms
step:201/500 train_loss:4.8366 train_time:54737ms step_avg:286.58ms w_mean:1.000 w_std:0.260 w_min:0.707 w_max:1.515
step:202/500 train_loss:4.7769 train_time:55016ms step_avg:286.54ms
step:203/500 train_loss:4.9723 train_time:55297ms step_avg:286.51ms
step:204/500 train_loss:4.9024 train_time:55577ms step_avg:286.48ms
step:205/500 train_loss:4.8430 train_time:55857ms step_avg:286.45ms
step:206/500 train_loss:5.0130 train_time:56137ms step_avg:286.41ms
step:207/500 train_loss:4.6830 train_time:56418ms step_avg:286.39ms
step:208/500 train_loss:4.8425 train_time:56698ms step_avg:286.36ms
step:209/500 train_loss:4.7849 train_time:56978ms step_avg:286.32ms
step:210/500 train_loss:4.9561 train_time:57258ms step_avg:286.29ms
step:211/500 train_loss:4.8770 train_time:57537ms step_avg:286.25ms
step:212/500 train_loss:4.7742 train_time:57818ms step_avg:286.23ms
step:213/500 train_loss:4.8989 train_time:58097ms step_avg:286.19ms
step:214/500 train_loss:4.7489 train_time:58377ms step_avg:286.16ms
step:215/500 train_loss:4.8194 train_time:58658ms step_avg:286.14ms
step:216/500 train_loss:4.7095 train_time:58938ms step_avg:286.11ms
step:217/500 train_loss:4.8035 train_time:59218ms step_avg:286.08ms
step:218/500 train_loss:4.7939 train_time:59497ms step_avg:286.04ms
step:219/500 train_loss:4.7522 train_time:59778ms step_avg:286.02ms
step:220/500 train_loss:4.7821 train_time:60058ms step_avg:285.99ms
step:221/500 train_loss:4.7972 train_time:60337ms step_avg:285.96ms
step:222/500 train_loss:4.8383 train_time:60618ms step_avg:285.93ms
step:223/500 train_loss:4.7787 train_time:60897ms step_avg:285.90ms
step:224/500 train_loss:4.7829 train_time:61177ms step_avg:285.87ms
step:225/500 train_loss:4.9025 train_time:61457ms step_avg:285.84ms
step:226/500 train_loss:4.6510 train_time:61737ms step_avg:285.82ms
step:227/500 train_loss:4.6863 train_time:62018ms step_avg:285.80ms
step:228/500 train_loss:4.6714 train_time:62298ms step_avg:285.77ms
step:229/500 train_loss:4.8331 train_time:62578ms step_avg:285.74ms
step:230/500 train_loss:4.6598 train_time:62858ms step_avg:285.72ms
step:231/500 train_loss:4.8148 train_time:63139ms step_avg:285.69ms
step:232/500 train_loss:4.6783 train_time:63421ms step_avg:285.68ms
step:233/500 train_loss:4.6430 train_time:63702ms step_avg:285.66ms
step:234/500 train_loss:4.8382 train_time:63982ms step_avg:285.63ms
step:235/500 train_loss:4.6852 train_time:64268ms step_avg:285.63ms
step:236/500 train_loss:4.6159 train_time:64551ms step_avg:285.62ms
step:237/500 train_loss:4.8739 train_time:64833ms step_avg:285.61ms
step:238/500 train_loss:4.7514 train_time:65115ms step_avg:285.59ms
step:239/500 train_loss:4.6662 train_time:65395ms step_avg:285.57ms
step:240/500 train_loss:4.8031 train_time:65676ms step_avg:285.55ms
step:241/500 train_loss:4.7846 train_time:65956ms step_avg:285.53ms
step:242/500 train_loss:4.6996 train_time:66237ms step_avg:285.50ms
step:243/500 train_loss:4.8428 train_time:66517ms step_avg:285.48ms
step:244/500 train_loss:4.6880 train_time:66797ms step_avg:285.46ms
step:245/500 train_loss:4.6955 train_time:67076ms step_avg:285.43ms
step:246/500 train_loss:4.7699 train_time:67357ms step_avg:285.41ms
step:247/500 train_loss:4.7214 train_time:67637ms step_avg:285.39ms
step:248/500 train_loss:4.6819 train_time:67926ms step_avg:285.40ms
step:249/500 train_loss:4.8427 train_time:68208ms step_avg:285.39ms
step:250/500 train_loss:4.5890 train_time:68490ms step_avg:285.37ms
step:250/500 val_loss:4.6867 train_time:68492ms step_avg:285.38ms
step:251/500 train_loss:4.6230 train_time:68759ms step_avg:285.31ms w_mean:1.000 w_std:0.264 w_min:0.707 w_max:1.515
step:252/500 train_loss:4.7632 train_time:69039ms step_avg:285.29ms
step:253/500 train_loss:4.7354 train_time:69320ms step_avg:285.27ms
step:254/500 train_loss:4.6407 train_time:69600ms step_avg:285.25ms
step:255/500 train_loss:4.6488 train_time:69880ms step_avg:285.22ms
step:256/500 train_loss:4.7740 train_time:70161ms step_avg:285.21ms
step:257/500 train_loss:4.7184 train_time:70441ms step_avg:285.19ms
step:258/500 train_loss:4.7011 train_time:70721ms step_avg:285.17ms
step:259/500 train_loss:4.6286 train_time:71002ms step_avg:285.15ms
step:260/500 train_loss:4.6444 train_time:71281ms step_avg:285.12ms
step:261/500 train_loss:4.7109 train_time:71561ms step_avg:285.10ms
step:262/500 train_loss:4.7068 train_time:71841ms step_avg:285.08ms
step:263/500 train_loss:4.6289 train_time:72122ms step_avg:285.07ms
step:264/500 train_loss:4.5718 train_time:72400ms step_avg:285.04ms
step:265/500 train_loss:4.6301 train_time:72679ms step_avg:285.02ms
step:266/500 train_loss:4.4918 train_time:72959ms step_avg:285.00ms
step:267/500 train_loss:4.5384 train_time:73240ms step_avg:284.98ms
step:268/500 train_loss:4.5884 train_time:73522ms step_avg:284.97ms
step:269/500 train_loss:4.5392 train_time:73801ms step_avg:284.95ms
step:270/500 train_loss:4.5223 train_time:74082ms step_avg:284.93ms
step:271/500 train_loss:4.7187 train_time:74362ms step_avg:284.91ms
step:272/500 train_loss:4.6681 train_time:74642ms step_avg:284.89ms
step:273/500 train_loss:4.5220 train_time:74921ms step_avg:284.87ms
step:274/500 train_loss:4.5779 train_time:75202ms step_avg:284.86ms
step:275/500 train_loss:4.6853 train_time:75484ms step_avg:284.85ms
step:276/500 train_loss:4.6982 train_time:75765ms step_avg:284.83ms
step:277/500 train_loss:4.9006 train_time:76048ms step_avg:284.82ms
step:278/500 train_loss:4.6434 train_time:76331ms step_avg:284.82ms
step:279/500 train_loss:4.7816 train_time:76613ms step_avg:284.81ms
step:280/500 train_loss:4.6247 train_time:76897ms step_avg:284.80ms
step:281/500 train_loss:4.6932 train_time:77177ms step_avg:284.79ms
step:282/500 train_loss:4.5889 train_time:77458ms step_avg:284.77ms
step:283/500 train_loss:4.7007 train_time:77738ms step_avg:284.75ms
step:284/500 train_loss:4.5241 train_time:78019ms step_avg:284.74ms
step:285/500 train_loss:4.6856 train_time:78299ms step_avg:284.72ms
step:286/500 train_loss:4.6749 train_time:78579ms step_avg:284.71ms
step:287/500 train_loss:4.6991 train_time:78859ms step_avg:284.69ms
step:288/500 train_loss:4.5710 train_time:79139ms step_avg:284.67ms
step:289/500 train_loss:4.6333 train_time:79419ms step_avg:284.66ms
step:290/500 train_loss:4.4935 train_time:79699ms step_avg:284.64ms
step:291/500 train_loss:4.5021 train_time:79979ms step_avg:284.62ms
step:292/500 train_loss:4.6134 train_time:80259ms step_avg:284.61ms
step:293/500 train_loss:4.5132 train_time:80539ms step_avg:284.59ms
step:294/500 train_loss:4.5483 train_time:80819ms step_avg:284.58ms
step:295/500 train_loss:4.5850 train_time:81099ms step_avg:284.56ms
step:296/500 train_loss:4.4411 train_time:81379ms step_avg:284.54ms
step:297/500 train_loss:4.4418 train_time:81660ms step_avg:284.53ms
step:298/500 train_loss:4.4571 train_time:81940ms step_avg:284.52ms
step:299/500 train_loss:4.5706 train_time:82220ms step_avg:284.50ms
step:300/500 train_loss:4.4497 train_time:82500ms step_avg:284.48ms
step:301/500 train_loss:4.6359 train_time:82779ms step_avg:284.47ms w_mean:1.000 w_std:0.263 w_min:0.707 w_max:1.516
step:302/500 train_loss:4.5941 train_time:83059ms step_avg:284.45ms
step:303/500 train_loss:4.5304 train_time:83339ms step_avg:284.43ms
step:304/500 train_loss:4.5840 train_time:83618ms step_avg:284.42ms
step:305/500 train_loss:4.5777 train_time:83899ms step_avg:284.40ms
step:306/500 train_loss:5.0303 train_time:84180ms step_avg:284.39ms
step:307/500 train_loss:4.5404 train_time:84459ms step_avg:284.38ms
step:308/500 train_loss:4.4320 train_time:84739ms step_avg:284.36ms
step:309/500 train_loss:4.6241 train_time:85020ms step_avg:284.35ms
step:310/500 train_loss:4.4290 train_time:85299ms step_avg:284.33ms
step:311/500 train_loss:4.6540 train_time:85579ms step_avg:284.31ms
step:312/500 train_loss:4.5676 train_time:85859ms step_avg:284.30ms
step:313/500 train_loss:4.4846 train_time:86139ms step_avg:284.29ms
step:314/500 train_loss:4.6074 train_time:86418ms step_avg:284.27ms
step:315/500 train_loss:4.7411 train_time:86699ms step_avg:284.26ms
step:316/500 train_loss:4.5713 train_time:86979ms step_avg:284.24ms
step:317/500 train_loss:4.4724 train_time:87260ms step_avg:284.23ms
step:318/500 train_loss:4.4693 train_time:87540ms step_avg:284.22ms
step:319/500 train_loss:4.4974 train_time:87819ms step_avg:284.20ms
step:320/500 train_loss:4.4386 train_time:88099ms step_avg:284.19ms
step:321/500 train_loss:4.5404 train_time:88380ms step_avg:284.18ms
step:322/500 train_loss:4.5421 train_time:88660ms step_avg:284.17ms
step:323/500 train_loss:4.5108 train_time:88940ms step_avg:284.15ms
step:324/500 train_loss:4.5794 train_time:89219ms step_avg:284.14ms
step:325/500 train_loss:4.5745 train_time:89498ms step_avg:284.12ms
step:326/500 train_loss:4.6402 train_time:89779ms step_avg:284.11ms
step:327/500 train_loss:4.4925 train_time:90059ms step_avg:284.10ms
step:328/500 train_loss:4.9271 train_time:90339ms step_avg:284.08ms
step:329/500 train_loss:4.6451 train_time:90618ms step_avg:284.07ms
step:330/500 train_loss:4.4414 train_time:90899ms step_avg:284.06ms
step:331/500 train_loss:4.4107 train_time:91179ms step_avg:284.05ms
step:332/500 train_loss:4.5525 train_time:91459ms step_avg:284.04ms
step:333/500 train_loss:4.4774 train_time:91739ms step_avg:284.02ms
step:334/500 train_loss:4.4630 train_time:92019ms step_avg:284.01ms
step:335/500 train_loss:4.4189 train_time:92299ms step_avg:284.00ms
step:336/500 train_loss:4.6110 train_time:92579ms step_avg:283.99ms
step:337/500 train_loss:4.5531 train_time:92863ms step_avg:283.99ms
step:338/500 train_loss:5.0697 train_time:93144ms step_avg:283.98ms
step:339/500 train_loss:4.5247 train_time:93425ms step_avg:283.97ms
step:340/500 train_loss:4.4906 train_time:93704ms step_avg:283.95ms
step:341/500 train_loss:4.4754 train_time:93986ms step_avg:283.94ms
step:342/500 train_loss:4.4184 train_time:94266ms step_avg:283.93ms
step:343/500 train_loss:4.3999 train_time:94549ms step_avg:283.93ms
step:344/500 train_loss:4.4562 train_time:94831ms step_avg:283.93ms
step:345/500 train_loss:4.5422 train_time:95116ms step_avg:283.93ms
step:346/500 train_loss:4.4421 train_time:95397ms step_avg:283.92ms
step:347/500 train_loss:4.3899 train_time:95678ms step_avg:283.91ms
step:348/500 train_loss:4.4417 train_time:95959ms step_avg:283.90ms
step:349/500 train_loss:4.4385 train_time:96239ms step_avg:283.89ms
step:350/500 train_loss:4.3647 train_time:96519ms step_avg:283.88ms
step:351/500 train_loss:4.0618 train_time:96800ms step_avg:283.87ms w_mean:1.000 w_std:0.270 w_min:0.711 w_max:1.523
step:352/500 train_loss:4.3434 train_time:97080ms step_avg:283.86ms
step:353/500 train_loss:4.6789 train_time:97359ms step_avg:283.85ms
step:354/500 train_loss:4.2353 train_time:97640ms step_avg:283.84ms
step:355/500 train_loss:4.4703 train_time:97919ms step_avg:283.82ms
step:356/500 train_loss:4.3848 train_time:98199ms step_avg:283.81ms
step:357/500 train_loss:4.4734 train_time:98479ms step_avg:283.80ms
step:358/500 train_loss:4.4884 train_time:98760ms step_avg:283.79ms
step:359/500 train_loss:4.3817 train_time:99040ms step_avg:283.78ms
step:360/500 train_loss:4.7278 train_time:99320ms step_avg:283.77ms
step:361/500 train_loss:4.1243 train_time:99600ms step_avg:283.76ms
step:362/500 train_loss:4.5997 train_time:99880ms step_avg:283.75ms
step:363/500 train_loss:4.4991 train_time:100159ms step_avg:283.74ms
step:364/500 train_loss:4.3866 train_time:100439ms step_avg:283.73ms
step:365/500 train_loss:4.3237 train_time:100719ms step_avg:283.72ms
step:366/500 train_loss:4.4804 train_time:100999ms step_avg:283.71ms
step:367/500 train_loss:4.4080 train_time:101280ms step_avg:283.70ms
step:368/500 train_loss:4.3956 train_time:101559ms step_avg:283.68ms
step:369/500 train_loss:4.3993 train_time:101839ms step_avg:283.68ms
step:370/500 train_loss:4.2974 train_time:102120ms step_avg:283.67ms
step:371/500 train_loss:4.4360 train_time:102400ms step_avg:283.66ms
step:372/500 train_loss:4.3768 train_time:102679ms step_avg:283.64ms
step:373/500 train_loss:4.2567 train_time:102959ms step_avg:283.63ms
step:374/500 train_loss:4.4450 train_time:103239ms step_avg:283.62ms
step:375/500 train_loss:4.3814 train_time:103519ms step_avg:283.61ms
step:375/500 val_loss:4.3928 train_time:103521ms step_avg:283.62ms
step:376/500 train_loss:4.3713 train_time:103790ms step_avg:283.58ms
step:377/500 train_loss:4.4315 train_time:104073ms step_avg:283.58ms
step:378/500 train_loss:4.3296 train_time:104666ms step_avg:284.42ms
step:379/500 train_loss:4.3787 train_time:104946ms step_avg:284.41ms
step:380/500 train_loss:4.4621 train_time:105540ms step_avg:285.24ms
step:381/500 train_loss:4.4808 train_time:105820ms step_avg:285.23ms
step:382/500 train_loss:4.4310 train_time:106100ms step_avg:285.21ms
step:383/500 train_loss:4.4078 train_time:106380ms step_avg:285.20ms
step:384/500 train_loss:4.3089 train_time:106662ms step_avg:285.19ms
step:385/500 train_loss:4.4095 train_time:106942ms step_avg:285.18ms
step:386/500 train_loss:4.3289 train_time:107221ms step_avg:285.16ms
step:387/500 train_loss:4.4523 train_time:107501ms step_avg:285.15ms
step:388/500 train_loss:4.6499 train_time:107782ms step_avg:285.14ms
step:389/500 train_loss:4.3468 train_time:108062ms step_avg:285.12ms
step:390/500 train_loss:4.3094 train_time:108342ms step_avg:285.11ms
step:391/500 train_loss:4.4301 train_time:108623ms step_avg:285.10ms
step:392/500 train_loss:4.3624 train_time:108902ms step_avg:285.08ms
step:393/500 train_loss:4.4580 train_time:109183ms step_avg:285.07ms
step:394/500 train_loss:4.2901 train_time:109462ms step_avg:285.06ms
step:395/500 train_loss:4.4195 train_time:109742ms step_avg:285.04ms
step:396/500 train_loss:4.2023 train_time:110021ms step_avg:285.03ms
step:397/500 train_loss:4.3678 train_time:110302ms step_avg:285.02ms
step:398/500 train_loss:4.4578 train_time:110582ms step_avg:285.01ms
step:399/500 train_loss:4.4010 train_time:110862ms step_avg:284.99ms
step:400/500 train_loss:4.3319 train_time:111141ms step_avg:284.98ms
step:401/500 train_loss:4.4005 train_time:111422ms step_avg:284.97ms w_mean:1.000 w_std:0.269 w_min:0.708 w_max:1.516
step:402/500 train_loss:4.4317 train_time:111702ms step_avg:284.95ms
step:403/500 train_loss:4.3991 train_time:111982ms step_avg:284.94ms
step:404/500 train_loss:4.4894 train_time:112261ms step_avg:284.93ms
step:405/500 train_loss:4.2803 train_time:112542ms step_avg:284.92ms
step:406/500 train_loss:4.3254 train_time:112822ms step_avg:284.90ms
step:407/500 train_loss:4.5948 train_time:113101ms step_avg:284.89ms
step:408/500 train_loss:4.3615 train_time:113382ms step_avg:284.88ms
step:409/500 train_loss:4.3547 train_time:113662ms step_avg:284.87ms
step:410/500 train_loss:4.4063 train_time:113941ms step_avg:284.85ms
step:411/500 train_loss:4.2907 train_time:114221ms step_avg:284.84ms
step:412/500 train_loss:4.3069 train_time:114500ms step_avg:284.83ms
step:413/500 train_loss:4.7237 train_time:114780ms step_avg:284.81ms
step:414/500 train_loss:4.1800 train_time:115061ms step_avg:284.80ms
step:415/500 train_loss:4.5312 train_time:115342ms step_avg:284.79ms
step:416/500 train_loss:4.3156 train_time:115622ms step_avg:284.78ms
step:417/500 train_loss:4.3047 train_time:115902ms step_avg:284.77ms
step:418/500 train_loss:4.4843 train_time:116182ms step_avg:284.76ms
step:419/500 train_loss:4.2284 train_time:116461ms step_avg:284.75ms
step:420/500 train_loss:4.3237 train_time:116742ms step_avg:284.74ms
step:421/500 train_loss:4.2998 train_time:117022ms step_avg:284.73ms
step:422/500 train_loss:4.1846 train_time:117302ms step_avg:284.71ms
step:423/500 train_loss:4.2894 train_time:117582ms step_avg:284.70ms
step:424/500 train_loss:4.4005 train_time:117862ms step_avg:284.69ms
step:425/500 train_loss:4.2161 train_time:118143ms step_avg:284.68ms
step:426/500 train_loss:4.3702 train_time:118423ms step_avg:284.67ms
step:427/500 train_loss:4.2483 train_time:118702ms step_avg:284.66ms
step:428/500 train_loss:4.4215 train_time:118983ms step_avg:284.65ms
step:429/500 train_loss:4.3802 train_time:119263ms step_avg:284.64ms
step:430/500 train_loss:4.2910 train_time:119543ms step_avg:284.63ms
step:431/500 train_loss:4.2696 train_time:119822ms step_avg:284.61ms
step:432/500 train_loss:4.2277 train_time:120102ms step_avg:284.60ms
step:433/500 train_loss:4.2991 train_time:120382ms step_avg:284.59ms
step:434/500 train_loss:4.3758 train_time:120662ms step_avg:284.58ms
step:435/500 train_loss:4.3082 train_time:120942ms step_avg:284.57ms
step:436/500 train_loss:4.3525 train_time:121222ms step_avg:284.56ms
step:437/500 train_loss:4.3636 train_time:121504ms step_avg:284.55ms
step:438/500 train_loss:4.2556 train_time:121784ms step_avg:284.54ms
step:439/500 train_loss:4.2684 train_time:122064ms step_avg:284.53ms
step:440/500 train_loss:4.2288 train_time:122343ms step_avg:284.52ms
step:441/500 train_loss:4.4148 train_time:122623ms step_avg:284.51ms
step:442/500 train_loss:4.3243 train_time:122903ms step_avg:284.50ms
step:443/500 train_loss:4.2938 train_time:123183ms step_avg:284.49ms
step:444/500 train_loss:4.1866 train_time:123463ms step_avg:284.48ms
step:445/500 train_loss:4.4413 train_time:123743ms step_avg:284.47ms
step:446/500 train_loss:4.3702 train_time:124023ms step_avg:284.46ms
step:447/500 train_loss:4.3690 train_time:124303ms step_avg:284.45ms
step:448/500 train_loss:4.2868 train_time:124583ms step_avg:284.44ms
step:449/500 train_loss:4.3634 train_time:124863ms step_avg:284.43ms
step:450/500 train_loss:4.2082 train_time:125143ms step_avg:284.42ms
step:451/500 train_loss:4.2553 train_time:125422ms step_avg:284.40ms w_mean:1.000 w_std:0.271 w_min:0.709 w_max:1.519
step:452/500 train_loss:4.1402 train_time:125702ms step_avg:284.39ms
step:453/500 train_loss:4.2409 train_time:125983ms step_avg:284.39ms
step:454/500 train_loss:4.2150 train_time:126263ms step_avg:284.38ms
step:455/500 train_loss:4.1891 train_time:126543ms step_avg:284.37ms
step:456/500 train_loss:4.3965 train_time:126823ms step_avg:284.36ms
step:457/500 train_loss:4.2499 train_time:127102ms step_avg:284.34ms
step:458/500 train_loss:4.3412 train_time:127381ms step_avg:284.33ms
step:459/500 train_loss:4.3750 train_time:127662ms step_avg:284.33ms
step:460/500 train_loss:4.1766 train_time:127942ms step_avg:284.32ms
step:461/500 train_loss:4.3467 train_time:128222ms step_avg:284.31ms
step:462/500 train_loss:4.2573 train_time:128501ms step_avg:284.30ms
step:463/500 train_loss:4.2393 train_time:128782ms step_avg:284.29ms
step:464/500 train_loss:4.3208 train_time:129063ms step_avg:284.28ms
step:465/500 train_loss:4.2586 train_time:129343ms step_avg:284.27ms
step:466/500 train_loss:4.2583 train_time:129623ms step_avg:284.26ms
step:467/500 train_loss:4.3852 train_time:129903ms step_avg:284.25ms
step:468/500 train_loss:4.3951 train_time:130183ms step_avg:284.24ms
step:469/500 train_loss:4.3503 train_time:130463ms step_avg:284.23ms
step:470/500 train_loss:4.2609 train_time:130743ms step_avg:284.22ms
step:471/500 train_loss:4.3511 train_time:131024ms step_avg:284.22ms
step:472/500 train_loss:4.3954 train_time:131304ms step_avg:284.21ms
step:473/500 train_loss:4.3005 train_time:131584ms step_avg:284.20ms
step:474/500 train_loss:4.2726 train_time:131863ms step_avg:284.19ms
step:475/500 train_loss:4.1512 train_time:132142ms step_avg:284.18ms
step:476/500 train_loss:4.5691 train_time:132422ms step_avg:284.17ms
step:477/500 train_loss:4.3252 train_time:132702ms step_avg:284.16ms
step:478/500 train_loss:4.1410 train_time:132984ms step_avg:284.15ms
step:479/500 train_loss:4.3333 train_time:133262ms step_avg:284.14ms
step:480/500 train_loss:4.3185 train_time:133543ms step_avg:284.13ms
step:481/500 train_loss:4.4378 train_time:133823ms step_avg:284.12ms
step:482/500 train_loss:4.2747 train_time:134103ms step_avg:284.12ms
step:483/500 train_loss:4.0945 train_time:134382ms step_avg:284.11ms
step:484/500 train_loss:4.3649 train_time:134663ms step_avg:284.10ms
step:485/500 train_loss:4.2131 train_time:134941ms step_avg:284.09ms
step:486/500 train_loss:4.2404 train_time:135221ms step_avg:284.08ms
step:487/500 train_loss:4.1871 train_time:135501ms step_avg:284.07ms
step:488/500 train_loss:4.2090 train_time:135781ms step_avg:284.06ms
step:489/500 train_loss:4.4161 train_time:136062ms step_avg:284.06ms
step:490/500 train_loss:4.2763 train_time:136342ms step_avg:284.05ms
step:491/500 train_loss:4.1755 train_time:136622ms step_avg:284.04ms
step:492/500 train_loss:4.1822 train_time:136902ms step_avg:284.03ms
step:493/500 train_loss:4.2954 train_time:137182ms step_avg:284.02ms
step:494/500 train_loss:4.1427 train_time:137462ms step_avg:284.01ms
step:495/500 train_loss:4.2883 train_time:137742ms step_avg:284.00ms
step:496/500 train_loss:4.2056 train_time:138022ms step_avg:284.00ms
step:497/500 train_loss:4.1550 train_time:138302ms step_avg:283.99ms
step:498/500 train_loss:4.2936 train_time:138584ms step_avg:283.98ms
step:499/500 train_loss:4.3795 train_time:138863ms step_avg:283.97ms
step:500/500 train_loss:4.4306 train_time:139144ms step_avg:283.97ms
step:500/500 val_loss:4.2797 train_time:139145ms step_avg:283.97ms
